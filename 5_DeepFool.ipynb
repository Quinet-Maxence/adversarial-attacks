{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1091dcb-e0e0-4767-95df-30247ae372dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DeepFool- Adversarial Attacks\n",
    "### Paper link : https://arxiv.org/abs/1511.04599"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63cd03-35c1-4228-a9f9-851b27ed02eb",
   "metadata": {},
   "source": [
    "## Objectives & Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f86b0a-b2dd-41c8-8858-2f406012541b",
   "metadata": {},
   "source": [
    "In the previous sections, we explored **FGSM, BIM, PGD, and Auto-PGD**. If you haven‚Äôt gone through these notebooks yet, we highly recommend reviewing them first, as **DeepFool is a fundamentally different approach**.\n",
    "\n",
    "So far, we‚Äôve seen:\n",
    "\n",
    "- **FGSM** applies a single-step attack (fast but easy to defend against).\n",
    "- **BIM** refines the attack by applying multiple smaller steps instead of one big step (stronger than FGSM but lacks control).\n",
    "- **PGD** corrects BIM by projecting perturbations into a norm-constrained space to ensure they remain valid (more stable and harder to defend against).\n",
    "- **Auto-PGD** dynamically adjusts `eps_step` during the attack process to make the attack more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4881b-0055-4da9-a770-e760b34042d0",
   "metadata": {},
   "source": [
    "### **Why do we need DeepFool? What problem does it solve?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447a56b-8667-4688-bbec-2a9911d1aee2",
   "metadata": {},
   "source": [
    "All previous **Gradient-Based Attacks** (FGSM, BIM, PGD) follow the same logic:\n",
    "üëâ They **move in the gradient direction** without thinking about the **best** way to cross the decision boundary.\n",
    "\n",
    "**The Problem with PGD & Auto-PGD:**\n",
    "- They **do not look at the geometry of the decision boundary**.\n",
    "- They **blindly apply perturbations** in a fixed or adaptive manner but may use more perturbations than needed.\n",
    "- They **waste steps and energy** by not optimizing the attack path.\n",
    "\n",
    "### **DeepFool brings a new perspective!**\n",
    "DeepFool is the first attack to focus on **finding the *minimal* perturbation** required to cross the decision boundary.\n",
    "\n",
    "It answers the question:  **What is the smallest possible change I can apply to fool the model?**\n",
    "\n",
    "\n",
    "DeepFool solves this problem by:\n",
    "- **Finding the closest decision boundary** instead of blindly following the gradient.\n",
    "- **Applying only the necessary perturbation** to cross this boundary.\n",
    "- **Stopping immediately** once the adversarial image is misclassified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d08825-7b07-4df2-9003-e281cc0f8bbf",
   "metadata": {},
   "source": [
    "## How Does DeepFool Stop Automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f58043-301e-4131-9231-8b855e79ec0c",
   "metadata": {},
   "source": [
    "Unlike PGD and Auto-PGD, **DeepFool doesn't require a fixed number of iterations**.\n",
    "\n",
    "Instead, DeepFool:\n",
    "1. **Checks the classification at each step (that's why it's a White-Box Attack ! It used the model to classify at each iteration)**.\n",
    "2. **If the class has changed, it stops immediately**.\n",
    "3. **If not, it keeps moving in the optimal direction**.\n",
    "\n",
    "**This makes DeepFool one of the most efficient gradient-based attacks** because it **only applies the minimal necessary perturbation**.\n",
    "\n",
    "*NOTE*: Imagine a scenario where DeepFool doesn't success to create adversarial image. It will run indefinitely. To avoid this scenario, we will put the parameter `max_iter` which will specify the maximum number of iteration. After this number, it will simply stop the process. Even another class is not reached.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd79114-3e1d-47aa-ba81-49fbaab85b4a",
   "metadata": {},
   "source": [
    "### Weaknesses\n",
    "\n",
    "However, because DeepFool **don't search to maximise the confidence score** of the error classification, it just takes de first bad classification get, even if :\n",
    "* The target class has a **weak confidence** score (i,e. 30%)\n",
    "* The original class is **still close** in the confidence score (i,e. 25%)\n",
    "\n",
    "**Because, after the moment that the classification is wrong, DeepFool consider that his job is done !**\n",
    "\n",
    "This weakness will be solve by another attack that we'll see later ;)\n",
    "\n",
    "--> DeepFool is efficient to create **minimal perturbations**, but his adversarial examples can be **unstable and easy to correct** !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4224cb-fa26-4a37-aab5-168c536ac7f2",
   "metadata": {},
   "source": [
    "## Mathmatics Formula & Step Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008eebf-77a7-46bd-b0c1-7329015eaf13",
   "metadata": {},
   "source": [
    "Unlike PGD, which moves with a **fixed step size** or an **adaptive step size**, DeepFool continuously estimates **the minimal distance to the decision boundary**.\n",
    "\n",
    "At each step **t**, the adversarial image is updated using the formula:\n",
    "\n",
    "**$$x_{t+1} = x_t + \\frac{- f(x_t)}{\\|\\nabla f(x_t) \\|^2} \\nabla f(x_t)$$**\n",
    "\n",
    "Where:\n",
    "- **$ x_t $** is the current adversarial image at iteration **t**.\n",
    "- **$ f(x_t) $** is the model's classification score for the current class.\n",
    "- **$ \\nabla f(x_t) $** is the gradient of the model (the direction where classification changes the most).\n",
    "- **$ \\frac{- f(x_t)}{\\|\\nabla f(x_t) \\|^2} $** Normalization factor to prevent overly large perturbations.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we divide by $\\|\\nabla f(x_t) \\|^2$ ?**\n",
    "Because it **normalizes** the gradient direction, making sure the step size is proportional to the confidence of the model, instead of just blindly following the steepest descent.\n",
    "\n",
    "**Why do we multiply by $- f(x_t)$ ?**\n",
    "This term **measures the distance to the decision boundary, so the perturbation is **scaled accordingly**. The closer we are to the boundary, the smaller the step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea2168-a11f-4c6b-8231-ffbad443d7ed",
   "metadata": {},
   "source": [
    "### Example with a Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46def103-5295-4b47-9466-200d6a74efa3",
   "metadata": {},
   "source": [
    "Let‚Äôs take **a single neuron** in a binary classification model that distinguishes between **cats** and **dogs**.\n",
    "\n",
    "#### **Initial Conditions:**\n",
    "- **Input $x_t = 0.6$** ‚Üí A feature value representing an image characteristic (e.g., pixel intensity, normalized between 0 and 1).\n",
    "- **Score $f(x_t) = 0.2$** ‚Üí The model still classifies it as a **cat**, but it is **close to the decision boundary**.\n",
    "- **Gradient $\\nabla f(x_t) = 0.5$** ‚Üí The direction in which the score changes the most when modifying $x_t$.\n",
    "\n",
    "#### **Applying the DeepFool Formula:**\n",
    "\n",
    "$$x_{t+1} = x_t + \\left(-\\frac{f(x_t)}{||\\nabla f(x_t)||^2}\\right) \\cdot \\nabla f(x_t)$$\n",
    "\n",
    "\n",
    "1. **Computing the correction factor** (how far we are from the decision boundary):  \n",
    "$$\n",
    "   -\\frac{f(x_t)}{||\\nabla f(x_t)||^2} = -\\frac{0.2}{(0.5)^2} = -\\frac{0.2}{0.25} = -0.8\n",
    "$$\n",
    "\n",
    "2. **Updating the input value \\( x_t \\)**:  \n",
    "$$\n",
    "   x_{t+1} = 0.6 + (-0.8 \\times 0.5)\n",
    "$$\n",
    "$$\n",
    "   x_{t+1} = 0.6 - 0.4\n",
    "$$\n",
    "$$\n",
    "   \\mathbf{x_{t+1} = 0.2}\n",
    "$$\n",
    "\n",
    "**Result:**  \n",
    "The attack **efficiently moves** towards the decision boundary by reducing $x$, and the model might now misclassify the image as a **dog** instead of a **cat**. If not, we repeat another iteration by following the same process. The step size will be surely lower.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61e1db-917e-483e-96c5-8d3a4601baa9",
   "metadata": {},
   "source": [
    "## Analogy: Down a Mountain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d42902-3e76-4a32-89bd-98eb066c0f76",
   "metadata": {},
   "source": [
    "Imagine you want to **go down a moutain (change class in the classification)**:\n",
    "\n",
    "With PGD/Auto-PGD (brute-force method):\n",
    "- You descend always following the **steepest** slope.\n",
    "- But, you can take a longer route if the seepest slope doesn't lead straight to the bottom\n",
    "\n",
    "*Problem:*\n",
    "* Sometimes the **steepest** gradient makes you take unecessary detours. You take more steps than necessary. The attack is stronger, but less optimised.\n",
    "\n",
    "With DeepFool (Optimised Method):\n",
    "- You look at the map and **find the shortest route** to the valley.\n",
    "- You take only the **minimum number of steps necessary**\n",
    "- You get there **faster with less effort** !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2bf243-7e47-45cf-9e91-ad0c4e1846c6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d40a7b-e0ab-46f0-baef-4e7baf2b494d",
   "metadata": {},
   "source": [
    "DeepFool doesn't just follow the steepest descent (like PGD).\n",
    "\n",
    "Instead, **it calculates the exact distance to the decision boundary** and makes an **optimal step size adjustment**.\n",
    "\n",
    "Moreover, it checks the **classification at each iteration to stop his process if the classification is wrong** !\n",
    "\n",
    "This reduces the **number of iterations** while keeping **perturbations minimal**, making the attack more efficient and harder to defend against.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94731-0d90-4d19-8222-191fb4c647ff",
   "metadata": {},
   "source": [
    "# Code\n",
    "### **AUTHOR** : Maxence QUINET (University Of Luxembourg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92716a-5538-4941-ae1e-585ff0cebd21",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae61f74-994a-44c7-82fe-f09fdb63ea5b",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "Please ensure all dependencies are installed using the `requirements.txt` file.\n",
    "\n",
    "For additional environment setup details, refers to **\"environment_configuration.txt\"**.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e3f54-45df-43c5-9a1f-956d88b53d62",
   "metadata": {},
   "source": [
    "Below are the required **libraries and frameworks** for running Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e6787-99cc-4fc7-9d64-6e8cf224e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # (Reduce TensorFlow logs)\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712eb9f-41ed-426b-b796-2eb7c3fe1ea5",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "**Machine Learning & Neural Network Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce5cd-cabe-470f-a557-d6bedb48499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6f62f-0f79-4fc6-ad7e-6e27035804a5",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**Datasets & Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb4172-8e19-4049-b240-c74387c74f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4ab2c-52cb-428a-b51e-c4a2f73160ea",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**Adversarial Robustness Toolbox (ART)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef553-adcb-4d14-b65f-7870359abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier, PyTorchClassifier\n",
    "from art.attacks.evasion import DeepFool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e44a1-8cfe-4143-9ca3-37645ddf5b95",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "**Vision Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b328a53-2e94-4c2e-b388-8e3e1c037d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb9ee5-13ef-4bb2-bde7-0cc3cc77c9ec",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "**imagenet_stubs** \n",
    "\n",
    "imagenet_stubs is a small dataset available at this link : https://github.com/nottombrown/imagenet-stubs\n",
    "\n",
    "#### Why use it ?\n",
    "\n",
    "* Ideal for **testing adversarial attacks quickly** before applying them on larger datasets.\n",
    "* Provides **two useful functions**:\n",
    "  - `label_to_name(index)` --> Convert an ImageNet label (number) to its corresponding name\n",
    "  - `name_to_label(name)` --> Convert an ImageNet class name back to its numerical label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aac80-f6d0-4fdc-9a0a-68daf1c11fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagenet_stubs\n",
    "from imagenet_stubs.imagenet_2012_labels import label_to_name, name_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1af9f-493c-4822-9f5c-5ddec18010bb",
   "metadata": {},
   "source": [
    "## Checking PyTorch & TensorFlow Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918348c7-eacd-400a-af9a-7f0c5865df92",
   "metadata": {},
   "source": [
    "### **CUDA & GPU Verification**\n",
    "Since we need **CUDA** for accelerated deep learning computations, we ensure that **PyTorch and TensorFlow** are properly configured with CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12e367-3ac8-4bc2-929b-62fa7e586453",
   "metadata": {},
   "source": [
    "------------------\n",
    "**PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57fe8-8f5e-42ce-a951-c9e580038775",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Versions\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b1229-cfdf-484a-a7fe-3c08be588ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9e593-a5d0-4d4b-a327-a65f933ba887",
   "metadata": {},
   "source": [
    "------------------\n",
    "**TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ae0d0-84da-4ffd-835e-ce2648294735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tensorflow\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df32df-c368-4049-953a-86497c4b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA & CUDNN Version\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372090f-77fd-44d4-be97-05b820efda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b3e77-63a1-4940-8f4a-7b542df3f6d9",
   "metadata": {},
   "source": [
    "## Dataset Selection & Configuration\n",
    "\n",
    "In this section, you can **choose the dataset** you want to use for adversarial attacks:\n",
    "\n",
    "- **MNIST**\n",
    "- **CIFAR10**\n",
    "- **CIFAR100**\n",
    "- **ImageNet**\n",
    "\n",
    "#### NOTE: On the \"DeepFool Paper\" they test MNIST, CIFAR10 & ImageNet datasets !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644150a5-4f2e-4e8c-b03a-28f7e3cc442f",
   "metadata": {},
   "source": [
    "### **Select Your Dataset & Model Configuration**\n",
    "Modify the variables below to **choose the dataset and model**.  \n",
    "\n",
    "The **official DeepFool** paper test the attack on 2 models for each datasets, like LeNet, NIN, CaffeNet, GoogLeNet (Inception) !\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390344aa-5665-4dc2-942e-a0d33ce38fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = \"MNIST\" # OPTIONS : \"MNIST\", \"CIFAR10\", \"CIFAR100\", and \"ImageNet\"\n",
    "\n",
    "selected_cifar10_model = \"standard_resnet\" # OPTIONS : \"standard_resnet\", \"resnet_10x_variant\", \"conv_maxout\"\n",
    "selected_mnist_model = \"simple_cnn\" # OPTIONS : \"simple_cnn\", \"shallow_softmax\", \"maxout\", \"logistic\" (For MNIST only)\n",
    "\n",
    "selected_attack = \"DeepFool\" # Used for report name only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc0664-be37-43cc-afc2-f5dfbbf25a85",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "**Define class labels for each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed70d3-60ac-4818-bc90-abaa03e8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the ancestors_name & ancestors_label corresponding to the selected dataset.\n",
    "\n",
    "# Note: All labels are available on Internet. They are not created from us. They are official, often in a .json format.\n",
    "if selected_dataset == \"CIFAR10\":\n",
    "    # Correspondance between name & label for CIFAR10\n",
    "    ancestors_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    # Correspondance between name & label for CIFAR100\n",
    "    ancestors_name = ['apple', 'bridge', 'castle', 'elephant', 'house', 'orange', 'shark', 'table', 'tractor', 'whale']\n",
    "    ancestors_label = ['0', '12', '17', '31', '37', '53', '73', '84', '89', '95']\n",
    "\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['abacus', 'acorn', 'baseball', 'broom', 'brown_bear', 'canoe', 'hippopotamus', 'llama', 'maraca', 'mountain_bike']\n",
    "    ancestors_label = ['398', '988', '429', '462', '294', '472', '344', '355', '641', '671']\n",
    "\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "else:\n",
    "    print(f\"Your {selected_dataset} doesn't exist. Please provide an existing dataset between these choices : CIFAR10, CIFAR100, ImageNet & MNIST.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed2cd3-45de-4c8f-8046-d32c53fd4c1a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "**Fix seed to ensure reproducibility (comment to get random results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9afdd-ab88-409b-a161-2ece4778f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23689023-5108-4f89-8534-930e7c4f78e0",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "#### plot_prediction()\n",
    "\n",
    "This function will be used to display the original / attacked images.\n",
    "\n",
    "The function is designed to display the images correctly, depending on the dataset selected, with the following legend:\n",
    "\n",
    "<font color='green'>Green bars</font> = correct classification <br>\n",
    "<font color='red'>Red bars</font> = Attack target classification <br>\n",
    "<font color='blue'>Blue bars</font> = other classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6ad9-e831-4c77-a869-5896bc68e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_name_dynamic(index, dataset):\n",
    "    \"\"\"Retourne le nom du label en fonction du dataset s√©lectionn√©.\"\"\"\n",
    "    if dataset == \"MNIST\":\n",
    "        return str(index)  # For MNIST, the label name is simply the digit\n",
    "    elif dataset == \"ImageNet\":\n",
    "        return label_to_name(index)  # Use the imagenet_stubs function for ImageNet !\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        return ancestors_name[index]  # Return the name from our list\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        return cifar100_labels[index] if 0 <= index < 100 else \"Unknown\" # Return the name from our list \n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c39259-d85c-4231-815c-27a1b05183e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, probs, correct_class=None, target_class=None):\n",
    "    \"\"\"\n",
    "    Displays an image with predictions in the form of coloured bars :\n",
    "    - Green --> Correct Class\n",
    "    - Red --> Target Class\n",
    "    - Blue --> Other Classes\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "    # Display the picture\n",
    "    if selected_dataset==\"MNIST\":\n",
    "        ax1.imshow(img, cmap=\"gray\") # Force the display in gray level for MNIST !\n",
    "        ax1.axis(\"off\")\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "    # Keep the top 10 classes with highest probabilities\n",
    "    top_ten_indexes = list(probs[0].argsort()[-10:][::-1])\n",
    "    top_probs = probs[0, top_ten_indexes]\n",
    "    labels = [label_to_name_dynamic(i, selected_dataset) for i in top_ten_indexes]\n",
    "\n",
    "\n",
    "    # Bar plot creation with color rules defined above\n",
    "    barlist = ax2.bar(range(10), top_probs, color=\"blue\")  # Blue by default\n",
    "\n",
    "    if target_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(target_class)].set_color(\"red\")  # Red if this is the target class\n",
    "\n",
    "    if correct_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(correct_class)].set_color(\"green\")  # Green if this is the correct class\n",
    "\n",
    "    # Plot Graph\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10), labels, rotation=\"vertical\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Top 10 Predictions\")\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e703fb-0c0e-4931-992e-bd74f72969ac",
   "metadata": {},
   "source": [
    "## Step 1: Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484cab-962a-4634-8254-9bc740b92764",
   "metadata": {},
   "source": [
    "## Step 1: Define Parameters\n",
    "\n",
    "*NOTE*: To be faster, a new parameter appear with DeepFool : `nb_grads`\n",
    "\n",
    "As you know, **DeepFool must find the optimal direction** to get out from the current class and reach the closest boundar decision.\n",
    "\n",
    "By default, it will compute the gradient for ALL CLASSES (ex: 1000 with ImageNet), which is obviously **very expensive in computational ressources**.\n",
    "\n",
    "`nb_grads` will **limit this number of classes**. So if we say : `nb_grads=10`. DeepFool will consider **ONLY the 10 most probable classes instead of all classes !**\n",
    "\n",
    "It saves a lot of time and a lot of computational ressources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930689ae-64a9-4395-be5d-b152868d42ec",
   "metadata": {},
   "source": [
    "### **DeepFool Paper Parameters**\n",
    "In the **original DeepFool paper**, they tested 4 datasets : ImageNet, MNIST, CIFAR100 & CIFAR10. \n",
    "\n",
    "Now, it's useless to define `epsilon_step` because it will be handle dynamically.\n",
    "\n",
    "However, you can set up a value for `epsilon_step` . It will be considered as an `initial value`.\n",
    "\n",
    "**You can modify these values below to test different attack configurations**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523f423-ad6e-420d-8773-c32059a742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFool Paper Research Parameters\n",
    "epsilon = 8/255 # Overshoot parameter (small value to avoid going to far)\n",
    "# epsilon_step = ??? # Epsilon_step is now handle dynamically since Auto-PGD\n",
    "num_steps = 100 # Maximum number of iterations\n",
    "# norm = np.inf # Define your norm : np.inf = L_inf / 1 = L1 / 2 = L2 --> NO NORM WITH DEEPFOOL\n",
    "# restart = 1 # Number of restart that you allow --> NO RESTART WITH DEEPFOOL\n",
    "\n",
    "nb_grads = 10 # Number of most probable class gradients to consider.\n",
    "\n",
    "print(f\"Selected Attack: {selected_attack} | Dataset: {selected_dataset} | Epsilon: {epsilon} | Epsilon Step: Dynamics | Maximum Iterations: {num_steps} | Nb_grads: {nb_grads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea443-adc3-480c-8769-94cbc0e4325d",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "**Later, we'll see what EoT is. If you don't know what is EoT, skip this sub-section**\n",
    "\n",
    "*If you want to test EoT Transformation, find parameters below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5c68e-b441-4917-b6e3-6d149ea39d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EoT Transformation\n",
    "angle_max = 22.5 # Rotation angle used for evaluation in degrees\n",
    "eot_angle = angle_max # Maximum angle for sampling range in EoT rotation, applying range [-eot_angle, eot_angle]\n",
    "eot_samples = 10 # Number of samples with random rotations in parallel per loss gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcc897-543a-4c2a-a813-b3ea9699161c",
   "metadata": {},
   "source": [
    "### Dataset-Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101319c-706d-496f-86d1-b745b7834a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet has 1000 classes, CIFAR100 100 classes, and CIFAR10 & MNIST has 10 classes.\n",
    "nb_classes = 1000 if selected_dataset == \"ImageNet\" else 100 if selected_dataset == \"CIFAR100\" else 10\n",
    "\n",
    "# ImageNet Images Dimension : (299,299,3), CIFAR10 & CIFAR100 : (32,32,3), and MNIST : (28,28,1)\n",
    "input_shape = (299, 299, 3) if selected_dataset == \"ImageNet\" else (32, 32, 3) if \"CIFAR\" in selected_dataset else (28, 28, 1)\n",
    "\n",
    "# ImageNet use often a specific preprocessing. For the others dataset, it still an adapted normalisation (0,1)\n",
    "preprocessing = (0.5, 0.5) if selected_dataset == \"ImageNet\" else (0.0, 1.0)  # Normalisation adapt√©e\n",
    "\n",
    "# Clip values \n",
    "clip_values = (0.0, 1.0)  # Same for all datasets\n",
    "\n",
    "# Target Class Definition (You can change, here are just some examples)\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    y_target = np.array([641])  # \"maraca\"\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    y_target = np.array([3])  # \"bear\"\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    y_target = np.array([1])  # \"automobile\"\n",
    "else:  # MNIST\n",
    "    y_target = np.array([np.random.randint(0, 10)])  # random digit between 0 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787fe1-db8a-4d32-aa92-2f11ecd764af",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset Data & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73add696-3799-40c0-ac09-31ed827579e6",
   "metadata": {},
   "source": [
    "In this step, we **load all dataset images and their labels into memory**.\n",
    "\n",
    "#### **How does it work?**\n",
    "1. We retrive the dataset path (`datasets/selected_dataset/`).\n",
    "2. We read all images from the dataset folders.\n",
    "3. We **normalize** the images (scale pixel values between `[0, 1]`).\n",
    "4. We store **both images and labels** for further processing.\n",
    "\n",
    " -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5041c04-64ef-435c-9cb9-2374e84ca1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Initializations\n",
    "x_all, y_all, original_images = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e9aff-599b-46b4-80fa-b06e92f9673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get our dataset path in our computer to keep all pictures and put them into our lists.\n",
    "dataset_path = os.path.join(\"datasets\", selected_dataset)\n",
    "# Check\n",
    "assert(dataset_path==\"datasets/\"+selected_dataset) # If nothing : It's ok. Otherwise, you will get an error if the dataset path doesn't exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b009a5-c170-4d80-8d6b-4de6e1695908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from the selected dataset\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        continue\n",
    "    \n",
    "    for img_file in sorted(os.listdir(class_path)):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        if selected_dataset == \"MNIST\":\n",
    "            im = load_img(img_path, color_mode=\"grayscale\", target_size=(28, 28))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        elif selected_dataset == \"ImageNet\":\n",
    "            im = load_img(img_path, target_size=(299, 299))\n",
    "            im_array = img_to_array(im)\n",
    "\n",
    "        elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "            im = load_img(img_path, target_size=(32, 32))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        x = (im_array / 255.0).astype(np.float32)\n",
    "        \n",
    "        x_all.append(x)\n",
    "        y_all.append(int(class_label))\n",
    "        original_images.append(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dd7e1-03f5-42db-a0c6-48fb139331f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "-----------------------------------------------------\n",
    "#### Display Dataset (Optional)\n",
    "**You can choose to display all images or only one image per class)**\n",
    "\n",
    "#### How to enable visualization ?\n",
    "- To display **ALL images** --> **Uncomment the loop bellow**.\n",
    "- To display **ONLY 1 image per class** --> **Set `display_all_images = False`**.\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d59b-406f-4883-93bb-f0961d426afc",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying of the 100 pictures (can be long, you can modify the code to display only 1 picture per class if you want)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        print(class_path)\n",
    "        print(\"No os Path\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Class : {class_name} (Label: {class_label})\")\n",
    "    \n",
    "     # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = sorted(os.listdir(class_path))[:1] if not display_all_images else sorted(os.listdir(class_path))\n",
    "    # Go through the 10 pictures of each classes\n",
    "    for img_file in images_to_show:\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        # Load & Normalize the picture\n",
    "        im = load_img(img_path, target_size=(299, 299))\n",
    "        im_array = img_to_array(im)\n",
    "\n",
    "        # Displaying all pictures\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(im_array.astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Class: {class_name} | {img_file}\", fontsize=10, fontweight=\"bold\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{img_file} well displayed in : {class_name}\")\n",
    "\n",
    "print(f\"All of the {len(ancestors_name)} classes & their images has been displayed !\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7c65f-03a9-44f8-913a-09d2957ef192",
   "metadata": {},
   "source": [
    "### Convert to Numpy Arrays for TensorFlow\n",
    "Since TensorFlow requires NumPy arrays, we convert our lists into arrays.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bde4b5-0d8d-4366-8a99-3c770534f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a numpy array\n",
    "x_all = np.array(x_all)\n",
    "y_all = np.array(y_all).reshape(-1, 1)\n",
    "\n",
    "# Check\n",
    "#for img_x, img_y in zip(x_all, y_all):\n",
    "#    print(f\"x_all shape: {x_all.shape}\")  # (N, H, W, C)\n",
    "#    print(f\"y_all shape: {y_all.shape}\")  # (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dded0f-4832-4d83-9055-c24bb6c7fd57",
   "metadata": {},
   "source": [
    "## Step 3 : Load Model & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411ad0a-c669-417d-9a28-b86681dd231c",
   "metadata": {},
   "source": [
    "### 1. Loading Dataset for Model Training\n",
    "Before creating the model, we **load and preprocess** the dataset to ensure it is correctly formatted for TensorFlow.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc8c37-3fb4-48d1-85b9-2a7175b4e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_dataset == \"MNIST\":\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)\n",
    "    \n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")\n",
    "\n",
    "    # We reproduce the list of all classes of CIFAR100\n",
    "    cifar100_labels = [\n",
    "    \"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\",\n",
    "    \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\",\n",
    "    \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"crab\", \"crocodile\", \"cup\", \"dinosaur\",\n",
    "    \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"computer_keyboard\",\n",
    "    \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\",\n",
    "    \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\",\n",
    "    \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\", \"rose\", \"sea\",\n",
    "    \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\", \"squirrel\", \"streetcar\", \"sunflower\",\n",
    "    \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\", \"train\", \"trout\", \"tulip\",\n",
    "    \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"\n",
    "]\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51714d-e20b-4bab-8d09-5fc35f30e8cc",
   "metadata": {},
   "source": [
    "### 2. Model Selection & Architecture\n",
    "On the **DeepFool** paper, they have tested the attack on LeNet, GoogLeNet(Inception), NIN, CaffeNet etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d5538-1404-45c5-94d6-d98aa3bd3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definind a custom Maxout layer\n",
    "class MaxoutLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, **kwargs):\n",
    "        super(MaxoutLayer, self).__init__(**kwargs)\n",
    "        self.num_units = num_units\n",
    "        self.dense = Dense(num_units * 2)  # We create twice more neurons to have the max\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)  # Apply Linear Transformation\n",
    "        x = tf.reshape(x, (-1, self.num_units, 2))  #Group neurons by pair\n",
    "        return tf.reduce_max(x, axis=-1)  # Take the maximum from each pair\n",
    "\n",
    "# Creating the Maxout model\n",
    "def build_maxout_model():\n",
    "    input_layer = Input(shape=(28, 28, 1))  # Input: MNIST (28, 28, 1)\n",
    "    flattened = Flatten()(input_layer)\n",
    "\n",
    "    # Add Maxout Layers\n",
    "    maxout_1 = MaxoutLayer(256)(flattened)\n",
    "    maxout_2 = MaxoutLayer(128)(maxout_1)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = Dense(10, activation='softmax')(maxout_2)\n",
    "\n",
    "    # Creating Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Convolutional Maxout Network\n",
    "def build_conv_maxout_model(num_classes):\n",
    "    input_layer = Input(shape=(32, 32, 3))  # Input CIFAR-10  et CIFAR100 (32x32, RGB) so (32, 32, 3)\n",
    "\n",
    "    # Classic Convolutional Layers\n",
    "    conv1 = Conv2D(64, (3, 3), padding=\"same\", activation=None)(input_layer)  # No Activation (Handle by Maxout)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), padding=\"same\", activation=None)(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Add Maxout Layers\n",
    "    flattened = Flatten()(pool2)\n",
    "    maxout_1 = MaxoutLayer(256)(flattened)\n",
    "    maxout_2 = MaxoutLayer(128)(maxout_1)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = Dense(num_classes, activation='softmax')(maxout_2)\n",
    "\n",
    "    # Creating Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56f475-012b-43a9-b382-920db018eeaa",
   "metadata": {},
   "source": [
    "#### The code below represent the Simple Convolutional Neural Network used in the PGD Paper for **MNIST** !\n",
    "\n",
    "**The official code from the paper is available at this GitHub Link : https://github.com/MadryLab/mnist_challenge/blob/master/model.py**\n",
    "\n",
    "This code is more compact because we use a higher version of tensorflow, but the model is 100% the same (you can compare if you want)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4e4af-9354-4c98-abfa-affb0a270c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_simple_CNN_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for MNIST\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484d107-3a97-41e2-b2f9-2cac8cd20ac5",
   "metadata": {},
   "source": [
    "#### The code below represent the ResNet Model & in Variant 10 times larger used in the PGD Paper for **CIFAR10** !\n",
    "\n",
    "**The official code from the paper is available at this GitHub Link : https://github.com/MadryLab/cifar10_challenge/blob/master/model.py**\n",
    "\n",
    "This code is more compact because we use a higher version of tensorflow, but the model is 100% the same (you can compare if you want)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de8e0f-1a4c-41e8-a88e-24e104cb905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET STANDARD MODEL\n",
    "def build_resnet_model():\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False,  # Don't include the classification for ImageNet. When True --> Conserve the classification final layer pre-trained for ImageNet (1000 classes)\n",
    "        input_shape=(32, 32, 3),\n",
    "        weights=None  # We train the model from scratch\n",
    "    )\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f5cb1-945e-49e7-85cb-2d22de44f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET VARIANT 10 TIMES LARGER\n",
    "def build_wide_resnet():\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, # Don't include the classification for ImageNet. When True --> Conserve the classification final layer pre-trained for ImageNet (1000 classes)\n",
    "        input_shape=(32, 32, 3),\n",
    "        weights=None\n",
    "    )\n",
    "\n",
    "    # We increase the number of filters by 10\n",
    "    for layer in base_model.layers:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            layer.filters *= 10  \n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aed09b-80e0-48cc-b4c2-3659ebe7612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= IMAGENET =============\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    print(f\"SELECTED MODEL : InceptionV3.\") \n",
    "    model = InceptionV3(include_top=True, weights='imagenet', classifier_activation='softmax')\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# ============= MNIST =============\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    \n",
    "    # SHALLOW MAX CLASSIFER\n",
    "    if selected_mnist_model == \"shallow_softmax\":\n",
    "        print(f\"SELECTED MODEL : {selected_mnist_model}.\") \n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=input_shape), # MNIST has a shape of (28, 28)\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(10, activation='softmax') # 10 classes for MNIST (0 to 9)\n",
    "        ])\n",
    "\n",
    "    # MAXOUT\n",
    "    elif selected_mnist_model == \"maxout\":\n",
    "        print(f\"SELECTED MODEL : {selected_mnist_model}.\")\n",
    "        model = build_maxout_model()\n",
    "        \n",
    "    # LOGISTIC REGRESSION\n",
    "    elif selected_mnist_model == \"logistic\":\n",
    "        print(f\"SELECTED MODEL : {selected_mnist_model}.\")\n",
    "        # Logistic Regression = one dense simple layer with softmax\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=(28, 28)),  # MNIST has a shape of (28, 28)\n",
    "            Dense(10, activation='softmax')  # 10 classes (0-9)\n",
    "        ])\n",
    "\n",
    "    # SIMPLE CNN --> THE MODEL USED IN THE OFFICIAL PGD PAPER. OFFICIAL CODE AVAILABLE AT THIS GITHUB LINK  : https://github.com/MadryLab/mnist_challenge/blob/master/model.py\n",
    "    elif selected_mnist_model == \"simple_cnn\":\n",
    "        # Create the model\n",
    "        model = build_simple_CNN_model()\n",
    "\n",
    "        # (Optional) Print the model summary\n",
    "        model.summary()\n",
    "    else: \n",
    "        raise ValueError(f\"Error: Model '{selected_mnist_model}' is not recognized between : shallow_softmax, maxout and logistic.\")\n",
    "\n",
    "# ============= CIFAR10/CIFAR100 =============\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    if selected_cifar10_model == \"standard_resnet\":\n",
    "        print(f\"SELECTED MODEL: Standard ResNet\")\n",
    "        resnet_model = build_resnet_model()\n",
    "        resnet_model.summary()\n",
    "        \n",
    "    elif selected_cifar10_model == \"resnet_x10_variant\":\n",
    "        print(f\"SELECTED MODEL: ResNet 10x larger Variant\")\n",
    "        wide_resnet_model = build_wide_resnet()\n",
    "        wide_resnet_model.summary()\n",
    "    else:    \n",
    "        print(f\"SELECTED MODEL : Convolutional Maxout Network.\")\n",
    "        # Creating Model\n",
    "        model = build_conv_maxout_model(nb_classes)\n",
    "\n",
    "# ============= ERROR =============\n",
    "else:\n",
    "    raise ValueError(f\"Error: Dataset '{selected_dataset} not recognized. Please ensure to use one of this dataset : ImageNet, CIFAR10, CIFAR100 or MNIST.'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778a0f-f1d3-457b-a7a2-4fd94d5eb502",
   "metadata": {},
   "source": [
    "### 3. Model Compilation & Training\n",
    "Once the model is selected, we **compile and train** it.\n",
    "\n",
    "- **For ImageNet**, the model is already pretrained\n",
    "- **For other datasets**, a quick training step (5-10 epochs) is performed.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448f8f2-ef74-4986-bb1c-07fb14edd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "if selected_dataset != \"ImageNet\":\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy' if selected_dataset != \"MNIST\" else 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "if selected_dataset != \"ImageNet\":\n",
    "    epochs = 5 if selected_dataset == \"MNIST\" else 10  # Quick training\n",
    "    print(f\"Training model on {selected_dataset} for {epochs} epochs...\")\n",
    "    model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))\n",
    "    print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1fdd7-9038-4b6d-9326-227751a2a427",
   "metadata": {},
   "source": [
    "## Step 4 : Create the ART Classifier & Configure the Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53db541-c3ca-4a5f-8d75-c7dd4557d65d",
   "metadata": {},
   "source": [
    "Now that the model is **trained and ready**, we integrate it into **ART (Adversarial Robustness Toolbox)**.\n",
    "\n",
    "#### What is happening here ?\n",
    "1. We **create a classifier** for ART based on the trained model\n",
    "2. We **define an adversarial attack** (FGSM in this case)\n",
    "3. The attack can be **targeted or untargeted**, and parameters are fully configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597176e7-cdfb-4cc7-8f51-1f74f1a83f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model,\n",
    "                                    nb_classes=nb_classes,\n",
    "                                    loss_object=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                                    preprocessing=preprocessing,\n",
    "                                    preprocessing_defences=None,\n",
    "                                    clip_values=clip_values,\n",
    "                                    input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfaf70-dae6-486c-8c62-41ce8639136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = DeepFool(classifier=classifier,  \n",
    "                  max_iter=num_steps,  \n",
    "                  epsilon=epsilon,  \n",
    "                  nb_grads=nb_grads,\n",
    "                  verbose=True)  # Consider only the top `nb_grads` most probable classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34060c84-e2de-438b-995b-119d73d6b112",
   "metadata": {},
   "source": [
    "## Step 5 : Predict Clean (Original) Images BEFORE the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c927354-cc5b-45d0-bcb3-65f152c94efd",
   "metadata": {},
   "source": [
    "Before applying any attack, we **predict the clean images** with our trained model.\n",
    "\n",
    "#### What happens here ?\n",
    "1. We run the classifier on all images **before the attack**.\n",
    "2. We display the **top-10 predictions** for each image.\n",
    "3. You can choose to **display all images or only one per class**.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54e745-a5c0-485a-8a21-b30f3f08cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Clean Images\n",
    "y_pred_clean_all = classifier.predict(np.array(x_all))\n",
    "\n",
    "# Check prediction shape\n",
    "print(\"Shape of Clean Predictions:\", y_pred_clean_all.shape)  # Expected (N, nb_classes)\n",
    "\n",
    "# Summarize Prediction\n",
    "top1_correct = np.mean(np.argmax(y_pred_clean_all, axis=1) == y_all.flatten()) * 100\n",
    "print(f\"Top-1 Accuracy on Clean Images: {top1_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570014ad-2367-4279-8515-96ff1eb668e3",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "#### Display Clean Images & Predictions\n",
    "You can **choose whether to display all images or just one per class**.\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a7ed8-e5c7-4a3b-ba27-bc7e4d6e4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying Clean Images with Predictions\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    # Get all images from this class\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No Images found for {class_name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_all[index]),  # Original clean image\n",
    "            y_pred_clean_all[index].reshape(1, -1),  # Reshaped prediction\n",
    "            correct_class=y_all[index],  # True class\n",
    "            target_class=None  # No target class for clean images\n",
    "        )\n",
    "        print(f\"Image {index} displayed for class: {class_name}\")\n",
    "\n",
    "print(f\"\\n All {len(x_all)} clean images have been processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526ff66-f8ff-499a-b44d-5ed770fdd749",
   "metadata": {},
   "source": [
    "## Step 6: Generate and Evaluate Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7f94a-fb49-48f2-a6b1-a4ca9cb6dab6",
   "metadata": {},
   "source": [
    "Now, we **generate adversarial examples** and evaluate the effectiveness of the attack.\n",
    "\n",
    " **What happens here?**\n",
    "1. We **generate adversarial examples** using the selected attack.\n",
    "2. We **save the adversarial images** for later analysis. (optional)\n",
    "3. We **evaluate the attack's success** (accuracy, confidence scores, and performance metrics).\n",
    "4. We **generate a detailed report** summarizing the attack results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490c430-ba96-4d24-b09f-7ce8a808825d",
   "metadata": {},
   "source": [
    "TensorFlow‚Äôs GradientTape is used to compute gradients efficiently. However, DeepFool requires multiple gradient calculations, which can cause high memory usage. To optimize this, we replace GradientTape with a lightweight version that improves memory management and speeds up computation.\n",
    "\n",
    "But it could be nice to check if its works better without this new version of GradientTape !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c91f4-6b2b-445b-accb-44c13f640d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALLOWS YOU TO NOT HAVE THE GRADIENTTAPE WARNING, AND SAVE TIME AND CPU RESSOURCES.\n",
    "# Main Improvement : Improve Memory Management.\n",
    "\n",
    "# Modified version of GradientTape without persistent=True\n",
    "class CustomGradientTape(tf.GradientTape):\n",
    "    def __init__(self, persistent=False, *args, **kwargs):\n",
    "        super().__init__(persistent=persistent, *args, **kwargs)\n",
    "\n",
    "# Replace GradientTape by the modified version\n",
    "tf.GradientTape = CustomGradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e17c6-ac35-4194-a7d9-c09ae1aa8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Progress bar for attack generation\n",
    "\n",
    "target_labels = np.tile(y_target, (len(x_all), 1))  # Repeat y_target for each image (same target for all images)\n",
    "\n",
    "if attack.targeted:\n",
    "    print(\"SCENARIO ATTACK : TARGETED\")\n",
    "    # Attack all images with the target class\n",
    "    x_adv_all = attack.generate(x=x_all, y=target_labels)\n",
    "else:\n",
    "    print(\"SCENARIO ATTACK : UNTARGETED\")\n",
    "    # No Target Class\n",
    "    x_adv_all = attack.generate(x=x_all)\n",
    "\n",
    "    \n",
    "# Shape Check of the Adversarial Examples\n",
    "print(\"Shape of Adversarial Examples:\", x_adv_all.shape)  # Expected (N, H, W, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff0f8d-9591-441b-af62-c1a038141487",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "**Do you want to save all adversarial images?**  \n",
    "- **YES** ‚Üí Uncomment the saving function below.\n",
    "- **NO** ‚Üí Comment the function to skip saving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b885-6864-4199-9a58-632a74a93a13",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Define the save path for adversarial images\n",
    "adv_save_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset)\n",
    "os.makedirs(adv_save_path, exist_ok=True)  \n",
    "\n",
    "# Iterate through all classes to save adversarial images\n",
    "class_counters = {class_name: 1 for class_name in ancestors_name}  # Dictionary to track image indices per class\n",
    "\n",
    "for adv_img, class_label in zip(x_adv_all, y_all.flatten()):  # Ensure y_all is 1D\n",
    "    # Find the class name corresponding to the label\n",
    "    if str(class_label) not in ancestors_label:\n",
    "        print(f\"Label {class_label} not found in ancestors_label, skipping image.\")\n",
    "        continue  \n",
    "\n",
    "    class_index = ancestors_label.index(str(class_label))\n",
    "    class_name = ancestors_name[class_index]\n",
    "\n",
    "    # Determine the subfolder for the class\n",
    "    class_folder = os.path.join(adv_save_path, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True) \n",
    "\n",
    "    # Generate a unique filename with a counter (e.g., abacus1_adv.jpeg, abacus2_adv.jpeg, ..., acorn1_adv.jpeg, ...)\n",
    "    img_filename = f\"{class_name}{class_counters[class_name]:02d}_adv.jpeg\"\n",
    "    img_path = os.path.join(class_folder, img_filename)\n",
    "\n",
    "    # Convert and save the image\n",
    "    img = array_to_img(adv_img)\n",
    "    img.save(img_path, \"JPEG\")\n",
    "\n",
    "    print(f\"Image saved : {img_path}\")\n",
    "\n",
    "    # Increment the counter for this class\n",
    "    class_counters[class_name] += 1\n",
    "\n",
    "print(f\"\\nAll  {len(x_adv_all)} adversarial images have been successfully saved!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254fe32-85b2-4c37-85ce-1009bbe05e83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate Adversarial Example\n",
    "\n",
    "We now evaluate the adversarial examples by:\n",
    "- Measuring the **model's accuracy** on these images.\n",
    "- Computing the **confidence score** of predictions.\n",
    "- Generating a **visual comparison** between clean and adversarial images.\n",
    "\n",
    "---\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17136d23-7325-40dd-a592-aeffd89df52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on adversarial images\n",
    "y_pred_adv_all = classifier.predict(x_adv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4390776-4d46-4ae0-8dba-3e033b99c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Display Adversarial Examples (Optional)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "    \n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No images found for {class_name}skipping...\")\n",
    "        continue\n",
    "\n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_all[index]),\n",
    "            y_pred_adv_all[index].reshape(1, -1),\n",
    "            correct_class=y_all[index],\n",
    "            target_class=target_labels[index]\n",
    "        )\n",
    "        print(f\"Adversarial Image {index} displayed for class: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d4cd8-dd10-4ac8-9b8d-f9e82797bb5b",
   "metadata": {},
   "source": [
    "### Compute Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f72e-928c-4d21-8da9-e69c53add45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence score\n",
    "confidence_scores = np.max(y_pred_clean_all, axis=1)\n",
    "average_confidence = np.mean(confidence_scores) * 100\n",
    "\n",
    "# Compute Tok-K Accuracy\n",
    "def compute_accuracy(predictions, true_labels, top_k=1):\n",
    "    top_k_preds = np.argsort(predictions, axis=1)[:, -top_k:]\n",
    "    match = np.any(top_k_preds == np.array(true_labels).reshape(-1, 1), axis=1)\n",
    "    return np.mean(match) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a0b18-9939-4b00-80a7-5dd7c57f141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_top1 = compute_accuracy(y_pred_clean_all, y_all, top_k=1)\n",
    "clean_top5 = compute_accuracy(y_pred_clean_all, y_all, top_k=5)\n",
    "adv_top1 = compute_accuracy(y_pred_adv_all, y_all, top_k=1)\n",
    "adv_top5 = compute_accuracy(y_pred_adv_all, y_all, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f2c9-093e-45ec-b3b7-e4079e242270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Performance Results\n",
    "attack_name = \"deepfool\" if isinstance(attack, DeepFool) else \"fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98b0b-0281-45c0-8d8b-a56b1ab187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Selected Attack: {selected_attack} | Dataset: {selected_dataset} | Epsilon: {epsilon} | Epsilon Step: Dynamics | Number of Iterations: {num_steps}\")\n",
    "print(f\"Clean Images : Top-1 : {clean_top1:.2f}% | Top-5 : {clean_top5:.1f}%\")\n",
    "print(f\"Adv. Images  : Top-1 : {adv_top1:.2f}% | Top-5 : {adv_top5:.1f}%\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Confidence Score: {average_confidence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22d39a-2ee4-4ae4-ab55-4e63057fd0a0",
   "metadata": {},
   "source": [
    "### Generate a Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584bf76-61d4-46cd-89b4-776d59ae88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round epsilon for eadability\n",
    "eps_rounded = round(epsilon, 3)\n",
    "\n",
    "# Define report save path\n",
    "if selected_dataset == \"MNIST\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_{selected_mnist_model}_report_eps={eps_rounded}_iteration={num_steps}.txt\"\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_{selected_cifar10_model}_report_eps={eps_rounded}_iteration={num_steps}.txt\"\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_InceptionV3_report_eps={eps_rounded}.txt\"\n",
    "else:\n",
    "    print(f\"This {selected_dataset} is not recognized. Be careful to provide an existing dataset between MNIST, CIFAR\")\n",
    "    \n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Report Title\n",
    "    f.write(f\"====== {selected_attack} Adversarial Attack Report (Œµ = {eps_rounded}) ======\\n\\n\")\n",
    "\n",
    "    # Information generation for each image\n",
    "    for i in range(len(y_pred_adv_all)):  \n",
    "        # Find the class index in ancestors_label\n",
    "        class_label = str(y_all[i][0])  # Convert to string to match ancestors_label\n",
    "        if class_label in ancestors_label:\n",
    "            class_index = ancestors_label.index(class_label)  # Get index in ancestors_name\n",
    "            class_name = ancestors_name[class_index]  # Retrieve class name\n",
    "        else:\n",
    "            class_name = \"Unknown\"  # If not found, prevent error\n",
    "\n",
    "        # Original Image file name (ensuring correct numbering)\n",
    "        original_image_name = f\"{class_name}{(i % 10) + 1:02d}.jpeg\"\n",
    "\n",
    "        # Predict Class for the original image (top-1)\n",
    "        clean_pred_index = np.argmax(y_pred_clean_all[i])\n",
    "\n",
    "        # Predict Class for the Adversarial image (top-1)\n",
    "        adv_pred_index = np.argmax(y_pred_adv_all[i])\n",
    "\n",
    "        # Prediction\n",
    "        clean_pred_label = label_to_name_dynamic(clean_pred_index, selected_dataset)\n",
    "        adv_pred_label = label_to_name_dynamic(adv_pred_index, selected_dataset)\n",
    "\n",
    "\n",
    "        # Targeted or Untargeted Scenario Attack\n",
    "        attack_type = \"Targeted\" if attack.targeted else \"Untargeted\"\n",
    "\n",
    "        # If Targeted : Target Class\n",
    "        target_label = label_to_name(y_target[0]) if attack.targeted else \"N/A\"\n",
    "\n",
    "        # Write results in the report:\n",
    "        f.write(f\"------ CLASS : {class_name.upper()} ------\\n\")\n",
    "        f.write(f\"Original image name : {original_image_name}\\n\")\n",
    "        f.write(f\"Original Prediction : {clean_pred_label}\\n\")\n",
    "        f.write(f\"Targeted / Untargeted : {attack_type}\\n\")\n",
    "        if attack.targeted:\n",
    "            f.write(f\"Target Class : {target_label}\\n\")\n",
    "        f.write(f\"Adversarial Prediction : {adv_pred_label}\\n\")\n",
    "        f.write(\"------------------------------------------------\\n\\n\")\n",
    "\n",
    "    # Performance Summary at the end of the file\n",
    "    f.write(\"============ PERFORMANCE RESUME ============\\n\")\n",
    "    f.write(f\"Selected Attack: {selected_attack} | Dataset: {selected_dataset} | Epsilon: {eps_rounded} | Epsilon Step: Dynamics | Number of Iterations: {num_steps}\\n\")\n",
    "    f.write(f\"Clean Images : Top-1 : {clean_top1:.1f}% | Top-5 : {clean_top5:.1f}%\\n\")\n",
    "    f.write(f\"Adv. Images  : Top-1 : {adv_top1:.1f}% | Top-5 : {adv_top5:.1f}%\\n\")\n",
    "    f.write(\"----------------------------------------------------------\")\n",
    "    f.write(f\"Confidence Score: {average_confidence:.2f}%\")\n",
    "\n",
    "    attack_eff_top1 = 100 - adv_top1\n",
    "    attack_eff_top5 = 100 - adv_top5\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"{selected_attack} Efficiency : Top-1 : {attack_eff_top1:.1f}% | Top-5 : {attack_eff_top5:.1f}%\\n\")\n",
    "\n",
    "# Saving Confirmation\n",
    "print(f\"Report saved : {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a025e-ddb1-4939-b24b-bcf8373c50a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Going further (optional) : Expectation Over Transformation (EoT) \n",
    "Adversarial attacks like **FGSM** are often **sensitive to image transformations** such as **rotation, scaling, or noise**.\n",
    "\n",
    "**Why does this happen?**  \n",
    "- A small rotation (e.g., **5¬∞**) can **invalidate** an adversarial example.\n",
    "- This **breaks the perturbation pattern** that misleads the classifier.\n",
    "  \n",
    "**How does EoT (Expectation Over Transformation) help?**  \n",
    "- Instead of using **a single perturbed image**, EoT **randomly transforms** the image (rotation, blur, etc.).\n",
    "- The attack is then **optimized over multiple transformations**, making it **more robust**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66723512-78b5-4b39-a210-545272a7c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "# Define rotation angles to test\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Apply rotation to all adversarial examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(0, 1), order=1, mode='constant')\n",
    "        for img in x_adv_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "# Get predictions after rotation\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995db06-f091-426e-89f2-30041d9861cc",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d583b-7e22-4b62-ac64-c0f727f6d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  # Set to True to display all, False to show a few per angle\n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}¬∞\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:\n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa39f3d-a018-4ac6-85b4-604077cfa0ff",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a7f96-3779-403c-9a78-4f48e713819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}¬∞ ‚Üí Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39c55a-fd95-4681-b97e-1b09b79835f1",
   "metadata": {},
   "source": [
    "## Step 7: Apply Expectation Over Transformation (EoT)\n",
    "\n",
    "### **What is EoT and Why is it Useful?**\n",
    "FGSM and adversarial attacks often **fail** when images undergo transformations like **rotations**.\n",
    "\n",
    "**EoT (Expectation Over Transformation) mitigates this issue by:**\n",
    "- Generating multiple **randomly transformed** versions of the adversarial image.\n",
    "- Applying these transformations **during model evaluation** (predictions & gradients).\n",
    "- Making the adversarial attack **robust to transformations** like **rotations, noise, and blur**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enable EoT in ART**\n",
    "We use ART‚Äôs **`EoTImageRotationTensorFlow`** to introduce **random rotations** during classification.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00187be4-535b-4780-8831-418fce1f3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ART Classifier with EoT\n",
    "eot_rotation = EoTImageRotationTensorFlow(nb_samples=eot_samples,  \n",
    "                                          clip_values=clip_values,  \n",
    "                                          angles=eot_angle)  # Random rotation range\n",
    "\n",
    "classifier_eot = TensorFlowV2Classifier(model=model,\n",
    "                                        nb_classes=nb_classes,\n",
    "                                        loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                        preprocessing=preprocessing,\n",
    "                                        preprocessing_defences=[eot_rotation],  # EoT applied\n",
    "                                        clip_values=clip_values,\n",
    "                                        input_shape=input_shape)\n",
    "\n",
    "print(f\"EoT Classifier created with {eot_samples} transformation samples per evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116e55a-284c-4fe5-b4b6-3dd27961243d",
   "metadata": {},
   "source": [
    "### Generate Adversarial Examples with EoT\n",
    "We generate **adversarial examples** that remain effective even **after transformations**.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235e4b7-6331-44d7-99bc-276b49b9bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare target labels for targeted attacks\n",
    "y_target_one_hot = np.zeros((1, nb_classes), dtype=np.float32)\n",
    "y_target_one_hot[0, name_to_label(\"guacamole\")] = 1.0  \n",
    "y_target_all = np.tile(y_target_one_hot, (len(x_all), 1))  \n",
    "\n",
    "x_adv_eot_all = []\n",
    "\n",
    "for i in tqdm(range(len(x_all)), desc=\"Generating EoT Examples\"):\n",
    "    x_i = np.expand_dims(x_all[i], axis=0)  \n",
    "    y_i = np.expand_dims(y_target_all[i], axis=0)  \n",
    "\n",
    "    if attack.targeted:\n",
    "        x_adv_i = attack.generate(x=x_i, y=y_i)\n",
    "    else:\n",
    "        x_adv_i = attack.generate(x=x_i)\n",
    "\n",
    "    x_adv_eot_all.append(np.squeeze(x_adv_i))  \n",
    "\n",
    "x_adv_eot_all = np.array(x_adv_eot_all)\n",
    "\n",
    "print(f\"Shape of EoT Adversarial Examples: {x_adv_eot_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0278d2-9459-4d43-b261-6db1e9b07a1f",
   "metadata": {},
   "source": [
    "### Apply Rotation to Adversarial Examples\n",
    "We now test the **robustness** of these adversarial examples by **rotating them** at different angles.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69ea7a-b552-4f9e-bd92-baaa5719ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotation angles\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Rotate and Evaluate Adversarial Examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(1, 2), order=1, mode='constant')\n",
    "        for img in x_adv_eot_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342a24c-5eb9-4e8c-8bcb-9fe1feacf2f4",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few per rotation angle**.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece90eb-c169-4be1-becf-42a7df619fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  \n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}¬∞\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:  \n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb82c3-856a-4174-a218-3e3b80061d17",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc01fc-c6b9-43c7-ada2-c2affe63d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}¬∞ ‚Üí Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2709c-5a09-4a6a-a670-8aeafacdc02f",
   "metadata": {},
   "source": [
    "### Generate a Report on EoT Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c28d0f-e529-43fb-a248-67e6914fe814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define report save path\n",
    "report_filename = f\"EoT_{selected_attack}_{selected_dataset}_eps={round(epsilon, 2)}.txt\"\n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "# Generate Report\n",
    "print(\"\\nGenerating EoT attack report...\")\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"====== EoT Adversarial Attack Report (Œµ = {round(epsilon, 2)}) ======\\n\\n\")\n",
    "    \n",
    "    for angle in rotation_angles:\n",
    "        adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "        adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "        \n",
    "        f.write(f\"\\n=== Rotation {angle}¬∞ ===\\n\")\n",
    "        f.write(f\"Top-1 Accuracy: {adv_top1_rotated:.1f}%\\n\")\n",
    "        f.write(f\"Top-5 Accuracy: {adv_top5_rotated:.1f}%\\n\")\n",
    "\n",
    "print(f\"EoT Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c1bf-f0f3-460e-9e39-9f2056710a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whiteboxattack)",
   "language": "python",
   "name": "whiteboxattack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
