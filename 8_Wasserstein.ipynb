{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1091dcb-e0e0-4767-95df-30247ae372dc",
   "metadata": {},
   "source": [
    "# Wasserstein - Adversarial Attacks\n",
    "### Paper link : https://arxiv.org/abs/1902.07906"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63cd03-35c1-4228-a9f9-851b27ed02eb",
   "metadata": {},
   "source": [
    "## Objectives & Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f86b0a-b2dd-41c8-8858-2f406012541b",
   "metadata": {},
   "source": [
    "All the attacks we've seen so far (**FGSM**, **PGD**, **Auto-PGD**, **DeepFool**, etc.) measure the size of perturbations using an **Lp norm** (L1, L2, or L∞):\n",
    "\n",
    "- They compare the original and adversarial images by measuring **pixel-by-pixel differences**.\n",
    "- However, these measurements **do not account for the spatial structure** of the image.\n",
    "\n",
    "In other words:  \n",
    "Even if two images look **visually very similar**, they can still have a **high Lp distance** if the perturbations are simply **shifted in space**  \n",
    "(e.g., moving a cat’s ear one pixel to the left)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4881b-0055-4da9-a770-e760b34042d0",
   "metadata": {},
   "source": [
    "### Introducing Wasserstein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447a56b-8667-4688-bbec-2a9911d1aee2",
   "metadata": {},
   "source": [
    "Wasserstein Attack leverages the **Wasserstein distance** (also known as **Earth Mover's Distance**), which comes from **optimal transport theory**.\n",
    "\n",
    "- It measures the **cost to transform one distribution** (the original image) into another (the perturbed image).\n",
    "- This cost is computed by **“moving mass”** (i.e., pixel values) from one location to another.\n",
    "\n",
    "#### Advantages:\n",
    "- Takes into account the **spatial geometry** of the image.\n",
    "- Produces **more natural perturbations**, often **less perceptible** to the human eye.\n",
    "- Can succeed where **Lp-based methods fail**, especially against models robust to Lp perturbations.\n",
    "\n",
    "Wasserstein Attack operates through **Projected Gradient Descent (PGD)**,  \n",
    "but instead of projecting into an **Lp-ball**, it projects into a **Wasserstein ball**.\n",
    "\n",
    "At each iteration, it solves a **regularized optimal transport problem** using the **Sinkhorn algorithm**,  \n",
    "to find the most effective perturbation within the allowed Wasserstein radius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d08825-7b07-4df2-9003-e281cc0f8bbf",
   "metadata": {},
   "source": [
    "### What Is the Spatial Structure of an Image?\n",
    "\n",
    "When we talk about the **spatial structure** of an image, we refer to the **relative position of pixels** to each other. For example:\n",
    "\n",
    "- An ear is **to the right** of the head  \n",
    "- The eyes are **above** the nose  \n",
    "- A shadow is **beneath** an object  \n",
    "\n",
    "This spatial structure is essential because an image is not just a collection of isolated pixels — it is an **organized composition**.\n",
    "\n",
    "Now, imagine an image of a cat. If we move a **single pixel of the ear** slightly to the left,  \n",
    "a human will still see a cat without noticing the difference.  \n",
    "But under an **Lp distance**, that pixel has **moved in position**, so it's counted as a **strong change**,  \n",
    "even if the **visual impact is minimal**.\n",
    "\n",
    "> **Lp distances ignore spatial context** and **pixel mass displacement**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd79114-3e1d-47aa-ba81-49fbaab85b4a",
   "metadata": {},
   "source": [
    "### What Is Optimal Transport?\n",
    "\n",
    "**Optimal transport** is a mathematical concept originally used in **logistics**.\n",
    "\n",
    "Imagine the following scenario:\n",
    "- We have **factories** with stock (supply),\n",
    "- We need to move that stock to **warehouses** (demand),\n",
    "- We want to do it with **minimal transport cost** (distance, time, energy…).\n",
    "\n",
    "Optimal transport is about finding the **most efficient way to move mass** from one location to another.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does It Apply to Images?\n",
    "\n",
    "Now, let’s apply this idea to an image:\n",
    "\n",
    "- Each pixel (and its color or intensity) is treated as a **small amount of mass**.\n",
    "- The **perturbed image** is the new state of this mass.\n",
    "- The goal is to measure the **total cost** to transform the original image into the adversarial one,\n",
    "  taking into account the **spatial displacement of pixel values**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Lp Attacks Fall Short\n",
    "\n",
    "Until now, attacks like **FGSM**, **PGD**, and **DeepFool** modify pixels **independently**:\n",
    "- They simply **add or subtract values** at specific pixels,\n",
    "- But they do **not actually move mass** around the image.\n",
    "\n",
    "It’s as if they **change the color** of a pixel without **changing its position**.\n",
    "\n",
    "> None of the previous attacks consider **moving pixels in space** — they ignore the spatial structure.\n",
    "\n",
    "---\n",
    "\n",
    "### What Wasserstein Does Differently\n",
    "\n",
    "* It **intelligently redistributes pixel mass** across the image,  \n",
    "* It **preserves visual perception** by moving rather than distorting,  \n",
    "* It **minimizes the total transport cost** required to fool the model.\n",
    "\n",
    "That’s why it uses the **Wasserstein Distance**, also called **Earth Mover’s Distance (EMD)**:\n",
    "> It measures the **minimum cost** to transform one distribution (the original image) into another (the adversarial image).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4224cb-fa26-4a37-aab5-168c536ac7f2",
   "metadata": {},
   "source": [
    "### Simple Example — Why Lp Norms Fail and Wasserstein Succeeds\n",
    "\n",
    "Imagine an image with two bright spots:\n",
    "\n",
    "**Image A** :\n",
    "**[. . 1]**\n",
    "\n",
    "**Image B** :\n",
    "**[. 1 .]**\n",
    "\n",
    "- Under **Lp distance**, this is considered a **large difference** because the bright pixel is in a different position.\n",
    "- Under **Wasserstein distance**, we say:  \n",
    "  *“Okay, I just moved a bright pixel one step to the left → cost = one small move.”*\n",
    "\n",
    "**Wasserstein doesn’t penalize slight spatial shifts** — visually, it looks the same!\n",
    "\n",
    "Previously, **we couldn’t move pixels** using Lp norms — we could only modify their values.  \n",
    "But with Wasserstein, we can now **move pixel mass**, opening up a **new way to attack** a model!\n",
    "\n",
    "> **Note:** In this simplified example, we act as if a full pixel is moved,  \n",
    "but in reality, **Wasserstein redistributes fractions of mass** (see next example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008eebf-77a7-46bd-b0c1-7329015eaf13",
   "metadata": {},
   "source": [
    "### Concrete Example — 1D Image Transport\n",
    "\n",
    "Imagine a 1D image with 5 pixels:\n",
    "\n",
    "**Original image**:  \n",
    "**[0, 0, 1, 0, 0]** --> The pixel at position 2 contains all the mass (value = 1)\n",
    "\n",
    "\n",
    "**Transport plan**:  \n",
    "- 50% of the mass is moved to pixel 1  \n",
    "- 50% of the mass is moved to pixel 3\n",
    "\n",
    "**Resulting image**:  \n",
    "**[0, 0.5, 0, 0.5, 0]**\n",
    "\n",
    "\n",
    "No duplication, no overwriting — the **mass is redistributed smoothly** across neighboring pixels.\n",
    "\n",
    "---\n",
    "\n",
    "This is why Wasserstein is so **fluid and natural**:\n",
    "\n",
    "- It doesn't **overwrite pixels** like PGD  \n",
    "- It doesn't **duplicate information** either  \n",
    "- It intelligently **reorganizes the distribution** of pixel values across the space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea2168-a11f-4c6b-8231-ffbad443d7ed",
   "metadata": {},
   "source": [
    "### Step-by-Step: How the Wasserstein Attack Works\n",
    "\n",
    "Unlike **FGSM**, **PGD**, or **DeepFool**, which modify pixels **individually**,  \n",
    "**Wasserstein Attack** does **not treat pixels independently**.  \n",
    "Instead, it applies a **global movement of mass** across the image, inspired by optimal transport theory.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-step process:\n",
    "\n",
    "1. **Define a geometric distance between pixels**  \n",
    "   Each pixel is assigned a position \\((x, y)\\) in the image.  \n",
    "   Moving a pixel to a far location is **more costly** than moving it nearby.\n",
    "\n",
    "2. **Formulate the optimal transport problem**  \n",
    "   The goal is to move the mass from the original image distribution to a new image that fools the model,  \n",
    "   while **minimizing the total transport cost**.\n",
    "\n",
    "3. **Solve a transport problem between the pixel intensity distributions**  \n",
    "   (e.g., grayscale or RGB values)  \n",
    "   → This produces a **transport plan**, showing which pixels are moved and where.\n",
    "\n",
    "4. **Apply entropy-based regularization (Sinkhorn distance)**  \n",
    "   to make the computation **faster and more stable**.\n",
    "\n",
    "5. **Generate the adversarial image**  \n",
    "   by **moving the pixel mass**, without changing individual pixel values.  \n",
    "   → The **values remain**, but they are **spatially displaced** in a smooth, coordinated way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46def103-5295-4b47-9466-200d6a74efa3",
   "metadata": {},
   "source": [
    "### Mathematical Formula — Sinkhorn-Regularized Wasserstein Distance\n",
    "\n",
    "Let $\\mu$ and $\\nu$ be two discrete image distributions,  \n",
    "represented as **probability vectors** (i.e., their values sum to 1 across all pixels).  \n",
    "The goal is to minimize the following objective:\n",
    "\n",
    "$\n",
    "W_{\\epsilon}(\\mu, \\nu) = \\min_{P \\in \\Pi(\\mu, \\nu)} \\langle P, C \\rangle - \\epsilon H(P)\n",
    "$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\Pi(\\mu, \\nu)$ is the set of **admissible transport plans** $P$ such that:\n",
    "\n",
    "$\n",
    "P \\mathbf{1} = \\mu, \\quad P^T \\mathbf{1} = \\nu\n",
    "$\n",
    "\n",
    "- $C \\in \\mathbb{R}^{n \\times n}$ : the **cost matrix** that represents the transport cost between pixels  \n",
    "  (usually computed as the **squared Euclidean distance** between pixel positions)\n",
    "\n",
    "- $\\langle P, C \\rangle$: the **total transport cost**, computed as a scalar product\n",
    "\n",
    "- $H(P) = - \\sum_{i,j} P_{ij} \\log P_{ij}$: the **entropy** of the transport plan  \n",
    "  (encourages smoother and more regular solutions)\n",
    "\n",
    "- $\\epsilon > 0$: the **regularization coefficient**  \n",
    "  (the larger it is, the smoother and faster the solution)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d40a7b-e0ab-46f0-baef-4e7baf2b494d",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- This **regularized Wasserstein distance** aims to find the **most efficient way to transport mass** from one image to another.\n",
    "- The term $\\epsilon H(P)$ makes the problem **numerically more stable** and **faster to optimize**,  \n",
    "  particularly thanks to the use of the **Sinkhorn algorithm**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94731-0d90-4d19-8222-191fb4c647ff",
   "metadata": {},
   "source": [
    "# Code\n",
    "### **AUTHOR** : Maxence QUINET (University Of Luxembourg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92716a-5538-4941-ae1e-585ff0cebd21",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae61f74-994a-44c7-82fe-f09fdb63ea5b",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "Please ensure all dependencies are installed using the `requirements.txt` file.\n",
    "\n",
    "For additional environment setup details, refers to **\"environment_configuration.txt\"**.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e3f54-45df-43c5-9a1f-956d88b53d62",
   "metadata": {},
   "source": [
    "Below are the required **libraries and frameworks** for running Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e6787-99cc-4fc7-9d64-6e8cf224e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # (Reduce TensorFlow logs)\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712eb9f-41ed-426b-b796-2eb7c3fe1ea5",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "**Machine Learning & Neural Network Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce5cd-cabe-470f-a557-d6bedb48499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6f62f-0f79-4fc6-ad7e-6e27035804a5",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**Datasets & Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb4172-8e19-4049-b240-c74387c74f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4ab2c-52cb-428a-b51e-c4a2f73160ea",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**Adversarial Robustness Toolbox (ART)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef553-adcb-4d14-b65f-7870359abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier, PyTorchClassifier\n",
    "from art.attacks.evasion import Wasserstein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e44a1-8cfe-4143-9ca3-37645ddf5b95",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "**Vision Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b328a53-2e94-4c2e-b388-8e3e1c037d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb9ee5-13ef-4bb2-bde7-0cc3cc77c9ec",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "**imagenet_stubs** \n",
    "\n",
    "imagenet_stubs is a small dataset available at this link : https://github.com/nottombrown/imagenet-stubs\n",
    "\n",
    "#### Why use it ?\n",
    "\n",
    "* Ideal for **testing adversarial attacks quickly** before applying them on larger datasets.\n",
    "* Provides **two useful functions**:\n",
    "  - `label_to_name(index)` --> Convert an ImageNet label (number) to its corresponding name\n",
    "  - `name_to_label(name)` --> Convert an ImageNet class name back to its numerical label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aac80-f6d0-4fdc-9a0a-68daf1c11fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagenet_stubs\n",
    "from imagenet_stubs.imagenet_2012_labels import label_to_name, name_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1af9f-493c-4822-9f5c-5ddec18010bb",
   "metadata": {},
   "source": [
    "## Checking PyTorch & TensorFlow Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918348c7-eacd-400a-af9a-7f0c5865df92",
   "metadata": {},
   "source": [
    "### **CUDA & GPU Verification**\n",
    "Since we need **CUDA** for accelerated deep learning computations, we ensure that **PyTorch and TensorFlow** are properly configured with CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12e367-3ac8-4bc2-929b-62fa7e586453",
   "metadata": {},
   "source": [
    "------------------\n",
    "**PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57fe8-8f5e-42ce-a951-c9e580038775",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Versions\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b1229-cfdf-484a-a7fe-3c08be588ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9e593-a5d0-4d4b-a327-a65f933ba887",
   "metadata": {},
   "source": [
    "------------------\n",
    "**TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ae0d0-84da-4ffd-835e-ce2648294735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tensorflow\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df32df-c368-4049-953a-86497c4b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA & CUDNN Version\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372090f-77fd-44d4-be97-05b820efda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b3e77-63a1-4940-8f4a-7b542df3f6d9",
   "metadata": {},
   "source": [
    "## Dataset Selection & Configuration\n",
    "\n",
    "In this section, you can **choose the dataset** you want to use for adversarial attacks:\n",
    "\n",
    "- **MNIST**\n",
    "- **CIFAR10**\n",
    "- **CIFAR100**\n",
    "- **ImageNet**\n",
    "\n",
    "#### NOTE: On the \"DeepFool Paper\" they test MNIST, CIFAR10 & ImageNet datasets !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644150a5-4f2e-4e8c-b03a-28f7e3cc442f",
   "metadata": {},
   "source": [
    "### **Select Your Dataset & Model Configuration**\n",
    "### **Select Your Dataset & Model Configuration**\n",
    "> ⚠️ **Note on Wasserstein Attack and Dataset Size**\n",
    "\n",
    "The Wasserstein Attack can theoretically be applied to any dataset.  \n",
    "However, due to its reliance on **optimal transport computations**, it behaves differently depending on the dataset:\n",
    "\n",
    "- ✅ **MNIST** (28×28 grayscale): Ideal — small size, low computational cost.\n",
    "- ⚠️ **CIFAR10 / CIFAR100** (32×32 RGB): Usable — works but may require longer runtimes.\n",
    "- ❌ **ImageNet** (224×224 or 299×299 RGB): Not practical out-of-the-box — requires downsampling or local approximations to avoid excessive memory and computation time.\n",
    "\n",
    "For best results and reasonable execution time, we recommend testing **Wasserstein on small-scale datasets like MNIST**.\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390344aa-5665-4dc2-942e-a0d33ce38fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = \"MNIST\" # OPTIONS : \"MNIST\", \"CIFAR10\", \"CIFAR100\", and \"ImageNet\"\n",
    "\n",
    "selected_cifar10_model = \"standard_resnet\" # OPTIONS : \"standard_resnet\", \"resnet_10x_variant\", \"conv_maxout\"\n",
    "selected_mnist_model = \"simple_cnn\" # OPTIONS : \"simple_cnn\", \"shallow_softmax\", \"maxout\", \"logistic\" (For MNIST only)\n",
    "\n",
    "selected_attack = \"Wasserstein\" # Used for report name only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc0664-be37-43cc-afc2-f5dfbbf25a85",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "**Define class labels for each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed70d3-60ac-4818-bc90-abaa03e8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the ancestors_name & ancestors_label corresponding to the selected dataset.\n",
    "\n",
    "# Note: All labels are available on Internet. They are not created from us. They are official, often in a .json format.\n",
    "if selected_dataset == \"CIFAR10\":\n",
    "    # Correspondance between name & label for CIFAR10\n",
    "    ancestors_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    # Correspondance between name & label for CIFAR100\n",
    "    ancestors_name = ['apple', 'bridge', 'castle', 'elephant', 'house', 'orange', 'shark', 'table', 'tractor', 'whale']\n",
    "    ancestors_label = ['0', '12', '17', '31', '37', '53', '73', '84', '89', '95']\n",
    "\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['abacus', 'acorn', 'baseball', 'broom', 'brown_bear', 'canoe', 'hippopotamus', 'llama', 'maraca', 'mountain_bike']\n",
    "    ancestors_label = ['398', '988', '429', '462', '294', '472', '344', '355', '641', '671']\n",
    "\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "else:\n",
    "    print(f\"Your {selected_dataset} doesn't exist. Please provide an existing dataset between these choices : CIFAR10, CIFAR100, ImageNet & MNIST.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed2cd3-45de-4c8f-8046-d32c53fd4c1a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "**Fix seed to ensure reproducibility (comment to get random results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9afdd-ab88-409b-a161-2ece4778f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23689023-5108-4f89-8534-930e7c4f78e0",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "#### plot_prediction()\n",
    "\n",
    "This function will be used to display the original / attacked images.\n",
    "\n",
    "The function is designed to display the images correctly, depending on the dataset selected, with the following legend:\n",
    "\n",
    "<font color='green'>Green bars</font> = correct classification <br>\n",
    "<font color='red'>Red bars</font> = Attack target classification <br>\n",
    "<font color='blue'>Blue bars</font> = other classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6ad9-e831-4c77-a869-5896bc68e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_name_dynamic(index, dataset):\n",
    "    \"\"\"Return the name of the label depending on the selectionned dataset.\"\"\"\n",
    "    if dataset == \"MNIST\":\n",
    "        return str(index)  # For MNIST, the label name is simply the digit\n",
    "    elif dataset == \"ImageNet\":\n",
    "        return label_to_name(index)  # Use the imagenet_stubs function for ImageNet !\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        return ancestors_name[index]  # Return the name from our list\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        return cifar100_labels[index] if 0 <= index < 100 else \"Unknown\" # Return the name from our list \n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c39259-d85c-4231-815c-27a1b05183e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, probs, correct_class=None, target_class=None):\n",
    "    \"\"\"\n",
    "    Displays an image with predictions in the form of coloured bars :\n",
    "    - Green --> Correct Class\n",
    "    - Red --> Target Class\n",
    "    - Blue --> Other Classes\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "    # Display the picture\n",
    "    if selected_dataset==\"MNIST\":\n",
    "        ax1.imshow(img, cmap=\"gray\") # Force the display in gray level for MNIST !\n",
    "        ax1.axis(\"off\")\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "    # Keep the top 10 classes with highest probabilities\n",
    "    top_ten_indexes = list(probs[0].argsort()[-10:][::-1])\n",
    "    top_probs = probs[0, top_ten_indexes]\n",
    "    labels = [label_to_name_dynamic(i, selected_dataset) for i in top_ten_indexes]\n",
    "\n",
    "\n",
    "    # Bar plot creation with color rules defined above\n",
    "    barlist = ax2.bar(range(10), top_probs, color=\"blue\")  # Blue by default\n",
    "\n",
    "    if target_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(target_class)].set_color(\"red\")  # Red if this is the target class\n",
    "\n",
    "    if correct_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(correct_class)].set_color(\"green\")  # Green if this is the correct class\n",
    "\n",
    "    # Plot Graph\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10), labels, rotation=\"vertical\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Top 10 Predictions\")\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484cab-962a-4634-8254-9bc740b92764",
   "metadata": {},
   "source": [
    "## Step 1: Define Parameters\n",
    "\n",
    "> *NOTE*: Wasserstein is a **Targeted or Untargeted Attack**  \n",
    "You can optionally define a **target class** to force the model to misclassify into it, or let the attack simply cause **any misclassification**.\n",
    "\n",
    "Unlike traditional Lₚ-based attacks (FGSM, PGD...), Wasserstein attack uses **Optimal Transport theory** to move pixel masses intelligently across the image.\n",
    "\n",
    "It preserves the spatial structure and tends to generate **natural-looking perturbations** that are hard to detect visually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930689ae-64a9-4395-be5d-b152868d42ec",
   "metadata": {},
   "source": [
    "### Wasserstein Attack Parameters\n",
    "\n",
    "The **Wasserstein Attack** introduces several advanced parameters, but here are the most important ones for most use cases:\n",
    "\n",
    "- `targeted`: Whether to perform a **targeted** or **untargeted** attack.\n",
    "\n",
    "- `regularization`: Entropy regularization term.  \n",
    "  Controls the **smoothness** of the transport plan. Higher values lead to **softer**, more diffuse perturbations.\n",
    "\n",
    "- `p`: The **Wasserstein-p distance** used. Typically set to 1 or 2.\n",
    "\n",
    "- `kernel_size`: The size of the kernel used to compute the **cost matrix** between pixels.  \n",
    "  A larger kernel considers **broader spatial movement**.\n",
    "\n",
    "- `eps_step`: The **step size** (i.e. how much change is allowed per iteration).\n",
    "\n",
    "- `norm`: Defines the **norm used to measure the perturbation distance**.  \n",
    "  Setting both `norm` and `ball` to `\"wasserstein\"` enables the Wasserstein constraint.\n",
    "\n",
    "- `ball`: Defines the **shape of the allowed perturbation region** (e.g., `\"wasserstein\"`, `\"inf\"`, `\"l2\"`).\n",
    "\n",
    "- `eps`: The **maximum perturbation** allowed.\n",
    "\n",
    "- `eps_iter`: Number of steps used to progressively increase `eps` over the course of the attack.\n",
    "\n",
    "- `eps_factor`: Factor by which `eps_step` is multiplied at each `eps_iter` step (i.e. exponential growth of perturbation).\n",
    "\n",
    "- `max_iter`: Number of PGD iterations (default: 100).  \n",
    "  Like PGD, Wasserstein iteratively improves its perturbation through projection and gradient updates.\n",
    "\n",
    "- `conjugate_sinkhorn_max_iter`: Maximum iterations used to solve the **Sinkhorn optimization** in the transport step.\n",
    "\n",
    "- `projected_sinkhorn_max_iter`: Maximum iterations used for the **projection step** to enforce the Wasserstein constraint.\n",
    "\n",
    "\n",
    "You can now modify the following values to test different configurations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece686ae-29fc-4699-9254-be0e3cbc0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein Paper Research Parameters\n",
    "targeted = False        # Change to True if you want to force a specific target class\n",
    "regularization = 0.1    # Entropy regularization (Sinkhorn smoothing)\n",
    "p = 2                   # p-Wasserstein distance (usually 1(Manhattan) or 2(Euclidean))\n",
    "kernel_size = 5         # Locality for transport cost (size of the moving window)\n",
    "eps_step = 0.1          # Step size per iteration\n",
    "norm=\"wasserstein\"      # Norme de la perturbation (peut être \"inf\", 1, 2, \"wasserstein\")\n",
    "ball=\"wasserstein\"      # Contrainte sur la perturbation (peut être \"inf\", 1, 2, \"wasserstein\")\n",
    "eps = 0.3               # Maximum perturbation\n",
    "eps_iter=10             # Nombre d'itérations pour augmenter epsilon\n",
    "eps_factor=1.2          # Facteur d'augmentation d'epsilon\n",
    "max_iter = 100          # Number of attack iterations\n",
    "conjugate_sinkhorn_max_iter=10  # Nombre max d'itérations pour l'optimiseur conjugate Sinkhorn\n",
    "projected_sinkhorn_max_iter=10  # Nombre max d'itérations pour l'optimiseur projected Sinkhorn\n",
    "batch = 1               # Batch size\n",
    "\n",
    "# Targeted Attack Setup (only if targeted = True)\n",
    "target_class = 3        # Target class index\n",
    "nb_samples = 100        # Number of images\n",
    "y_target = np.full((nb_samples,), target_class)  # Shape: (nb_samples,)\n",
    "\n",
    "print(f\"Selected Attack: {selected_attack} | Dataset: {selected_dataset} | Targeted: {targeted} | \"\n",
    "      f\"Eps: {eps} | Eps Step: {eps_step} | Regularization: {regularization} | Kernel Size: {kernel_size} | P: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea443-adc3-480c-8769-94cbc0e4325d",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "**Later, we'll see what EoT is. If you don't know what is EoT, skip this sub-section**\n",
    "\n",
    "*If you want to test EoT Transformation, find parameters below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5c68e-b441-4917-b6e3-6d149ea39d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EoT Transformation\n",
    "angle_max = 22.5 # Rotation angle used for evaluation in degrees\n",
    "eot_angle = angle_max # Maximum angle for sampling range in EoT rotation, applying range [-eot_angle, eot_angle]\n",
    "eot_samples = 10 # Number of samples with random rotations in parallel per loss gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcc897-543a-4c2a-a813-b3ea9699161c",
   "metadata": {},
   "source": [
    "### Dataset-Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101319c-706d-496f-86d1-b745b7834a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet has 1000 classes, CIFAR100 100 classes, and CIFAR10 & MNIST has 10 classes.\n",
    "nb_classes = 1000 if selected_dataset == \"ImageNet\" else 100 if selected_dataset == \"CIFAR100\" else 10\n",
    "\n",
    "# ImageNet Images Dimension : (299,299,3), CIFAR10 & CIFAR100 : (32,32,3), and MNIST : (28,28,1)\n",
    "input_shape = (299, 299, 3) if selected_dataset == \"ImageNet\" else (32, 32, 3) if \"CIFAR\" in selected_dataset else (28, 28, 1)\n",
    "\n",
    "# ImageNet use often a specific preprocessing. For the others dataset, it still an adapted normalisation (0,1)\n",
    "preprocessing = (0.5, 0.5) if selected_dataset == \"ImageNet\" else (0.0, 1.0)  # Normalisation adaptée\n",
    "\n",
    "# Clip values \n",
    "clip_values = (0.0, 1.0)  # Same for all datasets\n",
    "\n",
    "# Target Class Definition (You can change, here are just some examples)\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    y_target = np.array([641])  # \"maraca\"\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    y_target = np.array([3])  # \"bear\"\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    y_target = np.array([1])  # \"automobile\"\n",
    "else:  # MNIST\n",
    "    y_target = np.array([np.random.randint(0, 10)])  # random digit between 0 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787fe1-db8a-4d32-aa92-2f11ecd764af",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset Data & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73add696-3799-40c0-ac09-31ed827579e6",
   "metadata": {},
   "source": [
    "In this step, we **load all dataset images and their labels into memory**.\n",
    "\n",
    "#### **How does it work?**\n",
    "1. We retrive the dataset path (`datasets/selected_dataset/`).\n",
    "2. We read all images from the dataset folders.\n",
    "3. We **normalize** the images (scale pixel values between `[0, 1]`).\n",
    "4. We store **both images and labels** for further processing.\n",
    "\n",
    " -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5041c04-64ef-435c-9cb9-2374e84ca1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Initializations\n",
    "x_all, y_all, original_images = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e9aff-599b-46b4-80fa-b06e92f9673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get our dataset path in our computer to keep all pictures and put them into our lists.\n",
    "dataset_path = os.path.join(\"datasets\", selected_dataset)\n",
    "# Check\n",
    "assert(dataset_path==\"datasets/\"+selected_dataset) # If nothing : It's ok. Otherwise, you will get an error if the dataset path doesn't exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b009a5-c170-4d80-8d6b-4de6e1695908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from the selected dataset\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        continue\n",
    "    \n",
    "    for img_file in sorted(os.listdir(class_path)):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        if selected_dataset == \"MNIST\":\n",
    "            im = load_img(img_path, color_mode=\"grayscale\", target_size=(28, 28))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        elif selected_dataset == \"ImageNet\":\n",
    "            im = load_img(img_path, target_size=(299, 299))\n",
    "            im_array = img_to_array(im)\n",
    "\n",
    "        elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "            im = load_img(img_path, target_size=(32, 32))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        x = (im_array / 255.0).astype(np.float32)\n",
    "        \n",
    "        x_all.append(x)\n",
    "        y_all.append(int(class_label))\n",
    "        original_images.append(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dd7e1-03f5-42db-a0c6-48fb139331f6",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "#### Display Dataset (Optional)\n",
    "**You can choose to display all images or only one image per class)**\n",
    "\n",
    "#### How to enable visualization ?\n",
    "- To display **ALL images** --> **Uncomment the loop bellow**.\n",
    "- To display **ONLY 1 image per class** --> **Set `display_all_images = False`**.\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d59b-406f-4883-93bb-f0961d426afc",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying of the 100 pictures (can be long, you can modify the code to display only 1 picture per class if you want)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        print(class_path)\n",
    "        print(\"No os Path\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Class : {class_name} (Label: {class_label})\")\n",
    "    \n",
    "     # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = sorted(os.listdir(class_path))[:1] if not display_all_images else sorted(os.listdir(class_path))\n",
    "    # Go through the 10 pictures of each classes\n",
    "    for img_file in images_to_show:\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        # Load & Normalize the picture\n",
    "        im = load_img(img_path, target_size=(299, 299))\n",
    "        im_array = img_to_array(im)\n",
    "\n",
    "        # Displaying all pictures\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(im_array.astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Class: {class_name} | {img_file}\", fontsize=10, fontweight=\"bold\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{img_file} well displayed in : {class_name}\")\n",
    "\n",
    "print(f\"All of the {len(ancestors_name)} classes & their images has been displayed !\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7c65f-03a9-44f8-913a-09d2957ef192",
   "metadata": {},
   "source": [
    "### Convert to Numpy Arrays for TensorFlow\n",
    "Since TensorFlow requires NumPy arrays, we convert our lists into arrays.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bde4b5-0d8d-4366-8a99-3c770534f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a numpy array\n",
    "x_all = np.array(x_all)\n",
    "y_all = np.array(y_all).reshape(-1, 1)\n",
    "\n",
    "# Check\n",
    "#for img_x, img_y in zip(x_all, y_all):\n",
    "#    print(f\"x_all shape: {x_all.shape}\")  # (N, H, W, C)\n",
    "#    print(f\"y_all shape: {y_all.shape}\")  # (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dded0f-4832-4d83-9055-c24bb6c7fd57",
   "metadata": {},
   "source": [
    "## Step 3 : Load Model & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411ad0a-c669-417d-9a28-b86681dd231c",
   "metadata": {},
   "source": [
    "### 1. Loading Dataset for Model Training\n",
    "Before creating the model, we **load and preprocess** the dataset to ensure it is correctly formatted for TensorFlow.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc8c37-3fb4-48d1-85b9-2a7175b4e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_dataset == \"MNIST\":\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)\n",
    "    \n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")\n",
    "\n",
    "    # We reproduce the list of all classes of CIFAR100\n",
    "    cifar100_labels = [\n",
    "    \"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\",\n",
    "    \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\",\n",
    "    \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"crab\", \"crocodile\", \"cup\", \"dinosaur\",\n",
    "    \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"computer_keyboard\",\n",
    "    \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\",\n",
    "    \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\",\n",
    "    \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\", \"rose\", \"sea\",\n",
    "    \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\", \"squirrel\", \"streetcar\", \"sunflower\",\n",
    "    \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\", \"train\", \"trout\", \"tulip\",\n",
    "    \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"\n",
    "]\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51714d-e20b-4bab-8d09-5fc35f30e8cc",
   "metadata": {},
   "source": [
    "### 2. Model Selection & Architecture\n",
    "On the **DeepFool** paper, they have tested the attack on LeNet, GoogLeNet(Inception), NIN, CaffeNet etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d5538-1404-45c5-94d6-d98aa3bd3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definind a custom Maxout layer\n",
    "class MaxoutLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, **kwargs):\n",
    "        super(MaxoutLayer, self).__init__(**kwargs)\n",
    "        self.num_units = num_units\n",
    "        self.dense = Dense(num_units * 2)  # We create twice more neurons to have the max\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)  # Apply Linear Transformation\n",
    "        x = tf.reshape(x, (-1, self.num_units, 2))  #Group neurons by pair\n",
    "        return tf.reduce_max(x, axis=-1)  # Take the maximum from each pair\n",
    "\n",
    "# Creating the Maxout model\n",
    "def build_maxout_model():\n",
    "    input_layer = Input(shape=(28, 28, 1))  # Input: MNIST (28, 28, 1)\n",
    "    flattened = Flatten()(input_layer)\n",
    "\n",
    "    # Add Maxout Layers\n",
    "    maxout_1 = MaxoutLayer(256)(flattened)\n",
    "    maxout_2 = MaxoutLayer(128)(maxout_1)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = Dense(10, activation='softmax')(maxout_2)\n",
    "\n",
    "    # Creating Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Convolutional Maxout Network\n",
    "def build_conv_maxout_model(num_classes):\n",
    "    input_layer = Input(shape=(32, 32, 3))  # Input CIFAR-10  et CIFAR100 (32x32, RGB) so (32, 32, 3)\n",
    "\n",
    "    # Classic Convolutional Layers\n",
    "    conv1 = Conv2D(64, (3, 3), padding=\"same\", activation=None)(input_layer)  # No Activation (Handle by Maxout)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), padding=\"same\", activation=None)(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Add Maxout Layers\n",
    "    flattened = Flatten()(pool2)\n",
    "    maxout_1 = MaxoutLayer(256)(flattened)\n",
    "    maxout_2 = MaxoutLayer(128)(maxout_1)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = Dense(num_classes, activation='softmax')(maxout_2)\n",
    "\n",
    "    # Creating Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56f475-012b-43a9-b382-920db018eeaa",
   "metadata": {},
   "source": [
    "#### The code below represent the Simple Convolutional Neural Network used in the PGD Paper for **MNIST** !\n",
    "\n",
    "**The official code from the paper is available at this GitHub Link : https://github.com/MadryLab/mnist_challenge/blob/master/model.py**\n",
    "\n",
    "This code is more compact because we use a higher version of tensorflow, but the model is 100% the same (you can compare if you want)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4e4af-9354-4c98-abfa-affb0a270c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_simple_CNN_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for MNIST\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484d107-3a97-41e2-b2f9-2cac8cd20ac5",
   "metadata": {},
   "source": [
    "#### The code below represent the ResNet Model & in Variant 10 times larger used in the PGD Paper for **CIFAR10** !\n",
    "\n",
    "**The official code from the paper is available at this GitHub Link : https://github.com/MadryLab/cifar10_challenge/blob/master/model.py**\n",
    "\n",
    "This code is more compact because we use a higher version of tensorflow, but the model is 100% the same (you can compare if you want)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de8e0f-1a4c-41e8-a88e-24e104cb905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET STANDARD MODEL\n",
    "def build_resnet_model():\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False,  # Don't include the classification for ImageNet. When True --> Conserve the classification final layer pre-trained for ImageNet (1000 classes)\n",
    "        input_shape=(32, 32, 3),\n",
    "        weights=None  # We train the model from scratch\n",
    "    )\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f5cb1-945e-49e7-85cb-2d22de44f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET VARIANT 10 TIMES LARGER\n",
    "def build_wide_resnet():\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, # Don't include the classification for ImageNet. When True --> Conserve the classification final layer pre-trained for ImageNet (1000 classes)\n",
    "        input_shape=(32, 32, 3),\n",
    "        weights=None\n",
    "    )\n",
    "\n",
    "    # We increase the number of filters by 10\n",
    "    for layer in base_model.layers:\n",
    "        if isinstance(layer, layers.Conv2D):\n",
    "            layer.filters *= 10  \n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aed09b-80e0-48cc-b4c2-3659ebe7612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= IMAGENET =============\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    print(f\"SELECTED MODEL : InceptionV3.\") \n",
    "    model = InceptionV3(include_top=True, weights='imagenet', classifier_activation='softmax')\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# ============= MNIST =============\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    \n",
    "    # SHALLOW MAX CLASSIFER\n",
    "    if selected_mnist_model == \"shallow_softmax\":\n",
    "        print(f\"SELECTED MODEL : {selected_mnist_model}.\") \n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=input_shape), # MNIST has a shape of (28, 28)\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(10, activation='softmax') # 10 classes for MNIST (0 to 9)\n",
    "        ])\n",
    "\n",
    "    # MAXOUT\n",
    "    elif selected_mnist_model == \"maxout\":\n",
    "        print(f\"SELECTED MODEL : {selected_mnist_model}.\")\n",
    "        model = build_maxout_model()\n",
    "        \n",
    "    # LOGISTIC REGRESSION\n",
    "    elif selected_mnist_model == \"logistic\":\n",
    "        print(f\"SELECTED MODEL : {selected_mnist_model}.\")\n",
    "        # Logistic Regression = one dense simple layer with softmax\n",
    "        model = Sequential([\n",
    "            Flatten(input_shape=(28, 28)),  # MNIST has a shape of (28, 28)\n",
    "            Dense(10, activation='softmax')  # 10 classes (0-9)\n",
    "        ])\n",
    "\n",
    "    # SIMPLE CNN --> THE MODEL USED IN THE OFFICIAL PGD PAPER. OFFICIAL CODE AVAILABLE AT THIS GITHUB LINK  : https://github.com/MadryLab/mnist_challenge/blob/master/model.py\n",
    "    elif selected_mnist_model == \"simple_cnn\":\n",
    "        # Create the model\n",
    "        model = build_simple_CNN_model()\n",
    "\n",
    "        # (Optional) Print the model summary\n",
    "        model.summary()\n",
    "    else: \n",
    "        raise ValueError(f\"Error: Model '{selected_mnist_model}' is not recognized between : shallow_softmax, maxout and logistic.\")\n",
    "\n",
    "# ============= CIFAR10/CIFAR100 =============\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    if selected_cifar10_model == \"standard_resnet\":\n",
    "        print(f\"SELECTED MODEL: Standard ResNet\")\n",
    "        resnet_model = build_resnet_model()\n",
    "        resnet_model.summary()\n",
    "        \n",
    "    elif selected_cifar10_model == \"resnet_x10_variant\":\n",
    "        print(f\"SELECTED MODEL: ResNet 10x larger Variant\")\n",
    "        wide_resnet_model = build_wide_resnet()\n",
    "        wide_resnet_model.summary()\n",
    "    else:    \n",
    "        print(f\"SELECTED MODEL : Convolutional Maxout Network.\")\n",
    "        # Creating Model\n",
    "        model = build_conv_maxout_model(nb_classes)\n",
    "\n",
    "# ============= ERROR =============\n",
    "else:\n",
    "    raise ValueError(f\"Error: Dataset '{selected_dataset} not recognized. Please ensure to use one of this dataset : ImageNet, CIFAR10, CIFAR100 or MNIST.'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778a0f-f1d3-457b-a7a2-4fd94d5eb502",
   "metadata": {},
   "source": [
    "### 3. Model Compilation & Training\n",
    "Once the model is selected, we **compile and train** it.\n",
    "\n",
    "- **For ImageNet**, the model is already pretrained\n",
    "- **For other datasets**, a quick training step (5-10 epochs) is performed.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448f8f2-ef74-4986-bb1c-07fb14edd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "if selected_dataset != \"ImageNet\":\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy' if selected_dataset != \"MNIST\" else 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "if selected_dataset != \"ImageNet\":\n",
    "    epochs = 5 if selected_dataset == \"MNIST\" else 10  # Quick training\n",
    "    print(f\"Training model on {selected_dataset} for {epochs} epochs...\")\n",
    "    model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))\n",
    "    print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1fdd7-9038-4b6d-9326-227751a2a427",
   "metadata": {},
   "source": [
    "## Step 4 : Create the ART Classifier & Configure the Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53db541-c3ca-4a5f-8d75-c7dd4557d65d",
   "metadata": {},
   "source": [
    "Now that the model is **trained and ready**, we integrate it into **ART (Adversarial Robustness Toolbox)**.\n",
    "\n",
    "#### What is happening here ?\n",
    "1. We **create a classifier** for ART based on the trained model\n",
    "2. We **define an adversarial attack** (FGSM in this case)\n",
    "3. The attack can be **targeted or untargeted**, and parameters are fully configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597176e7-cdfb-4cc7-8f51-1f74f1a83f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model,\n",
    "                                    nb_classes=nb_classes,\n",
    "                                    loss_object=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                                    preprocessing=preprocessing,\n",
    "                                    preprocessing_defences=None,\n",
    "                                    clip_values=clip_values,\n",
    "                                    input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfaf70-dae6-486c-8c62-41ce8639136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = Wasserstein(estimator=classifier,\n",
    "                     targeted=targeted,\n",
    "                     regularization=regularization,\n",
    "                     p=p,\n",
    "                     kernel_size=kernel_size,\n",
    "                     eps_step=eps_step,\n",
    "                     norm=\"wasserstein\",\n",
    "                     ball=\"wasserstein\",\n",
    "                     eps=eps,\n",
    "                     eps_iter=eps_iter,\n",
    "                     eps_factor=eps_factor,\n",
    "                     max_iter=max_iter,\n",
    "                     conjugate_sinkhorn_max_iter=conjugate_sinkhorn_max_iter,\n",
    "                     projected_sinkhorn_max_iter=projected_sinkhorn_max_iter,\n",
    "                     batch_size=batch,\n",
    "                     verbose=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34060c84-e2de-438b-995b-119d73d6b112",
   "metadata": {},
   "source": [
    "## Step 5 : Predict Clean (Original) Images BEFORE the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c927354-cc5b-45d0-bcb3-65f152c94efd",
   "metadata": {},
   "source": [
    "Before applying any attack, we **predict the clean images** with our trained model.\n",
    "\n",
    "#### What happens here ?\n",
    "1. We run the classifier on all images **before the attack**.\n",
    "2. We display the **top-10 predictions** for each image.\n",
    "3. You can choose to **display all images or only one per class**.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54e745-a5c0-485a-8a21-b30f3f08cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Clean Images\n",
    "y_pred_clean_all = classifier.predict(np.array(x_all))\n",
    "\n",
    "# Check prediction shape\n",
    "print(\"Shape of Clean Predictions:\", y_pred_clean_all.shape)  # Expected (N, nb_classes)\n",
    "\n",
    "# Summarize Prediction\n",
    "top1_correct = np.mean(np.argmax(y_pred_clean_all, axis=1) == y_all.flatten()) * 100\n",
    "print(f\"Top-1 Accuracy on Clean Images: {top1_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570014ad-2367-4279-8515-96ff1eb668e3",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "#### Display Clean Images & Predictions\n",
    "You can **choose whether to display all images or just one per class**.\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a7ed8-e5c7-4a3b-ba27-bc7e4d6e4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying Clean Images with Predictions\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    # Get all images from this class\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No Images found for {class_name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_all[index]),  # Original clean image\n",
    "            y_pred_clean_all[index].reshape(1, -1),  # Reshaped prediction\n",
    "            correct_class=y_all[index],  # True class\n",
    "            target_class=None  # No target class for clean images\n",
    "        )\n",
    "        print(f\"Image {index} displayed for class: {class_name}\")\n",
    "\n",
    "print(f\"\\n All {len(x_all)} clean images have been processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526ff66-f8ff-499a-b44d-5ed770fdd749",
   "metadata": {},
   "source": [
    "## Step 6: Generate and Evaluate Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7f94a-fb49-48f2-a6b1-a4ca9cb6dab6",
   "metadata": {},
   "source": [
    "Now, we **generate adversarial examples** and evaluate the effectiveness of the attack.\n",
    "\n",
    " **What happens here?**\n",
    "1. We **generate adversarial examples** using the selected attack.\n",
    "2. We **save the adversarial images** for later analysis. (optional)\n",
    "3. We **evaluate the attack's success** (accuracy, confidence scores, and performance metrics).\n",
    "4. We **generate a detailed report** summarizing the attack results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e17c6-ac35-4194-a7d9-c09ae1aa8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Progress bar for attack generation\n",
    "\n",
    "target_labels = np.tile(y_target, (len(x_all), 1))  # Repeat y_target for each image (same target for all images)\n",
    "\n",
    "if attack.targeted:\n",
    "    print(\"SCENARIO ATTACK : TARGETED\")\n",
    "    # Attack all images with the target class\n",
    "    x_adv_all = attack.generate(x=x_all, y=target_labels)\n",
    "else:\n",
    "    print(\"SCENARIO ATTACK : UNTARGETED\")\n",
    "    # No Target Class\n",
    "    x_adv_all = attack.generate(x=x_all)\n",
    "\n",
    "    \n",
    "# Shape Check of the Adversarial Examples\n",
    "print(\"Shape of Adversarial Examples:\", x_adv_all.shape)  # Expected (N, H, W, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff0f8d-9591-441b-af62-c1a038141487",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "**Do you want to save all adversarial images?**  \n",
    "- **YES** → Uncomment the saving function below.\n",
    "- **NO** → Comment the function to skip saving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b885-6864-4199-9a58-632a74a93a13",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Define the save path for adversarial images\n",
    "adv_save_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset)\n",
    "os.makedirs(adv_save_path, exist_ok=True)  \n",
    "\n",
    "# Iterate through all classes to save adversarial images\n",
    "class_counters = {class_name: 1 for class_name in ancestors_name}  # Dictionary to track image indices per class\n",
    "\n",
    "for adv_img, class_label in zip(x_adv_all, y_all.flatten()):  # Ensure y_all is 1D\n",
    "    # Find the class name corresponding to the label\n",
    "    if str(class_label) not in ancestors_label:\n",
    "        print(f\"Label {class_label} not found in ancestors_label, skipping image.\")\n",
    "        continue  \n",
    "\n",
    "    class_index = ancestors_label.index(str(class_label))\n",
    "    class_name = ancestors_name[class_index]\n",
    "\n",
    "    # Determine the subfolder for the class\n",
    "    class_folder = os.path.join(adv_save_path, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True) \n",
    "\n",
    "    # Generate a unique filename with a counter (e.g., abacus1_adv.jpeg, abacus2_adv.jpeg, ..., acorn1_adv.jpeg, ...)\n",
    "    img_filename = f\"{class_name}{class_counters[class_name]:02d}_adv.jpeg\"\n",
    "    img_path = os.path.join(class_folder, img_filename)\n",
    "\n",
    "    # Convert and save the image\n",
    "    img = array_to_img(adv_img)\n",
    "    img.save(img_path, \"JPEG\")\n",
    "\n",
    "    print(f\"Image saved : {img_path}\")\n",
    "\n",
    "    # Increment the counter for this class\n",
    "    class_counters[class_name] += 1\n",
    "\n",
    "print(f\"\\nAll  {len(x_adv_all)} adversarial images have been successfully saved!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254fe32-85b2-4c37-85ce-1009bbe05e83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate Adversarial Example\n",
    "\n",
    "We now evaluate the adversarial examples by:\n",
    "- Measuring the **model's accuracy** on these images.\n",
    "- Computing the **confidence score** of predictions.\n",
    "- Generating a **visual comparison** between clean and adversarial images.\n",
    "\n",
    "---\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17136d23-7325-40dd-a592-aeffd89df52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on adversarial images\n",
    "y_pred_adv_all = classifier.predict(x_adv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4390776-4d46-4ae0-8dba-3e033b99c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = True  \n",
    "\n",
    "# Display Adversarial Examples (Optional)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "    \n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No images found for {class_name}skipping...\")\n",
    "        continue\n",
    "\n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_all[index]),\n",
    "            y_pred_adv_all[index].reshape(1, -1),\n",
    "            correct_class=y_all[index],\n",
    "            target_class=target_labels[index]\n",
    "        )\n",
    "        print(f\"Adversarial Image {index} displayed for class: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d4cd8-dd10-4ac8-9b8d-f9e82797bb5b",
   "metadata": {},
   "source": [
    "### Compute Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f72e-928c-4d21-8da9-e69c53add45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence score\n",
    "confidence_scores = np.max(y_pred_clean_all, axis=1)\n",
    "average_confidence = np.mean(confidence_scores) * 100\n",
    "\n",
    "# Compute Tok-K Accuracy\n",
    "def compute_accuracy(predictions, true_labels, top_k=1):\n",
    "    top_k_preds = np.argsort(predictions, axis=1)[:, -top_k:]\n",
    "    match = np.any(top_k_preds == np.array(true_labels).reshape(-1, 1), axis=1)\n",
    "    return np.mean(match) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a0b18-9939-4b00-80a7-5dd7c57f141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_top1 = compute_accuracy(y_pred_clean_all, y_all, top_k=1)\n",
    "clean_top5 = compute_accuracy(y_pred_clean_all, y_all, top_k=5)\n",
    "adv_top1 = compute_accuracy(y_pred_adv_all, y_all, top_k=1)\n",
    "adv_top5 = compute_accuracy(y_pred_adv_all, y_all, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f2c9-093e-45ec-b3b7-e4079e242270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Performance Results\n",
    "attack_name = \"Wasserstein\" if isinstance(attack, Wasserstein) else \"fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98b0b-0281-45c0-8d8b-a56b1ab187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Selected Attack: {selected_attack} | Dataset: {selected_dataset} | Targeted: {targeted} | \"\n",
    "      f\"Eps: {eps} | Eps Step: {eps_step} | Regularization: {regularization} | Kernel Size: {kernel_size} | P: {p}\")\n",
    "print(f\"Clean Images : Top-1 : {clean_top1:.2f}% | Top-5 : {clean_top5:.1f}%\")\n",
    "print(f\"Adv. Images  : Top-1 : {adv_top1:.2f}% | Top-5 : {adv_top5:.1f}%\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Confidence Score: {average_confidence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22d39a-2ee4-4ae4-ab55-4e63057fd0a0",
   "metadata": {},
   "source": [
    "### Generate a Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584bf76-61d4-46cd-89b4-776d59ae88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round epsilon for readability -> No Epsilon anymore\n",
    "# eps_rounded = round(epsilon, 3)\n",
    "\n",
    "# Define report save path\n",
    "if selected_dataset == \"MNIST\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_{selected_mnist_model}_report.txt\"\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_{selected_cifar10_model}_report.txt\"\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_InceptionV3_report.txt\"\n",
    "else:\n",
    "    print(f\"This {selected_dataset} is not recognized. Be careful to provide an existing dataset between MNIST, CIFAR\")\n",
    "    \n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Report Title\n",
    "    f.write(f\"====== {selected_attack} Adversarial Attack Report (ε = dynamics ======\\n\\n\")\n",
    "\n",
    "    # Information generation for each image\n",
    "    for i in range(len(y_pred_adv_all)):  \n",
    "        # Find the class index in ancestors_label\n",
    "        class_label = str(y_all[i][0])  # Convert to string to match ancestors_label\n",
    "        if class_label in ancestors_label:\n",
    "            class_index = ancestors_label.index(class_label)  # Get index in ancestors_name\n",
    "            class_name = ancestors_name[class_index]  # Retrieve class name\n",
    "        else:\n",
    "            class_name = \"Unknown\"  # If not found, prevent error\n",
    "\n",
    "        # Original Image file name (ensuring correct numbering)\n",
    "        original_image_name = f\"{class_name}{(i % 10) + 1:02d}.jpeg\"\n",
    "\n",
    "        # Predict Class for the original image (top-1)\n",
    "        clean_pred_index = np.argmax(y_pred_clean_all[i])\n",
    "\n",
    "        # Predict Class for the Adversarial image (top-1)\n",
    "        adv_pred_index = np.argmax(y_pred_adv_all[i])\n",
    "\n",
    "        # Prediction\n",
    "        clean_pred_label = label_to_name_dynamic(clean_pred_index, selected_dataset)\n",
    "        adv_pred_label = label_to_name_dynamic(adv_pred_index, selected_dataset)\n",
    "\n",
    "\n",
    "        # Targeted or Untargeted Scenario Attack\n",
    "        attack_type = \"Targeted\" if attack.targeted else \"Untargeted\"\n",
    "\n",
    "        # If Targeted : Target Class\n",
    "        target_label = label_to_name(y_target[0]) if attack.targeted else \"N/A\"\n",
    "\n",
    "        # Write results in the report:\n",
    "        f.write(f\"------ CLASS : {class_name.upper()} ------\\n\")\n",
    "        f.write(f\"Original image name : {original_image_name}\\n\")\n",
    "        f.write(f\"Original Prediction : {clean_pred_label}\\n\")\n",
    "        f.write(f\"Targeted / Untargeted : {attack_type}\\n\")\n",
    "        if attack.targeted:\n",
    "            f.write(f\"Target Class : {target_label}\\n\")\n",
    "        f.write(f\"Adversarial Prediction : {adv_pred_label}\\n\")\n",
    "        f.write(\"------------------------------------------------\\n\\n\")\n",
    "\n",
    "    # Performance Summary at the end of the file\n",
    "    f.write(\"============ PERFORMANCE RESUME ============\\n\")\n",
    "    f.write(f\"Selected Attack: {selected_attack} | Dataset: {selected_dataset} | Theta: {theta} | Gamma: {gamma} | Target Class: {target_class}\")\n",
    "    f.write(f\"Clean Images : Top-1 : {clean_top1:.1f}% | Top-5 : {clean_top5:.1f}%\\n\")\n",
    "    f.write(f\"Adv. Images  : Top-1 : {adv_top1:.1f}% | Top-5 : {adv_top5:.1f}%\\n\")\n",
    "    f.write(\"----------------------------------------------------------\")\n",
    "    f.write(f\"Confidence Score: {average_confidence:.2f}%\")\n",
    "\n",
    "    attack_eff_top1 = 100 - adv_top1\n",
    "    attack_eff_top5 = 100 - adv_top5\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"{selected_attack} Efficiency : Top-1 : {attack_eff_top1:.1f}% | Top-5 : {attack_eff_top5:.1f}%\\n\")\n",
    "\n",
    "# Saving Confirmation\n",
    "print(f\"Report saved : {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a025e-ddb1-4939-b24b-bcf8373c50a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Going further (optional) : Expectation Over Transformation (EoT) \n",
    "Adversarial attacks like **FGSM** are often **sensitive to image transformations** such as **rotation, scaling, or noise**.\n",
    "\n",
    "**Why does this happen?**  \n",
    "- A small rotation (e.g., **5°**) can **invalidate** an adversarial example.\n",
    "- This **breaks the perturbation pattern** that misleads the classifier.\n",
    "  \n",
    "**How does EoT (Expectation Over Transformation) help?**  \n",
    "- Instead of using **a single perturbed image**, EoT **randomly transforms** the image (rotation, blur, etc.).\n",
    "- The attack is then **optimized over multiple transformations**, making it **more robust**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66723512-78b5-4b39-a210-545272a7c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "# Define rotation angles to test\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Apply rotation to all adversarial examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(0, 1), order=1, mode='constant')\n",
    "        for img in x_adv_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "# Get predictions after rotation\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995db06-f091-426e-89f2-30041d9861cc",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d583b-7e22-4b62-ac64-c0f727f6d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  # Set to True to display all, False to show a few per angle\n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}°\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:\n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa39f3d-a018-4ac6-85b4-604077cfa0ff",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a7f96-3779-403c-9a78-4f48e713819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}° → Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39c55a-fd95-4681-b97e-1b09b79835f1",
   "metadata": {},
   "source": [
    "## Step 7: Apply Expectation Over Transformation (EoT)\n",
    "\n",
    "### **What is EoT and Why is it Useful?**\n",
    "FGSM and adversarial attacks often **fail** when images undergo transformations like **rotations**.\n",
    "\n",
    "**EoT (Expectation Over Transformation) mitigates this issue by:**\n",
    "- Generating multiple **randomly transformed** versions of the adversarial image.\n",
    "- Applying these transformations **during model evaluation** (predictions & gradients).\n",
    "- Making the adversarial attack **robust to transformations** like **rotations, noise, and blur**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enable EoT in ART**\n",
    "We use ART’s **`EoTImageRotationTensorFlow`** to introduce **random rotations** during classification.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00187be4-535b-4780-8831-418fce1f3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ART Classifier with EoT\n",
    "eot_rotation = EoTImageRotationTensorFlow(nb_samples=eot_samples,  \n",
    "                                          clip_values=clip_values,  \n",
    "                                          angles=eot_angle)  # Random rotation range\n",
    "\n",
    "classifier_eot = TensorFlowV2Classifier(model=model,\n",
    "                                        nb_classes=nb_classes,\n",
    "                                        loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                        preprocessing=preprocessing,\n",
    "                                        preprocessing_defences=[eot_rotation],  # EoT applied\n",
    "                                        clip_values=clip_values,\n",
    "                                        input_shape=input_shape)\n",
    "\n",
    "print(f\"EoT Classifier created with {eot_samples} transformation samples per evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116e55a-284c-4fe5-b4b6-3dd27961243d",
   "metadata": {},
   "source": [
    "### Generate Adversarial Examples with EoT\n",
    "We generate **adversarial examples** that remain effective even **after transformations**.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235e4b7-6331-44d7-99bc-276b49b9bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare target labels for targeted attacks\n",
    "y_target_one_hot = np.zeros((1, nb_classes), dtype=np.float32)\n",
    "y_target_one_hot[0, name_to_label(\"guacamole\")] = 1.0  \n",
    "y_target_all = np.tile(y_target_one_hot, (len(x_all), 1))  \n",
    "\n",
    "x_adv_eot_all = []\n",
    "\n",
    "for i in tqdm(range(len(x_all)), desc=\"Generating EoT Examples\"):\n",
    "    x_i = np.expand_dims(x_all[i], axis=0)  \n",
    "    y_i = np.expand_dims(y_target_all[i], axis=0)  \n",
    "\n",
    "    if attack.targeted:\n",
    "        x_adv_i = attack.generate(x=x_i, y=y_i)\n",
    "    else:\n",
    "        x_adv_i = attack.generate(x=x_i)\n",
    "\n",
    "    x_adv_eot_all.append(np.squeeze(x_adv_i))  \n",
    "\n",
    "x_adv_eot_all = np.array(x_adv_eot_all)\n",
    "\n",
    "print(f\"Shape of EoT Adversarial Examples: {x_adv_eot_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0278d2-9459-4d43-b261-6db1e9b07a1f",
   "metadata": {},
   "source": [
    "### Apply Rotation to Adversarial Examples\n",
    "We now test the **robustness** of these adversarial examples by **rotating them** at different angles.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69ea7a-b552-4f9e-bd92-baaa5719ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotation angles\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Rotate and Evaluate Adversarial Examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(1, 2), order=1, mode='constant')\n",
    "        for img in x_adv_eot_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342a24c-5eb9-4e8c-8bcb-9fe1feacf2f4",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few per rotation angle**.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece90eb-c169-4be1-becf-42a7df619fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  \n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}°\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:  \n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb82c3-856a-4174-a218-3e3b80061d17",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc01fc-c6b9-43c7-ada2-c2affe63d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}° → Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2709c-5a09-4a6a-a670-8aeafacdc02f",
   "metadata": {},
   "source": [
    "### Generate a Report on EoT Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c28d0f-e529-43fb-a248-67e6914fe814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define report save path\n",
    "report_filename = f\"EoT_{selected_attack}_{selected_dataset}_eps={round(epsilon, 2)}.txt\"\n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "# Generate Report\n",
    "print(\"\\nGenerating EoT attack report...\")\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"====== EoT Adversarial Attack Report (ε = {round(epsilon, 2)}) ======\\n\\n\")\n",
    "    \n",
    "    for angle in rotation_angles:\n",
    "        adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "        adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "        \n",
    "        f.write(f\"\\n=== Rotation {angle}° ===\\n\")\n",
    "        f.write(f\"Top-1 Accuracy: {adv_top1_rotated:.1f}%\\n\")\n",
    "        f.write(f\"Top-5 Accuracy: {adv_top5_rotated:.1f}%\\n\")\n",
    "\n",
    "print(f\"EoT Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c1bf-f0f3-460e-9e39-9f2056710a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whiteboxattack)",
   "language": "python",
   "name": "whiteboxattack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
