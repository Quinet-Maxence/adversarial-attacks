{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1091dcb-e0e0-4767-95df-30247ae372dc",
   "metadata": {},
   "source": [
    "# Adversarial Patch - Adversarial Attacks\n",
    "### Paper link : https://arxiv.org/abs/1712.09665"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63cd03-35c1-4228-a9f9-851b27ed02eb",
   "metadata": {},
   "source": [
    "## Objectives & Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f86b0a-b2dd-41c8-8858-2f406012541b",
   "metadata": {},
   "source": [
    "Unlike classical attacks like **FGSM**, **PGD**, or **Carlini & Wagner**, which create **imperceptible perturbations** across the entire image, the **Adversarial Patch** attack is different.\n",
    "\n",
    "It generates a **small visible square** (\"patch\") that can be **pasted anywhere on an image** to fool the model.\n",
    "\n",
    "> **Goal**: Learn a single, universal patch that, when applied to any image, **forces the model to predict a specific target class**, regardless of the original content.\n",
    "\n",
    "This attack **does not train the model**, but rather **trains the patch itself**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4881b-0055-4da9-a770-e760b34042d0",
   "metadata": {},
   "source": [
    "We optimize a patch so that it **takes control over the model‚Äôs predictions**, without modifying the rest of the image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447a56b-8667-4688-bbec-2a9911d1aee2",
   "metadata": {},
   "source": [
    "### What makes *Adversarial Patch* unique?\n",
    "\n",
    "| Property                 | Description                                                                 |\n",
    "|--------------------------|-----------------------------------------------------------------------------|\n",
    "| **Visible**            | The patch is **intentionally visible** (not hidden like FGSM).             |\n",
    "| **Position-agnostic**  | It can be pasted **anywhere** on the image.                                |\n",
    "| **Universal**          | Works on **multiple images**, not one patch per image.                     |\n",
    "| **Targeted**           | Forces the prediction towards a **specific class** (e.g., *guacamole*).    |\n",
    "| **Physically plausible** | Can be **printed and used in the real world** (like a sticker!).           |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d08825-7b07-4df2-9003-e281cc0f8bbf",
   "metadata": {},
   "source": [
    "### What does the Feature Adversaries attack do?\n",
    "\n",
    "Here's what the attack does step-by-step:\n",
    "\n",
    "1. Take a **source image** $I_s$ (e.g., a cat),\n",
    "2. Take a **guide image** $I_g$ (e.g., a dog),\n",
    "3. Generate a **modified image** $I_\\alpha$, visually similar to $I_s$,\n",
    "4. Make $\\phi_k(I_\\alpha) \\approx \\phi_k(I_g)$, where $\\phi_k$ is the feature extractor at layer $k$.\n",
    "\n",
    "> The network will \"believe\" that $I_\\alpha$ is closer to $I_g$ than to $I_s$‚Äî**internally**, not visually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f58043-301e-4131-9231-8b855e79ec0c",
   "metadata": {},
   "source": [
    "### Intuitive Idea\n",
    "\n",
    "Imagine pasting a **magic sticker** on any photo, and each time a model sees it, it says:\n",
    "\n",
    "> \"**This is guacamole!**\" ‚Äî even if it‚Äôs a lion, a car, or a cactus.\n",
    "\n",
    "That‚Äôs exactly what **Adversarial Patch** does.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f3834-2c48-47cc-aa1d-d0a68ae53f0e",
   "metadata": {},
   "source": [
    "### How is the patch trained?\n",
    "\n",
    "**Using backpropagation**.\n",
    "\n",
    "The patch is treated as a **learnable parameter**, just like training weights in a network.  \n",
    "We apply **gradient descent directly on the patch**.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize** the patch $p$ randomly (as pixel values),\n",
    "2. **Apply** the patch to a batch of training images,\n",
    "3. **Feed** the patched images into the model,\n",
    "4. **Compute** the loss between model predictions and the **target class** $y_{\\text{target}}$,\n",
    "5. **Backpropagate** the loss and update the patch $p$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008eebf-77a7-46bd-b0c1-7329015eaf13",
   "metadata": {},
   "source": [
    "### Mathematical Formula\n",
    "\n",
    "We optimize the following loss:\n",
    "\n",
    "$$\n",
    "\\min_p \\; \\mathbb{E}_{x \\sim D} \\left[ \\mathcal{L} \\left( f(\\text{ApplyPatch}(x, p)), y_{\\text{target}} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $p$ is the **patch** (to optimize),\n",
    "- $x$ is an image sampled from the dataset $D$,\n",
    "- $\\text{ApplyPatch}(x, p)$ is the function that applies the patch $p$ to image $x$ (with transformations),\n",
    "- $f$ is the **model** (e.g., ResNet50),\n",
    "- $\\mathcal{L}$ is the **loss function** (e.g., softmax cross-entropy),\n",
    "- $y_{\\text{target}}$ is the **desired target class** (e.g., \"guacamole\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea2168-a11f-4c6b-8231-ffbad443d7ed",
   "metadata": {},
   "source": [
    "The result is a **universal targeted perturbation**, designed to fool the model **regardless of the background image**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61e1db-917e-483e-96c5-8d3a4601baa9",
   "metadata": {},
   "source": [
    "### Important Concept: EoT (Expectation Over Transformation)\n",
    "\n",
    "During training, we apply **random transformations** to the patch before applying it:\n",
    "\n",
    "- **Rotation** (e.g., from -22.5¬∞ to +22.5¬∞),\n",
    "- **Scale** (e.g., 40% to 100% of image size),\n",
    "- **Position** (random location within the image).\n",
    "\n",
    "**Why?**  \n",
    "To make the patch **robust in the real world**, even if it appears at different positions or angles.\n",
    "\n",
    "This technique is called **EoT** ‚Äî *Expectation Over Transformation*:  \n",
    "> \"Train over many transformed versions of the patch to make it invariant.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d42902-3e76-4a32-89bd-98eb066c0f76",
   "metadata": {},
   "source": [
    "### Patch Application Function\n",
    "\n",
    "There‚Äôs no complex optimization at inference time ‚Äî we just **apply the patch** to the image.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$$\n",
    "x_{\\text{patched}} = \\text{ApplyPatch}(x, p, \\text{position}, \\text{scale}, \\text{rotation})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ is a **clean image**,\n",
    "- $p$ is the **trained patch**,\n",
    "- Parameters define **where** and **how** to apply the patch:\n",
    "  - Examples: *bottom right corner*, *scale = 0.5*, *rotation = 10¬∞*, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94731-0d90-4d19-8222-191fb4c647ff",
   "metadata": {},
   "source": [
    "# Code\n",
    "### **AUTHOR** : Maxence QUINET (University Of Luxembourg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92716a-5538-4941-ae1e-585ff0cebd21",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae61f74-994a-44c7-82fe-f09fdb63ea5b",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "Please ensure all dependencies are installed using the `requirements.txt` file.\n",
    "\n",
    "For additional environment setup details, refers to **\"environment_configuration.txt\"**.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e3f54-45df-43c5-9a1f-956d88b53d62",
   "metadata": {},
   "source": [
    "Below are the required **libraries and frameworks** for running Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e6787-99cc-4fc7-9d64-6e8cf224e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # (Reduce TensorFlow logs)\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712eb9f-41ed-426b-b796-2eb7c3fe1ea5",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "**Machine Learning & Neural Network Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce5cd-cabe-470f-a557-d6bedb48499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Lambda, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6f62f-0f79-4fc6-ad7e-6e27035804a5",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**Datasets & Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb4172-8e19-4049-b240-c74387c74f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4ab2c-52cb-428a-b51e-c4a2f73160ea",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**Adversarial Robustness Toolbox (ART)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef553-adcb-4d14-b65f-7870359abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier, PyTorchClassifier\n",
    "from art.attacks.evasion import AdversarialPatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e44a1-8cfe-4143-9ca3-37645ddf5b95",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "**Vision Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b328a53-2e94-4c2e-b388-8e3e1c037d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb9ee5-13ef-4bb2-bde7-0cc3cc77c9ec",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "**imagenet_stubs** \n",
    "\n",
    "imagenet_stubs is a small dataset available at this link : https://github.com/nottombrown/imagenet-stubs\n",
    "\n",
    "#### Why use it ?\n",
    "\n",
    "* Ideal for **testing adversarial attacks quickly** before applying them on larger datasets.\n",
    "* Provides **two useful functions**:\n",
    "  - `label_to_name(index)` --> Convert an ImageNet label (number) to its corresponding name\n",
    "  - `name_to_label(name)` --> Convert an ImageNet class name back to its numerical label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aac80-f6d0-4fdc-9a0a-68daf1c11fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagenet_stubs\n",
    "from imagenet_stubs.imagenet_2012_labels import label_to_name, name_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1af9f-493c-4822-9f5c-5ddec18010bb",
   "metadata": {},
   "source": [
    "## Checking PyTorch & TensorFlow Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918348c7-eacd-400a-af9a-7f0c5865df92",
   "metadata": {},
   "source": [
    "### **CUDA & GPU Verification**\n",
    "Since we need **CUDA** for accelerated deep learning computations, we ensure that **PyTorch and TensorFlow** are properly configured with CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12e367-3ac8-4bc2-929b-62fa7e586453",
   "metadata": {},
   "source": [
    "------------------\n",
    "**PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57fe8-8f5e-42ce-a951-c9e580038775",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Versions\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b1229-cfdf-484a-a7fe-3c08be588ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9e593-a5d0-4d4b-a327-a65f933ba887",
   "metadata": {},
   "source": [
    "------------------\n",
    "**TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ae0d0-84da-4ffd-835e-ce2648294735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tensorflow\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df32df-c368-4049-953a-86497c4b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA & CUDNN Version\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372090f-77fd-44d4-be97-05b820efda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b3e77-63a1-4940-8f4a-7b542df3f6d9",
   "metadata": {},
   "source": [
    "## Dataset Selection & Configuration\n",
    "\n",
    "In this section, you can **choose the dataset** you want to use for the Adversarial Patch attack:\n",
    "\n",
    "- **MNIST**\n",
    "- **CIFAR10**\n",
    "- **CIFAR100**\n",
    "- **ImageNet** **<--**\n",
    "\n",
    "> **IMPORTANT NOTE**: The **official Adversarial Patch paper** explicitly tested the method on **ImageNet** only.  \n",
    "> They focused on **large-scale, high-resolution images**, since the patch needs to occupy a *salient* region in the image to be effective.\n",
    "\n",
    "> **Low-resolution datasets like MNIST or CIFAR10 may not produce reliable results** with this attack,  \n",
    "> because the patch needs space to apply rotation, scaling, and random placement.\n",
    "\n",
    "> Recommended: Use **ImageNet** with pre-trained **ResNet50**, **VGG16**, or **InceptionV3**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644150a5-4f2e-4e8c-b03a-28f7e3cc442f",
   "metadata": {},
   "source": [
    "### **Select Your Dataset & Model Configuration**\n",
    "\n",
    "Update the variables below to set the **dataset** and the **pre-trained model** you want to use.\n",
    "We recommend starting with:\n",
    "\n",
    "```python\n",
    "selected_dataset = \"ImageNet\"\n",
    "selected_attack = \"AdversarialPatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390344aa-5665-4dc2-942e-a0d33ce38fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = \"ImageNet\" # OPTIONS : \"MNIST\", \"CIFAR10\", \"CIFAR100\", and \"ImageNet\"\n",
    "\n",
    "selected_attack = \"AdversarialPatch\" # Used for report name only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc0664-be37-43cc-afc2-f5dfbbf25a85",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "**Define class labels for each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed70d3-60ac-4818-bc90-abaa03e8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the ancestors_name & ancestors_label corresponding to the selected dataset.\n",
    "\n",
    "# Note: All labels are available on Internet. They are not created from us. They are official, often in a .json format.\n",
    "if selected_dataset == \"CIFAR10\":\n",
    "    # Correspondance between name & label for CIFAR10\n",
    "    ancestors_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    # Correspondance between name & label for CIFAR100\n",
    "    ancestors_name = ['apple', 'bridge', 'castle', 'elephant', 'house', 'orange', 'shark', 'table', 'tractor', 'whale']\n",
    "    ancestors_label = ['0', '12', '17', '31', '37', '53', '73', '84', '89', '95']\n",
    "\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['abacus', 'acorn', 'baseball', 'broom', 'brown_bear', 'canoe', 'hippopotamus', 'llama', 'maraca', 'mountain_bike']\n",
    "    ancestors_label = ['398', '988', '429', '462', '294', '472', '344', '355', '641', '671']\n",
    "\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "else:\n",
    "    print(f\"Your {selected_dataset} doesn't exist. Please provide an existing dataset between these choices : CIFAR10, CIFAR100, ImageNet & MNIST.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed2cd3-45de-4c8f-8046-d32c53fd4c1a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "**Fix seed to ensure reproducibility (comment to get random results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9afdd-ab88-409b-a161-2ece4778f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23689023-5108-4f89-8534-930e7c4f78e0",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "#### plot_prediction()\n",
    "\n",
    "This function will be used to display the original / attacked images.\n",
    "\n",
    "The function is designed to display the images correctly, depending on the dataset selected, with the following legend:\n",
    "\n",
    "<font color='green'>Green bars</font> = correct classification <br>\n",
    "<font color='red'>Red bars</font> = Attack target classification <br>\n",
    "<font color='blue'>Blue bars</font> = other classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6ad9-e831-4c77-a869-5896bc68e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_name_dynamic(index, dataset):\n",
    "    \"\"\"Retourne le nom du label en fonction du dataset s√©lectionn√©.\"\"\"\n",
    "    if dataset == \"MNIST\":\n",
    "        return str(index)  # For MNIST, the label name is simply the digit\n",
    "    elif dataset == \"ImageNet\":\n",
    "        return label_to_name(index)  # Use the imagenet_stubs function for ImageNet !\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        return ancestors_name[index]  # Return the name from our list\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        return cifar100_labels[index] if 0 <= index < 100 else \"Unknown\" # Return the name from our list \n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c39259-d85c-4231-815c-27a1b05183e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, probs, correct_class=None, target_class=None):\n",
    "    \"\"\"\n",
    "    Displays an image with predictions in the form of coloured bars :\n",
    "    - Green --> Correct Class\n",
    "    - Red --> Target Class\n",
    "    - Blue --> Other Classes\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "    # Display the picture\n",
    "    if selected_dataset==\"MNIST\":\n",
    "        ax1.imshow(img, cmap=\"gray\") # Force the display in gray level for MNIST !\n",
    "        ax1.axis(\"off\")\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "    # Keep the top 10 classes with highest probabilities\n",
    "    top_ten_indexes = list(probs[0].argsort()[-10:][::-1])\n",
    "    top_probs = probs[0, top_ten_indexes]\n",
    "    labels = [label_to_name_dynamic(i, selected_dataset) for i in top_ten_indexes]\n",
    "\n",
    "\n",
    "    # Bar plot creation with color rules defined above\n",
    "    barlist = ax2.bar(range(10), top_probs, color=\"blue\")  # Blue by default\n",
    "\n",
    "    if target_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(target_class)].set_color(\"red\")  # Red if this is the target class\n",
    "\n",
    "    if correct_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(correct_class)].set_color(\"green\")  # Green if this is the correct class\n",
    "\n",
    "    # Plot Graph\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10), labels, rotation=\"vertical\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Top 10 Predictions\")\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484cab-962a-4634-8254-9bc740b92764",
   "metadata": {},
   "source": [
    "## Step 1: Define Parameters\n",
    "\n",
    "### **Adversarial Patch Paper Parameters**\n",
    "\n",
    "In the original **Adversarial Patch paper** (Brown et al., 2017), the authors proposed a **physically realizable attack** by training a universal patch on natural images with the following typical parameters:\n",
    "\n",
    "- **Scale** ‚Üí Range of patch size relative to the image:\n",
    "  - `scale_min = 0.4` (smallest size = 40% of image)\n",
    "  - `scale_max = 1.0` (up to full image size)\n",
    "\n",
    "- **Rotation** ‚Üí Max random rotation applied during training:\n",
    "  - `rotation_max = 22.5` degrees\n",
    "\n",
    "- **Learning Rate** ‚Üí Patch optimization speed:\n",
    "  - `learning_rate = 5000.0`\n",
    "\n",
    "- **Iterations** ‚Üí Number of gradient descent steps:\n",
    "  - `max_iter = 250`\n",
    "\n",
    "- **Batch Size** ‚Üí Number of images used per optimization step:\n",
    "  - `batch_size = 16`\n",
    "\n",
    "> üß† These parameters control how flexible and robust the patch becomes.\n",
    "Feel free to **tune them depending on your dataset or desired strength of the patch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523f423-ad6e-420d-8773-c32059a742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Patch Parameters (from original paper)\n",
    "\n",
    "scale_min = 0.4          # Minimum scale of patch during training (40% of image size)\n",
    "scale_max = 1.0          # Maximum scale (up to full image size)\n",
    "rotation_max = 22.5      # Maximum rotation in degrees\n",
    "learning_rate = 5000.0   # Learning rate for patch optimization\n",
    "max_iter = 25           # Number of optimization iterations\n",
    "batch_size = 16          # Number of images per optimization step\n",
    "\n",
    "print(f\"Selected Attack: Adversarial Patch | Dataset: {selected_dataset} | Scale Range: [{scale_min}, {scale_max}] | Rotation Max: {rotation_max}¬∞ | Batch Size: {batch_size}, Learning Rate: {learning_rate} | Max Iterations: {max_iter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea443-adc3-480c-8769-94cbc0e4325d",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "**Later, we'll see what EoT is. If you don't know what is EoT, skip this sub-section**\n",
    "\n",
    "*If you want to test EoT Transformation, find parameters below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5c68e-b441-4917-b6e3-6d149ea39d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EoT Transformation\n",
    "angle_max = 22.5 # Rotation angle used for evaluation in degrees\n",
    "eot_angle = angle_max # Maximum angle for sampling range in EoT rotation, applying range [-eot_angle, eot_angle]\n",
    "eot_samples = 10 # Number of samples with random rotations in parallel per loss gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcc897-543a-4c2a-a813-b3ea9699161c",
   "metadata": {},
   "source": [
    "### Dataset-Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101319c-706d-496f-86d1-b745b7834a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# ImageNet has 1000 classes, CIFAR100 100 classes, and CIFAR10 & MNIST has 10 classes.\n",
    "nb_classes = 1000 if selected_dataset == \"ImageNet\" else 100 if selected_dataset == \"CIFAR100\" else 10\n",
    "\n",
    "# ImageNet Images Dimension : (299,299,3), CIFAR10 & CIFAR100 : (32,32,3), and MNIST : (28,28,1)\n",
    "input_shape = (224, 224, 3) if selected_dataset == \"ImageNet\" else (32, 32, 3) if \"CIFAR\" in selected_dataset else (28, 28, 1)\n",
    "print(input_shape)\n",
    "# ImageNet use often a specific preprocessing. For the others dataset, it still an adapted normalisation (0,1)\n",
    "preprocessing = None if selected_dataset == \"ImageNet\" else (0.0, 1.0)  # Normalisation adapt√©e\n",
    "\n",
    "# Clip values \n",
    "clip_values = (0.0, 1.0)  # Same for all datasets\n",
    "\n",
    "# Target Class Definition (You can change, here are just some examples)\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    y_target = np.array([641])  # \"maraca\"\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    y_target = np.array([3])  # \"bear\"\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    y_target = np.array([1])  # \"automobile\"\n",
    "else:  # MNIST\n",
    "    y_target = np.array([np.random.randint(0, 10)])  # random digit between 0 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787fe1-db8a-4d32-aa92-2f11ecd764af",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset Data & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73add696-3799-40c0-ac09-31ed827579e6",
   "metadata": {},
   "source": [
    "In this step, we **load all dataset images and their labels into memory**.\n",
    "\n",
    "#### **How does it work?**\n",
    "1. We retrive the dataset path (`datasets/selected_dataset/`).\n",
    "2. We read all images from the dataset folders.\n",
    "3. We **normalize** the images (scale pixel values between `[0, 1]`).\n",
    "4. We store **both images and labels** for further processing.\n",
    "\n",
    " -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5041c04-64ef-435c-9cb9-2374e84ca1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Initializations\n",
    "x_all, y_all, original_images = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e9aff-599b-46b4-80fa-b06e92f9673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get our dataset path in our computer to keep all pictures and put them into our lists.\n",
    "dataset_path = os.path.join(\"datasets\", selected_dataset)\n",
    "# Check\n",
    "assert(dataset_path==\"datasets/\"+selected_dataset) # If nothing : It's ok. Otherwise, you will get an error if the dataset path doesn't exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b009a5-c170-4d80-8d6b-4de6e1695908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from the selected dataset\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        continue\n",
    "    \n",
    "    for img_file in sorted(os.listdir(class_path)):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        if selected_dataset == \"MNIST\":\n",
    "            im = load_img(img_path, color_mode=\"grayscale\", target_size=(28, 28))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        elif selected_dataset == \"ImageNet\":\n",
    "            im = load_img(img_path, target_size=(224, 224))\n",
    "            im_array = img_to_array(im)\n",
    "\n",
    "        elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "            im = load_img(img_path, target_size=(32, 32))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        x = (im_array / 255.0).astype(np.float32)\n",
    "        \n",
    "        x_all.append(x)\n",
    "        y_all.append(int(class_label))\n",
    "        original_images.append(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dd7e1-03f5-42db-a0c6-48fb139331f6",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "#### Display Dataset (Optional)\n",
    "**You can choose to display all images or only one image per class)**\n",
    "\n",
    "#### How to enable visualization ?\n",
    "- To display **ALL images** --> **Uncomment the loop bellow**.\n",
    "- To display **ONLY 1 image per class** --> **Set `display_all_images = False`**.\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d59b-406f-4883-93bb-f0961d426afc",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying of the 100 pictures (can be long, you can modify the code to display only 1 picture per class if you want)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        print(class_path)\n",
    "        print(\"No os Path\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Class : {class_name} (Label: {class_label})\")\n",
    "    \n",
    "     # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = sorted(os.listdir(class_path))[:1] if not display_all_images else sorted(os.listdir(class_path))\n",
    "    # Go through the 10 pictures of each classes\n",
    "    for img_file in images_to_show:\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        # Load & Normalize the picture\n",
    "        im = load_img(img_path, target_size=(299, 299))\n",
    "        im_array = img_to_array(im)\n",
    "\n",
    "        # Displaying all pictures\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(im_array.astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Class: {class_name} | {img_file}\", fontsize=10, fontweight=\"bold\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{img_file} well displayed in : {class_name}\")\n",
    "\n",
    "print(f\"All of the {len(ancestors_name)} classes & their images has been displayed !\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7c65f-03a9-44f8-913a-09d2957ef192",
   "metadata": {},
   "source": [
    "### Convert to Numpy Arrays for TensorFlow\n",
    "Since TensorFlow requires NumPy arrays, we convert our lists into arrays.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bde4b5-0d8d-4366-8a99-3c770534f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a numpy array\n",
    "x_all = np.array(x_all)\n",
    "y_all = np.array(y_all).astype(int).flatten()\n",
    "\n",
    "# Check\n",
    "#for img_x, img_y in zip(x_all, y_all):\n",
    "#    print(f\"x_all shape: {x_all.shape}\")  # (N, H, W, C)\n",
    "#    print(f\"y_all shape: {y_all.shape}\")  # (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dded0f-4832-4d83-9055-c24bb6c7fd57",
   "metadata": {},
   "source": [
    "## Step 3 : Load Model & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411ad0a-c669-417d-9a28-b86681dd231c",
   "metadata": {},
   "source": [
    "### 1. Loading Dataset for Model Training\n",
    "Before creating the model, we **load and preprocess** the dataset to ensure it is correctly formatted for TensorFlow.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc8c37-3fb4-48d1-85b9-2a7175b4e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_dataset == \"MNIST\":\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)\n",
    "    \n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")\n",
    "\n",
    "    # We reproduce the list of all classes of CIFAR100\n",
    "    cifar100_labels = [\n",
    "    \"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\",\n",
    "    \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\",\n",
    "    \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"crab\", \"crocodile\", \"cup\", \"dinosaur\",\n",
    "    \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"computer_keyboard\",\n",
    "    \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\",\n",
    "    \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\",\n",
    "    \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\", \"rose\", \"sea\",\n",
    "    \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\", \"squirrel\", \"streetcar\", \"sunflower\",\n",
    "    \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\", \"train\", \"trout\", \"tulip\",\n",
    "    \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"\n",
    "]\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51714d-e20b-4bab-8d09-5fc35f30e8cc",
   "metadata": {},
   "source": [
    "### 2. Model Selection & Architecture\n",
    "Different models are used depending on the selected dataset:\n",
    "- **ImageNet** --> **InceptionV3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aed09b-80e0-48cc-b4c2-3659ebe7612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= IMAGENET =============\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    print(f\"SELECTED MODEL : ResNet50\")  \n",
    "    model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "\n",
    "# ============= ERROR =============\n",
    "else:\n",
    "    raise ValueError(f\"Error: Dataset '{selected_dataset} not recognized. Please ensure to use one of this dataset : ImageNet, CIFAR10, CIFAR100 or MNIST.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1fdd7-9038-4b6d-9326-227751a2a427",
   "metadata": {},
   "source": [
    "## Step 4 : Create the ART Classifier & Configure the Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53db541-c3ca-4a5f-8d75-c7dd4557d65d",
   "metadata": {},
   "source": [
    "Now that the model is **trained and ready**, we integrate it into **ART (Adversarial Robustness Toolbox)**.\n",
    "\n",
    "#### What is happening here ?\n",
    "1. We **create a classifier** for ART based on the trained model\n",
    "2. We **define an adversarial attack** (FGSM in this case)\n",
    "3. The attack can be **targeted or untargeted**, and parameters are fully configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597176e7-cdfb-4cc7-8f51-1f74f1a83f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model,\n",
    "                                    nb_classes=1000,\n",
    "                                    input_shape=(224, 224, 3),\n",
    "                                    clip_values=(0, 255),\n",
    "                                    preprocessing = ([103.939, 116.779, 123.68], [1.0, 1.0, 1.0]),  # BGR mean values\n",
    "                                    loss_object=tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfaf70-dae6-486c-8c62-41ce8639136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = AdversarialPatch(\n",
    "    classifier=classifier,\n",
    "    rotation_max=rotation_max,\n",
    "    scale_min=scale_min,\n",
    "    scale_max=scale_max,\n",
    "    learning_rate=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    batch_size=batch_size,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4134d4-b0a9-44d5-8259-8a95d18ffa4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ONE-HOT ENCODING (Y_TARGET)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb94537-f95e-417d-8b4a-3bcba9d5c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = np.zeros(1000)\n",
    "y_one_hot[y_target] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4934fb-6165-4c6c-885f-18f50370a37e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**REPEAT TARGET FOR ALL IMAGES**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e6e30-7108-451d-a620-e1abef3160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we repeat the same target for all images.\n",
    "y_target_one_hot_encoded = np.tile(y_one_hot, (x_all.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34060c84-e2de-438b-995b-119d73d6b112",
   "metadata": {},
   "source": [
    "## Step 5 : Predict Clean (Original) Images BEFORE the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c927354-cc5b-45d0-bcb3-65f152c94efd",
   "metadata": {},
   "source": [
    "Before applying any attack, we **predict the clean images** with our trained model.\n",
    "\n",
    "#### What happens here ?\n",
    "1. We run the classifier on all images **before the attack**.\n",
    "2. We display the **top-10 predictions** for each image.\n",
    "3. You can choose to **display all images or only one per class**.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54e745-a5c0-485a-8a21-b30f3f08cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Apply specific process for ResNet50\n",
    "x_all_preprocessed = preprocess_input(x_all.copy() * 255.0) \n",
    "\n",
    "# Predict using the classifier\n",
    "y_pred_clean_all = classifier.predict(x_all_preprocessed)\n",
    "\n",
    "# Check prediction shape\n",
    "print(\"Shape of Clean Predictions:\", y_pred_clean_all.shape)  # Expected (N, nb_classes)\n",
    "\n",
    "top1_correct = np.mean(np.argmax(y_pred_clean_all, axis=1) == y_all) * 100\n",
    "print(f\"Top-1 Accuracy on Clean Images: {top1_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570014ad-2367-4279-8515-96ff1eb668e3",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "#### Display Clean Images & Predictions\n",
    "You can **choose whether to display all images or just one per class**.\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a7ed8-e5c7-4a3b-ba27-bc7e4d6e4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying Clean Images with Predictions\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    # Get all images from this class\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No Images found for {class_name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_all[index]),  # Original clean image\n",
    "            y_pred_clean_all[index].reshape(1, -1),  # Reshaped prediction\n",
    "            correct_class=y_all[index],  # True class\n",
    "            target_class=None  # No target class for clean images\n",
    "        )\n",
    "        print(f\"Image {index} displayed for class: {class_name}\")\n",
    "\n",
    "print(f\"\\n All {len(x_all)} clean images have been processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526ff66-f8ff-499a-b44d-5ed770fdd749",
   "metadata": {},
   "source": [
    "## Step 6: Generate and Evaluate Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7f94a-fb49-48f2-a6b1-a4ca9cb6dab6",
   "metadata": {},
   "source": [
    "Now, we **generate adversarial examples** and evaluate the effectiveness of the attack.\n",
    "\n",
    " **What happens here?**\n",
    "1. We **generate adversarial examples** using the selected attack.\n",
    "2. We **save the adversarial images** for later analysis. (optional)\n",
    "3. We **evaluate the attack's success** (accuracy, confidence scores, and performance metrics).\n",
    "4. We **generate a detailed report** summarizing the attack results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a844151-a291-4e12-95b5-3af1cc1b2bbf",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### GENERATE THE PATCH\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e17c6-ac35-4194-a7d9-c09ae1aa8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch, patch_mask = attack.generate(x=x_all, y=y_target_one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9166eb-997e-4243-909f-dabf2e85fbb0",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### GENERATE THE ATTACK\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312913f-df1a-4667-a4c6-d94048fe0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_images = attack.apply_patch(x_all, scale=0.5)\n",
    "\n",
    "# Prediction on Adversarial (Patched) Images\n",
    "y_pred_adv_all = classifier.predict(patched_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff0f8d-9591-441b-af62-c1a038141487",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "**Do you want to save all adversarial images?**  \n",
    "- **YES** ‚Üí Uncomment the saving function below.\n",
    "- **NO** ‚Üí Comment the function to skip saving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b885-6864-4199-9a58-632a74a93a13",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Define the save path for adversarial images\n",
    "adv_save_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset)\n",
    "os.makedirs(adv_save_path, exist_ok=True)  \n",
    "\n",
    "# Iterate through all classes to save adversarial images\n",
    "class_counters = {class_name: 1 for class_name in ancestors_name}  # Dictionary to track image indices per class\n",
    "\n",
    "for adv_img, class_label in zip(x_adv_all, y_all.flatten()):  # Ensure y_all is 1D\n",
    "    # Find the class name corresponding to the label\n",
    "    if str(class_label) not in ancestors_label:\n",
    "        print(f\"Label {class_label} not found in ancestors_label, skipping image.\")\n",
    "        continue  \n",
    "\n",
    "    class_index = ancestors_label.index(str(class_label))\n",
    "    class_name = ancestors_name[class_index]\n",
    "\n",
    "    # Determine the subfolder for the class\n",
    "    class_folder = os.path.join(adv_save_path, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True) \n",
    "\n",
    "    # Generate a unique filename with a counter (e.g., abacus1_adv.jpeg, abacus2_adv.jpeg, ..., acorn1_adv.jpeg, ...)\n",
    "    img_filename = f\"{class_name}{class_counters[class_name]:02d}_adv.jpeg\"\n",
    "    img_path = os.path.join(class_folder, img_filename)\n",
    "\n",
    "    # Convert and save the image\n",
    "    img = array_to_img(adv_img)\n",
    "    img.save(img_path, \"JPEG\")\n",
    "\n",
    "    print(f\"Image saved : {img_path}\")\n",
    "\n",
    "    # Increment the counter for this class\n",
    "    class_counters[class_name] += 1\n",
    "\n",
    "print(f\"\\nAll  {len(x_adv_all)} adversarial images have been successfully saved!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254fe32-85b2-4c37-85ce-1009bbe05e83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate Adversarial Example\n",
    "\n",
    "We now evaluate the adversarial examples by:\n",
    "- Measuring the **model's accuracy** on these images.\n",
    "- Computing the **confidence score** of predictions.\n",
    "- Generating a **visual comparison** between clean and adversarial images.\n",
    "\n",
    "---\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4390776-4d46-4ae0-8dba-3e033b99c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False\n",
    "\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No images found for {class_name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "\n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(patched_images[index]),\n",
    "            y_pred_adv_all[index].reshape(1, -1),\n",
    "            correct_class=y_all[index],\n",
    "            target_class=y_target\n",
    "        )\n",
    "        print(f\"Adversarial Image {index} displayed for class: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d4cd8-dd10-4ac8-9b8d-f9e82797bb5b",
   "metadata": {},
   "source": [
    "### Compute Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f72e-928c-4d21-8da9-e69c53add45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence score\n",
    "confidence_scores = np.max(y_pred_clean_all, axis=1)\n",
    "average_confidence = np.mean(confidence_scores) * 100\n",
    "\n",
    "# Compute Tok-K Accuracy\n",
    "def compute_accuracy(predictions, true_labels, top_k=1):\n",
    "    top_k_preds = np.argsort(predictions, axis=1)[:, -top_k:]\n",
    "    match = np.any(top_k_preds == np.array(true_labels).reshape(-1, 1), axis=1)\n",
    "    return np.mean(match) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a0b18-9939-4b00-80a7-5dd7c57f141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_top1 = compute_accuracy(y_pred_clean_all, y_all, top_k=1)\n",
    "clean_top5 = compute_accuracy(y_pred_clean_all, y_all, top_k=5)\n",
    "adv_top1 = compute_accuracy(y_pred_adv_all, y_all, top_k=1)\n",
    "adv_top5 = compute_accuracy(y_pred_adv_all, y_all, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f2c9-093e-45ec-b3b7-e4079e242270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Performance Results\n",
    "attack_name = \"AdversarialPatch\" if isinstance(attack, AdversarialPatch) else \"iter. basic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98b0b-0281-45c0-8d8b-a56b1ab187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Selected Attack: Adversarial Patch | Dataset: {selected_dataset} | Scale Range: [{scale_min}, {scale_max}] | Rotation Max: {rotation_max}¬∞ | Batch Size: {batch_size}, Learning Rate: {learning_rate} | Max Iterations: {max_iter}\")\n",
    "print(f\"Clean Images : Top-1 : {clean_top1:.2f}% | Top-5 : {clean_top5:.1f}%\")\n",
    "print(f\"Adv. Images  : Top-1 : {adv_top1:.2f}% | Top-5 : {adv_top5:.1f}%\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Confidence Score: {average_confidence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22d39a-2ee4-4ae4-ab55-4e63057fd0a0",
   "metadata": {},
   "source": [
    "### Generate a Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584bf76-61d4-46cd-89b4-776d59ae88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round epsilon for eadability\n",
    "#eps_rounded = round(epsilon, 3)\n",
    "\n",
    "# Define report save path\n",
    "if selected_dataset == \"MNIST\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_{selected_mnist_model}_report.txt\"\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}.txt\"\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_ResNet50_report.txt\"\n",
    "else:\n",
    "    print(f\"This {selected_dataset} is not recognized. Be careful to provide an existing dataset between MNIST, CIFAR\")\n",
    "    \n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Report Title\n",
    "    f.write(f\"====== {selected_attack} Adversarial Attack Report) ======\\n\\n\")\n",
    "\n",
    "    # Information generation for each image\n",
    "    for i in range(len(y_pred_adv_all)):  \n",
    "        # Find the class index in ancestors_label\n",
    "        class_label = str(int(y_all[i]))\n",
    "        if class_label in ancestors_label:\n",
    "            class_index = ancestors_label.index(class_label)  # Get index in ancestors_name\n",
    "            class_name = ancestors_name[class_index]  # Retrieve class name\n",
    "        else:\n",
    "            class_name = \"Unknown\"  # If not found, prevent error\n",
    "\n",
    "        # Original Image file name (ensuring correct numbering)\n",
    "        original_image_name = f\"{class_name}{(i % 10) + 1:02d}.jpeg\"\n",
    "\n",
    "        # Predict Class for the original image (top-1)\n",
    "        clean_pred_index = np.argmax(y_pred_clean_all[i])\n",
    "\n",
    "        # Predict Class for the Adversarial image (top-1)\n",
    "        adv_pred_index = np.argmax(y_pred_adv_all[i])\n",
    "\n",
    "        # Prediction\n",
    "        clean_pred_label = label_to_name_dynamic(clean_pred_index, selected_dataset)\n",
    "        adv_pred_label = label_to_name_dynamic(adv_pred_index, selected_dataset)\n",
    "\n",
    "\n",
    "        # Targeted or Untargeted Scenario Attack\n",
    "        attack_type = \"Targeted\" if attack.targeted else \"Untargeted\"\n",
    "\n",
    "        # If Targeted : Target Class\n",
    "        target_label = label_to_name(y_target[0]) if attack.targeted else \"N/A\"\n",
    "\n",
    "        # Write results in the report:\n",
    "        f.write(f\"------ CLASS : {class_name.upper()} ------\\n\")\n",
    "        f.write(f\"Original image name : {original_image_name}\\n\")\n",
    "        f.write(f\"Original Prediction : {clean_pred_label}\\n\")\n",
    "        f.write(f\"Targeted / Untargeted : {attack_type}\\n\")\n",
    "        if attack.targeted:\n",
    "            f.write(f\"Target Class : {target_label}\\n\")\n",
    "        f.write(f\"Adversarial Prediction : {adv_pred_label}\\n\")\n",
    "        f.write(\"------------------------------------------------\\n\\n\")\n",
    "\n",
    "    # Performance Summary at the end of the file\n",
    "    f.write(\"============ PERFORMANCE RESUME ============\\n\")\n",
    "    f.write(f\"Selected Attack: Adversarial Patch | Dataset: {selected_dataset} | Scale Range: [{scale_min}, {scale_max}] | Rotation Max: {rotation_max}¬∞ | Batch Size: {batch_size}, Learning Rate: {learning_rate} | Max Iterations: {max_iter}\")\n",
    "    f.write(f\"Clean Images : Top-1 : {clean_top1:.1f}% | Top-5 : {clean_top5:.1f}%\\n\")\n",
    "    f.write(f\"Adv. Images  : Top-1 : {adv_top1:.1f}% | Top-5 : {adv_top5:.1f}%\\n\")\n",
    "    f.write(\"----------------------------------------------------------\")\n",
    "    f.write(f\"Confidence Score: {average_confidence:.2f}%\")\n",
    "\n",
    "    attack_eff_top1 = 100 - adv_top1\n",
    "    attack_eff_top5 = 100 - adv_top5\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"{selected_attack} Efficiency : Top-1 : {attack_eff_top1:.1f}% | Top-5 : {attack_eff_top5:.1f}%\\n\")\n",
    "\n",
    "# Saving Confirmation\n",
    "print(f\"Report saved : {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a025e-ddb1-4939-b24b-bcf8373c50a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Going further (optional) : Expectation Over Transformation (EoT) \n",
    "Adversarial attacks like **FGSM** are often **sensitive to image transformations** such as **rotation, scaling, or noise**.\n",
    "\n",
    "**Why does this happen?**  \n",
    "- A small rotation (e.g., **5¬∞**) can **invalidate** an adversarial example.\n",
    "- This **breaks the perturbation pattern** that misleads the classifier.\n",
    "  \n",
    "**How does EoT (Expectation Over Transformation) help?**  \n",
    "- Instead of using **a single perturbed image**, EoT **randomly transforms** the image (rotation, blur, etc.).\n",
    "- The attack is then **optimized over multiple transformations**, making it **more robust**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66723512-78b5-4b39-a210-545272a7c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "# Define rotation angles to test\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Apply rotation to all adversarial examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(0, 1), order=1, mode='constant')\n",
    "        for img in x_adv_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "# Get predictions after rotation\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995db06-f091-426e-89f2-30041d9861cc",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d583b-7e22-4b62-ac64-c0f727f6d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  # Set to True to display all, False to show a few per angle\n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}¬∞\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:\n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa39f3d-a018-4ac6-85b4-604077cfa0ff",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a7f96-3779-403c-9a78-4f48e713819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}¬∞ ‚Üí Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39c55a-fd95-4681-b97e-1b09b79835f1",
   "metadata": {},
   "source": [
    "## Step 7: Apply Expectation Over Transformation (EoT)\n",
    "\n",
    "### **What is EoT and Why is it Useful?**\n",
    "FGSM and adversarial attacks often **fail** when images undergo transformations like **rotations**.\n",
    "\n",
    "**EoT (Expectation Over Transformation) mitigates this issue by:**\n",
    "- Generating multiple **randomly transformed** versions of the adversarial image.\n",
    "- Applying these transformations **during model evaluation** (predictions & gradients).\n",
    "- Making the adversarial attack **robust to transformations** like **rotations, noise, and blur**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enable EoT in ART**\n",
    "We use ART‚Äôs **`EoTImageRotationTensorFlow`** to introduce **random rotations** during classification.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00187be4-535b-4780-8831-418fce1f3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ART Classifier with EoT\n",
    "eot_rotation = EoTImageRotationTensorFlow(nb_samples=eot_samples,  \n",
    "                                          clip_values=clip_values,  \n",
    "                                          angles=eot_angle)  # Random rotation range\n",
    "\n",
    "classifier_eot = TensorFlowV2Classifier(model=model,\n",
    "                                        nb_classes=nb_classes,\n",
    "                                        loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                        preprocessing=preprocessing,\n",
    "                                        preprocessing_defences=[eot_rotation],  # EoT applied\n",
    "                                        clip_values=clip_values,\n",
    "                                        input_shape=input_shape)\n",
    "\n",
    "print(f\"EoT Classifier created with {eot_samples} transformation samples per evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116e55a-284c-4fe5-b4b6-3dd27961243d",
   "metadata": {},
   "source": [
    "### Generate Adversarial Examples with EoT\n",
    "We generate **adversarial examples** that remain effective even **after transformations**.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235e4b7-6331-44d7-99bc-276b49b9bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare target labels for targeted attacks\n",
    "y_target_one_hot = np.zeros((1, nb_classes), dtype=np.float32)\n",
    "y_target_one_hot[0, name_to_label(\"guacamole\")] = 1.0  \n",
    "y_target_all = np.tile(y_target_one_hot, (len(x_all), 1))  \n",
    "\n",
    "x_adv_eot_all = []\n",
    "\n",
    "for i in tqdm(range(len(x_all)), desc=\"Generating EoT Examples\"):\n",
    "    x_i = np.expand_dims(x_all[i], axis=0)  \n",
    "    y_i = np.expand_dims(y_target_all[i], axis=0)  \n",
    "\n",
    "    if attack.targeted:\n",
    "        x_adv_i = attack.generate(x=x_i, y=y_i)\n",
    "    else:\n",
    "        x_adv_i = attack.generate(x=x_i)\n",
    "\n",
    "    x_adv_eot_all.append(np.squeeze(x_adv_i))  \n",
    "\n",
    "x_adv_eot_all = np.array(x_adv_eot_all)\n",
    "\n",
    "print(f\"Shape of EoT Adversarial Examples: {x_adv_eot_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0278d2-9459-4d43-b261-6db1e9b07a1f",
   "metadata": {},
   "source": [
    "### Apply Rotation to Adversarial Examples\n",
    "We now test the **robustness** of these adversarial examples by **rotating them** at different angles.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69ea7a-b552-4f9e-bd92-baaa5719ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotation angles\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Rotate and Evaluate Adversarial Examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(1, 2), order=1, mode='constant')\n",
    "        for img in x_adv_eot_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342a24c-5eb9-4e8c-8bcb-9fe1feacf2f4",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few per rotation angle**.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece90eb-c169-4be1-becf-42a7df619fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  \n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}¬∞\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:  \n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb82c3-856a-4174-a218-3e3b80061d17",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc01fc-c6b9-43c7-ada2-c2affe63d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}¬∞ ‚Üí Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2709c-5a09-4a6a-a670-8aeafacdc02f",
   "metadata": {},
   "source": [
    "### Generate a Report on EoT Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c28d0f-e529-43fb-a248-67e6914fe814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define report save path\n",
    "report_filename = f\"EoT_{selected_attack}_{selected_dataset}_eps={round(epsilon, 2)}.txt\"\n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "# Generate Report\n",
    "print(\"\\nGenerating EoT attack report...\")\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"====== EoT Adversarial Attack Report (Œµ = {round(epsilon, 2)}) ======\\n\\n\")\n",
    "    \n",
    "    for angle in rotation_angles:\n",
    "        adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "        adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "        \n",
    "        f.write(f\"\\n=== Rotation {angle}¬∞ ===\\n\")\n",
    "        f.write(f\"Top-1 Accuracy: {adv_top1_rotated:.1f}%\\n\")\n",
    "        f.write(f\"Top-5 Accuracy: {adv_top5_rotated:.1f}%\\n\")\n",
    "\n",
    "print(f\"EoT Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c1bf-f0f3-460e-9e39-9f2056710a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whiteboxattack)",
   "language": "python",
   "name": "whiteboxattack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
