<h1 align="center">
# ğŸ›¡ï¸ Adversarial Attacks Explained & Implemented ğŸ¯  
</h1>

![GitHub Repo Stars](https://img.shields.io/github/stars/Quinet-Maxence/adversarial-attacks?style=social)  
![GitHub Repo Forks](https://img.shields.io/github/forks/Quinet-Maxence/adversarial-attacks?style=social)  
![GitHub Last Commit](https://img.shields.io/github/last-commit/Quinet-Maxence/adversarial-attacks)  

ğŸ“Œ **Welcome to the most comprehensive collection of Adversarial Attacks!**  
This repository provides **detailed explanations, implementations, and visualizations** of **20 different adversarial attacks** using **Python, TensorFlow, and the Adversarial Robustness Toolbox (ART)**.  

ğŸ”¥ **Why this repository?**  
The official ART documentation can be **difficult to follow**, and many implementations lack detailed explanations.  
This repository **bridges the gap** by providing:  
âœ” **Well-documented Jupyter Notebooks** ğŸ“’  
âœ” **Step-by-step attack implementations** âš¡  
âœ” **Examples on multiple datasets** ğŸ“Š  
âœ” **Pre-trained models for easy testing** ğŸ†  

---

---
## ğŸ“š Table of Contents  

- [ğŸ“– What are Adversarial Attacks?](#-what-are-adversarial-attacks)  
- [ğŸš€ How to Use This Repository?](#-how-to-use-this-repository)  
- [âš¡ Implemented Attacks](#-implemented-attacks)  
  - [ğŸ¯ White-Box Attacks](#-white-box-attacks-full-model-access)  
  - [ğŸ•µï¸â€â™‚ï¸ Black-Box Attacks](#-black-box-attacks-no-access-to-model-weights)  
  - [ğŸ­ Adversarial Patch Attacks](#-adversarial-patch-attacks)  
  - [ğŸ“Œ Universal & Feature-Space Attacks](#-universal--feature-space-attacks)  
- [ğŸ† Datasets Used](#-datasets-used)  
- [ğŸ›  Requirements](#-requirements)  
- [ğŸ¤ Contributing](#-contributing)  
- [ğŸ›  How to Contribute?](#-how-to-contribute)  
- [ğŸ“œ References & Credits](#-references--credits)  


---

## ğŸ“– What are Adversarial Attacks?  

Adversarial Attacks are **techniques that manipulate machine learning models** by applying **carefully crafted perturbations** to input data. These attacks can:  
- **Mislead classifiers** into making incorrect predictions.  
- **Bypass security models** (e.g., fool facial recognition or self-driving car algorithms).  
- **Expose weaknesses** in Deep Learning architectures.  

ğŸ“Œ **Example of an adversarial attack:**  
A classifier originally predicts **"Panda"** with **57.7% confidence**.  
After a **tiny perturbation**, it predicts **"Gibbon"** with **99.3% confidence!**  

<p align="center">
  <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQihyCOg3MeVDoOHNU7ept_fNaikPaJ01yHfQ&s" width="600"/>
</p>

---

## ğŸš€ How to Use This Repository?  

### ğŸ“Œ 1. Clone the Repository  
```bash
git clone https://github.com/Quinet-Maxence/adversarial-attacks.git
```
### ğŸ“Œ 2. Create conda environment
```bash
conda create ....
```
### ğŸ“Œ 3. Install Dependencies
```bash
cd adversarial-attacks
```
### ğŸ“Œ 4. Open Jupyter notebook
```bash
jupyter notebook
```

Note: A step-by-step tutorial is available in the ```environment_configuration.txt``` file !

---

## âš¡ Implemented Attacks  

This repository covers **both White-Box and Black-Box** adversarial attacks, structured as **one Jupyter Notebook per attack**.  

### ğŸ¯ White-Box Attacks (Full Model Access)  
| Attack Name | Description | Notebook |
|------------|-------------|-----------|
| **FGSM** (Fast Gradient Sign Method) | Classic one-step gradient-based attack. | [`FGSM_AdversarialAttack.ipynb`](FGSM_AdversarialAttack.ipynb) |
| **PGD** (Projected Gradient Descent) | Iterative version of FGSM with stronger perturbations. | [`PGD_AdversarialAttack.ipynb`](PGD_AdversarialAttack.ipynb) |
| **DeepFool** | Minimal perturbation attack based on decision boundaries. | [`DeepFool_AdversarialAttack.ipynb`](DeepFool_AdversarialAttack.ipynb) |
| **Carlini & Wagner (C&W)** | One of the strongest optimization-based attacks. | [`CW_AdversarialAttack.ipynb`](CW_AdversarialAttack.ipynb) |
| **Elastic Net Attack (EAD)** | A variation of C&W using Elastic Net regularization. | [`EAD_AdversarialAttack.ipynb`](EAD_AdversarialAttack.ipynb) |
| **NewtonFool** | Attack that reduces confidence in the true class. | [`NewtonFool_AdversarialAttack.ipynb`](NewtonFool_AdversarialAttack.ipynb) |
| **Jacobian-Based Saliency Map Attack (JSMA)** | Perturbation focused on important pixels. | [`JSMA_AdversarialAttack.ipynb`](JSMA_AdversarialAttack.ipynb) |

### ğŸ•µï¸â€â™‚ï¸ Black-Box Attacks (No Access to Model Weights)  
| Attack Name | Description | Notebook |
|------------|-------------|-----------|
| **Boundary Attack** | Queries the model iteratively to fool it. | [`Boundary_AdversarialAttack.ipynb`](Boundary_AdversarialAttack.ipynb) |
| **HopSkipJump Attack** | A more efficient version of Boundary Attack. | [`HopSkipJump_AdversarialAttack.ipynb`](HopSkipJump_AdversarialAttack.ipynb) |
| **ZOO (Zero Order Optimization)** | Optimization-based black-box attack. | [`ZOO_AdversarialAttack.ipynb`](ZOO_AdversarialAttack.ipynb) |
| **Square Attack** | Query-efficient adversarial attack. | [`Square_AdversarialAttack.ipynb`](Square_AdversarialAttack.ipynb) |

### ğŸ­ Adversarial Patch Attacks  
| Attack Name | Description | Notebook |
|------------|-------------|-----------|
| **Adversarial Patch** | Fooling models with a patch visible in the image. | [`AdversarialPatch_AdversarialAttack.ipynb`](AdversarialPatch_AdversarialAttack.ipynb) |
| **DPatch** | A variation of Adversarial Patch for object detection. | [`DPatch_AdversarialAttack.ipynb`](DPatch_AdversarialAttack.ipynb) |
| **Robust DPatch** | An improved version of DPatch. | [`RobustDPatch_AdversarialAttack.ipynb`](RobustDPatch_AdversarialAttack.ipynb) |

### ğŸ“Œ Universal & Feature-Space Attacks  
| Attack Name | Description | Notebook |
|------------|-------------|-----------|
| **Targeted Universal Perturbations** | A single perturbation fools many images. | [`UniversalPerturbation_AdversarialAttack.ipynb`](UniversalPerturbation_AdversarialAttack.ipynb) |
| **Feature Adversaries** | Attacks focused on high-level feature representations. | [`FeatureAdversaries_AdversarialAttack.ipynb`](FeatureAdversaries_AdversarialAttack.ipynb) |
| **Virtual Adversarial Method (VAT)** | Regularization-based adversarial method. | [`VAT_AdversarialAttack.ipynb`](VAT_AdversarialAttack.ipynb) |

---

## ğŸ† Datasets Used  

Each attack is tested on various datasets:  

| Dataset | Description | Number of Classes | Image Shape |
|---------|-------------|------------------|-------------|
| **MNIST** | Handwritten digit recognition | 10 | (28, 28, 1) |
| **CIFAR-10** | Small object classification | 10 | (32, 32, 3) |
| **CIFAR-100** | 100-class object classification | 100 | (32, 32, 3) |
| **ImageNet (Subset)** | Large-scale image classification | 1,000 | (299, 299, 3) |

ğŸ“Œ **Note:** You can choose your dataset inside each notebook! ğŸ“‚  

---

## ğŸ›  Requirements  

To run the notebooks, ensure you have **Python 3.8+** and install the required libraries using:  

```bash
pip install -r requirements.txt
```

or install them manually:

```bash
pip install tensorflow torch torchvision adversarial-robustness-toolbox numpy matplotlib tqdm scipy
```

### ğŸ“Œ Additional dependencies:

* Jupyter Notebook â†’ pip install notebook
* tqdm (for progress bars) â†’ pip install tqdm
* scipy (for rotations & transformations) â†’ pip install scipy

âœ… Ensure you have CUDA installed for GPU acceleration:
```bash
!nvidia-smi
```
If no GPU is detected, consider updating TensorFlow and PyTorch.

---

## ğŸ¤ Contributing  

ğŸ”¥ This repository is **open for contributions**! If you:  
âœ” Found an **error** in an implementation? **Create an issue!**  
âœ” Have a **new attack** to add? **Submit a pull request!**  
âœ” Want to **discuss adversarial robustness**? **Join the discussions!**  

---

## ğŸ›  How to Contribute?  

We welcome all contributions! Follow these steps to add your improvements:  

### ğŸ“Œ 1. Fork the repository  
Click on the **Fork** button at the top right corner of this page.  

### ğŸ“Œ 2. Clone your fork  
```bash
git clone https://github.com/Quinet-Maxence/adversarial-attacks.git
cd adversarial-attacks
```

### ğŸ“Œ 3. Create a new branch
```bash
git checkout -b my-new-feature
```  

### ğŸ“Œ 4. Make your changes
* Add a new adversarial attack ğŸ›¡ï¸
* Improve documentation ğŸ“
* Fix bugs ğŸ

### ğŸ“Œ 5. Commit your changes
```bash
git add .
git commit -m "Added XYZ adversarial attack"
```

### ğŸ“Œ 6. Push to your fork
```bash
git push origin my-new-feature
```

### ğŸ“Œ 7. Submit a pull request ğŸš€
Go to **Pull Requests** on this repository and submit your PR for review!
âœ… Once approved, your contributions will be added to the project!

---

### âœ… **ğŸ“Œ Section "References & Credits"**  

## ğŸ“œ References & Credits  

This repository is based on **ART (Adversarial Robustness Toolbox)** and research papers, including:  

ğŸ“– **Ian Goodfellow et al., "Explaining and Harnessing Adversarial Examples" (2015)**  
ğŸ”— [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572)  

ğŸ“– **Carlini & Wagner, "Towards Evaluating the Robustness of Neural Networks" (2017)**  
ğŸ”— [https://arxiv.org/abs/1608.04644](https://arxiv.org/abs/1608.04644)  

ğŸ“– **Su et al., "One Pixel Attack for Fooling Deep Neural Networks" (2019)**  
ğŸ”— [https://arxiv.org/abs/1710.08864](https://arxiv.org/abs/1710.08864)  

ğŸ“Œ **Libraries Used:**  
- **Adversarial Robustness Toolbox (ART)** â†’ [https://github.com/Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)  
- **TensorFlow** â†’ [https://www.tensorflow.org/](https://www.tensorflow.org/)  
- **PyTorch** â†’ [https://pytorch.org/](https://pytorch.org/)  

