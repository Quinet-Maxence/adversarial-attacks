{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1091dcb-e0e0-4767-95df-30247ae372dc",
   "metadata": {},
   "source": [
    "# Carlini & Wagner (C&W) L_inf - Adversarial Attacks\n",
    "### Paper link : https://arxiv.org/abs/1608.04644"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63cd03-35c1-4228-a9f9-851b27ed02eb",
   "metadata": {},
   "source": [
    "## Objectives & Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f86b0a-b2dd-41c8-8858-2f406012541b",
   "metadata": {},
   "source": [
    "Attacks we have studied so far — such as **FGSM, PGD, Auto-PGD, DeepFool, and NewtonFool** — all focus on crafting adversarial examples by **minimizing perturbations** added to the input images.\n",
    "\n",
    "We also explored attacks like **Brendel & Bethge**, which allow us to further **refine perturbations** on already misclassified images (e.g., starting from a PGD attack).\n",
    "\n",
    "This process typically involves two steps:\n",
    "\n",
    "1. **Attack with PGD** (to misclassify the image)\n",
    "2. **Minimize the perturbation** using Brendel & Bethge\n",
    "\n",
    "**Carlini & Wagner (C&W) performs both of those steps in one single unified process!**\n",
    "\n",
    "It is a **powerful optimization-based attack** that :\n",
    "* Mislead the model (as all attacks do)\n",
    "* Minimizes the perturbation using a **direct optimization objective** (usually with **L2 norm**)\n",
    "* Preserves **high confidence** in the misclassification\n",
    "* **Evades detection** by bypassing defenses based on softmax confidence, entropy, or visible perturbation patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4881b-0055-4da9-a770-e760b34042d0",
   "metadata": {},
   "source": [
    "### Isn't $L_2$ or $L_∞$ used in other attacks ?\n",
    "\n",
    "Yes ! Using **$L_2$, $L_∞$ or $L_1$** norm is not new :\n",
    "* PGD often uses **$L_∞$**\n",
    "* DeepFool focuses on **$L_2$**\n",
    "* Many others supports **multiple norms**\n",
    "\n",
    "**BUT**, Carlini & Wagner **goes far beyon just picking a norm !**\n",
    "\n",
    "C&W **reformulates the entire attack** as a **global optimization problem**.\n",
    "\n",
    "It invents a **custom loss function** and uses an internal optimizer (e.g., Adam) to find the **minimal perturbation required** to fool the model.\n",
    "\n",
    "---\n",
    "\n",
    "Unlike FGSM, PGD, or BIM that require a fixed **epsilon** (maximum perturbation), **C&W automatically finds the smallest possible perturbation** needed to achieve its objective.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447a56b-8667-4688-bbec-2a9911d1aee2",
   "metadata": {},
   "source": [
    "## Loss Function Used in C&W Attack\n",
    "\n",
    "Carlini & Wagner reformulate the attack as an optimization problem:\n",
    "\n",
    "$\n",
    "\\min_{\\delta} \\; \\|\\delta\\|_p + c \\cdot f(x + \\delta)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $x$ = original image  \n",
    "- $\\delta$ = perturbation (the variable being optimized)  \n",
    "- $x + \\delta$ = adversarial image  \n",
    "- $\\|\\delta\\|_p$ = perturbation size ($L_2$ norm is most common)  \n",
    "- $f(x + \\delta)$ = confidence-based loss to **ensure the image is misclassified**  \n",
    "- $c$ = regularization coefficient to **balance between perturbation size and model fooling**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d08825-7b07-4df2-9003-e281cc0f8bbf",
   "metadata": {},
   "source": [
    "## Inner Function: $f(x')$\n",
    "\n",
    "The function $f(x')$ determines **how far the model is from misclassifying the image**.  \n",
    "For a **targeted attack**, the typical formulation is:\n",
    "\n",
    "$\n",
    "f(x') = \\max\\left(\\max_{i \\neq t} \\left\\{ Z_i(x') \\right\\} - Z_t(x'), -\\kappa \\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $Z_i(x')$ are the **logits** (pre-softmax output)  \n",
    "- $t$ is the **target class**  \n",
    "- $\\kappa$ is a **confidence margin**: higher values make the attack stronger\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd79114-3e1d-47aa-ba81-49fbaab85b4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- We want the **logit of the target class $Z_t$** to be higher than **all other logits**.\n",
    "- If that's not the case → $f(x') > 0$ → the optimizer keeps adjusting.\n",
    "- Once the adversarial image **fools the model confidently** → $f(x') \\leq 0$ → this term becomes 0 and the optimizer **focuses only on reducing the perturbation**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4224cb-fa26-4a37-aab5-168c536ac7f2",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Imagine we want to force an image to be classified as **\"dog\"**.\n",
    "\n",
    "The model outputs these logits for the adversarial image $x'$:\n",
    "\n",
    "* $Z_{cat}(x')$ = 12.0\n",
    "* $Z_{dog}(x')$ = 8.0 <- (target)\n",
    "* $Z_{bird}(x')$ = 5.0\n",
    "\n",
    "Then:\n",
    "\n",
    "$\n",
    "f(x') = \\max(12 - 8, 5 - 8, -\\kappa) = \\max(4, -3, -\\kappa)\n",
    "$\n",
    "\n",
    "So long as **another class logit is higher than the target**, $f(x') > 0$, and the optimizer keeps adjusting $\\delta$.\n",
    "\n",
    "Once **Z_dog** becomes the highest, $f(x') \\leq 0$, and the attack focuses only on shrinking the perturbation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008eebf-77a7-46bd-b0c1-7329015eaf13",
   "metadata": {},
   "source": [
    "## $L_{inf}$ Version\n",
    "\n",
    "Carlini & Wagner has 3 types of attacks :\n",
    "* $L_0$\n",
    "* **$L2$**\n",
    "* $L_{inf}$\n",
    "\n",
    "In this code, we'll see the **$L_{inf}$** version.\n",
    "\n",
    "---\n",
    "\n",
    "### What's difference between these 3 versions ?\n",
    "\n",
    "#### $L_0$ :\n",
    "$L_0$ minimize the number of different pixels between the original image and the adversarial picture.\n",
    "\n",
    "Then, the perturbation is **strong on few pixels**. This version is ideal for **visually visible but minimal attacks**. $L_0$ is very useful for **masking or drop-pixel robustness**.\n",
    "\n",
    "\n",
    "#### **$L_2$** :\n",
    "**$L_2$** minimize **the sum of the squares of the pixel differences**.\n",
    "\n",
    "Then, we have **subtle perturbations** and **very difficult to detect visually**. This is the **most used one in litterature** !\n",
    "\n",
    "\n",
    "#### $L_{inf}$ :\n",
    "$L_{inf}$ minimize the **absolute maximum value of disturbance** ($max |\\delta|$).\n",
    "\n",
    "Then, we have **uniform perturbation** over the entire image. And targets adverasrial training type defences, often in $L_{inf}$ (e.g PGD).\n",
    "*Note* : This is a similar attack to PGD in terms of standard, but more optimised and targeted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94731-0d90-4d19-8222-191fb4c647ff",
   "metadata": {},
   "source": [
    "# Code\n",
    "### **AUTHOR** : Maxence QUINET (University Of Luxembourg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92716a-5538-4941-ae1e-585ff0cebd21",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae61f74-994a-44c7-82fe-f09fdb63ea5b",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "Please ensure all dependencies are installed using the `requirements.txt` file.\n",
    "\n",
    "For additional environment setup details, refers to **\"environment_configuration.txt\"**.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e3f54-45df-43c5-9a1f-956d88b53d62",
   "metadata": {},
   "source": [
    "Below are the required **libraries and frameworks** for running Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e6787-99cc-4fc7-9d64-6e8cf224e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # (Reduce TensorFlow logs)\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712eb9f-41ed-426b-b796-2eb7c3fe1ea5",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "**Machine Learning & Neural Network Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce5cd-cabe-470f-a557-d6bedb48499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6f62f-0f79-4fc6-ad7e-6e27035804a5",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**Datasets & Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb4172-8e19-4049-b240-c74387c74f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4ab2c-52cb-428a-b51e-c4a2f73160ea",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**Adversarial Robustness Toolbox (ART)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef553-adcb-4d14-b65f-7870359abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier, PyTorchClassifier\n",
    "from art.attacks.evasion import CarliniL0Method, CarliniL2Method, CarliniLInfMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e44a1-8cfe-4143-9ca3-37645ddf5b95",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "**Vision Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b328a53-2e94-4c2e-b388-8e3e1c037d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb9ee5-13ef-4bb2-bde7-0cc3cc77c9ec",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "**imagenet_stubs** \n",
    "\n",
    "imagenet_stubs is a small dataset available at this link : https://github.com/nottombrown/imagenet-stubs\n",
    "\n",
    "#### Why use it ?\n",
    "\n",
    "* Ideal for **testing adversarial attacks quickly** before applying them on larger datasets.\n",
    "* Provides **two useful functions**:\n",
    "  - `label_to_name(index)` --> Convert an ImageNet label (number) to its corresponding name\n",
    "  - `name_to_label(name)` --> Convert an ImageNet class name back to its numerical label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aac80-f6d0-4fdc-9a0a-68daf1c11fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagenet_stubs\n",
    "from imagenet_stubs.imagenet_2012_labels import label_to_name, name_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1af9f-493c-4822-9f5c-5ddec18010bb",
   "metadata": {},
   "source": [
    "## Checking PyTorch & TensorFlow Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918348c7-eacd-400a-af9a-7f0c5865df92",
   "metadata": {},
   "source": [
    "### **CUDA & GPU Verification**\n",
    "Since we need **CUDA** for accelerated deep learning computations, we ensure that **PyTorch and TensorFlow** are properly configured with CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12e367-3ac8-4bc2-929b-62fa7e586453",
   "metadata": {},
   "source": [
    "------------------\n",
    "**PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57fe8-8f5e-42ce-a951-c9e580038775",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Versions\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b1229-cfdf-484a-a7fe-3c08be588ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PyTorch\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9e593-a5d0-4d4b-a327-a65f933ba887",
   "metadata": {},
   "source": [
    "------------------\n",
    "**TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ae0d0-84da-4ffd-835e-ce2648294735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tensorflow\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df32df-c368-4049-953a-86497c4b558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA & CUDNN Version\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372090f-77fd-44d4-be97-05b820efda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b3e77-63a1-4940-8f4a-7b542df3f6d9",
   "metadata": {},
   "source": [
    "## Dataset Selection & Configuration\n",
    "\n",
    "In this section, you can **choose the dataset** you want to use for adversarial attacks:\n",
    "\n",
    "- **MNIST**\n",
    "- **CIFAR10**\n",
    "- **CIFAR100**\n",
    "- **ImageNet**\n",
    "\n",
    "#### NOTE: On the \"Carlini & Wagner Paper\" they test MNIST, CIFAR10 datasets !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644150a5-4f2e-4e8c-b03a-28f7e3cc442f",
   "metadata": {},
   "source": [
    "### **Select Your Dataset & Model Configuration**\n",
    "Modify the variables below to **choose the dataset and model**.  \n",
    "\n",
    "The **official C&W** paper test the attack on 1 models for each dataset !\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390344aa-5665-4dc2-942e-a0d33ce38fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = \"MNIST\" # OPTIONS : \"MNIST\", \"CIFAR10\", and \"ImageNet\" for Carlini & Wagner\n",
    "\n",
    "# These lines are commented because no choice here by following the paper. MNIST = 1 model, CIFAR10 = 1 Model\n",
    "\n",
    "#selected_cifar10_model = \"standard_resnet\" # OPTIONS : \"standard_resnet\", \"resnet_10x_variant\", \"conv_maxout\"\n",
    "#selected_mnist_model = \"logistic\" # OPTIONS : \"simple_cnn\", \"shallow_softmax\", \"maxout\", \"logistic\" (For MNIST only)\n",
    "\n",
    "selected_attack = \"CarliniWagnerLinf\" # Used for report name only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc0664-be37-43cc-afc2-f5dfbbf25a85",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "**Define class labels for each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed70d3-60ac-4818-bc90-abaa03e8f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the ancestors_name & ancestors_label corresponding to the selected dataset.\n",
    "\n",
    "# Note: All labels are available on Internet. They are not created from us. They are official, often in a .json format.\n",
    "if selected_dataset == \"CIFAR10\":\n",
    "    # Correspondance between name & label for CIFAR10\n",
    "    ancestors_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    # Correspondance between name & label for CIFAR100\n",
    "    ancestors_name = ['apple', 'bridge', 'castle', 'elephant', 'house', 'orange', 'shark', 'table', 'tractor', 'whale']\n",
    "    ancestors_label = ['0', '12', '17', '31', '37', '53', '73', '84', '89', '95']\n",
    "\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['abacus', 'acorn', 'baseball', 'broom', 'brown_bear', 'canoe', 'hippopotamus', 'llama', 'maraca', 'mountain_bike']\n",
    "    ancestors_label = ['398', '988', '429', '462', '294', '472', '344', '355', '641', '671']\n",
    "\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    # Correspondance between name & label for ImageNet\n",
    "    ancestors_name = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    ancestors_label = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "else:\n",
    "    print(f\"Your {selected_dataset} doesn't exist. Please provide an existing dataset between these choices : CIFAR10, CIFAR100, ImageNet & MNIST.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed2cd3-45de-4c8f-8046-d32c53fd4c1a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "**Fix seed to ensure reproducibility (comment to get random results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9afdd-ab88-409b-a161-2ece4778f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23689023-5108-4f89-8534-930e7c4f78e0",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "#### plot_prediction()\n",
    "\n",
    "This function will be used to display the original / attacked images.\n",
    "\n",
    "The function is designed to display the images correctly, depending on the dataset selected, with the following legend:\n",
    "\n",
    "<font color='green'>Green bars</font> = correct classification <br>\n",
    "<font color='red'>Red bars</font> = Attack target classification <br>\n",
    "<font color='blue'>Blue bars</font> = other classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6ad9-e831-4c77-a869-5896bc68e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_name_dynamic(index, dataset):\n",
    "    \"\"\"Return the name of the label depending on the selectionned dataset.\"\"\"\n",
    "    if dataset == \"MNIST\":\n",
    "        return str(index)  # For MNIST, the label name is simply the digit\n",
    "    elif dataset == \"ImageNet\":\n",
    "        return label_to_name(index)  # Use the imagenet_stubs function for ImageNet !\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        return ancestors_name[index]  # Return the name from our list\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        return cifar100_labels[index] if 0 <= index < 100 else \"Unknown\" # Return the name from our list \n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c39259-d85c-4231-815c-27a1b05183e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(img, probs, correct_class=None, target_class=None):\n",
    "    \"\"\"\n",
    "    Displays an image with predictions in the form of coloured bars :\n",
    "    - Green --> Correct Class\n",
    "    - Red --> Target Class\n",
    "    - Blue --> Other Classes\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "    # Display the picture\n",
    "    if selected_dataset==\"MNIST\":\n",
    "        ax1.imshow(img, cmap=\"gray\") # Force the display in gray level for MNIST !\n",
    "        ax1.axis(\"off\")\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "    # Keep the top 10 classes with highest probabilities\n",
    "    top_ten_indexes = list(probs[0].argsort()[-10:][::-1])\n",
    "    top_probs = probs[0, top_ten_indexes]\n",
    "    labels = [label_to_name_dynamic(i, selected_dataset) for i in top_ten_indexes]\n",
    "\n",
    "\n",
    "    # Bar plot creation with color rules defined above\n",
    "    barlist = ax2.bar(range(10), top_probs, color=\"blue\")  # Blue by default\n",
    "\n",
    "    if target_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(target_class)].set_color(\"red\")  # Red if this is the target class\n",
    "\n",
    "    if correct_class in top_ten_indexes:\n",
    "        barlist[top_ten_indexes.index(correct_class)].set_color(\"green\")  # Green if this is the correct class\n",
    "\n",
    "    # Plot Graph\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(10), labels, rotation=\"vertical\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Top 10 Predictions\")\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4484cab-962a-4634-8254-9bc740b92764",
   "metadata": {},
   "source": [
    "## Step 1: Define Parameters\n",
    "\n",
    "> NOTE: The Carlini & Wagner $L_∞$ attack uses a coordinate descent strategy to iteratively craft adversarial examples.\n",
    "Instead of minimizing the L2 norm, it minimizes the maximum absolute change (L∞ norm) while maintaining misclassification.\n",
    "\n",
    "This attack does not use binary search, but relies on tuning tau, a threshold controlling perturbation size.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930689ae-64a9-4395-be5d-b152868d42ec",
   "metadata": {},
   "source": [
    "### **Carlini & Wagner Paper Parameters**\n",
    "\n",
    "* ```confidence (float)``` :\n",
    "Controls the **minimum required confidence** for the attack to succeed.\n",
    "A higher value pushes the model to misclassify with **stronger certainty**.\n",
    "\n",
    "* ```targeted (bool)``` :\n",
    "Indicates whether the attack is **targeted or untargeted**.\n",
    "\n",
    "* ```learning_rate (float)``` :\n",
    "Step size used in the **coordinate descent**.\n",
    "Smaller = more precise, but slower.\n",
    "\n",
    "* ```max_iter (int)``` :\n",
    "Maximum number of **gradient steps** during optimization.\n",
    "\n",
    "* ```decrease_factor (float)``` :\n",
    "Controls how fast the ```tau``` threshold is reduced between steps.\n",
    "Lower = more precision (e.g., 0.9 means ```tau``` is decreased by 10% per iteration).\n",
    "\n",
    "* ```initial_const (float)``` :\n",
    "Starting constant ```c``` in the loss. Larger values increase the importance of **classification vs. perturbation**.\n",
    "\n",
    "* ```largest_const (float)``` :\n",
    "Upper bound on ```c``` during tuning.\n",
    "\n",
    "* ```const_factor (float)``` :\n",
    "Multiplicative factor for increasing ```c```. Should be >1 (e.g. 2.0).\n",
    "\n",
    "* ```batch_size (int)``` :\n",
    "Number of images to process per batch. Usually set to 1.\n",
    "\n",
    "* ```verbose (bool)``` :\n",
    "Shows **progress and debug info**.\n",
    "\n",
    "> You can change the values below to test different attack configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece686ae-29fc-4699-9254-be0e3cbc0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carlini & Wagner L∞ Attack Parameters\n",
    "\n",
    "confidence = 0            # Force misclassification with confidence\n",
    "targeted = True           # Targeted attack (as in the original paper)\n",
    "learning_rate = 0.01      # Optimization step size\n",
    "max_iter = 10             # Number of optimization steps\n",
    "decrease_factor = 0.9     # Controls how tau is reduced (tau *= decrease_factor)\n",
    "initial_const = 0.1      # Starting value of constant c\n",
    "largest_const = 10      # Maximum value c can take\n",
    "const_factor = 2.0        # c is multiplied by this factor after each iteration\n",
    "batch = 1                 # Attack one image at a time (recommended)\n",
    "verbose = True            # Show attack progress\n",
    "\n",
    "print(f\"Selected Attack: CarliniWagnerLinf | Dataset: {selected_dataset} | Confidence: {confidence} | Max Iter: {max_iter} | Batch Size: {batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea443-adc3-480c-8769-94cbc0e4325d",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "**Later, we'll see what EoT is. If you don't know what is EoT, skip this sub-section**\n",
    "\n",
    "*If you want to test EoT Transformation, find parameters below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5c68e-b441-4917-b6e3-6d149ea39d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for EoT Transformation\n",
    "angle_max = 22.5 # Rotation angle used for evaluation in degrees\n",
    "eot_angle = angle_max # Maximum angle for sampling range in EoT rotation, applying range [-eot_angle, eot_angle]\n",
    "eot_samples = 10 # Number of samples with random rotations in parallel per loss gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcc897-543a-4c2a-a813-b3ea9699161c",
   "metadata": {},
   "source": [
    "### Dataset-Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101319c-706d-496f-86d1-b745b7834a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet has 1000 classes, CIFAR100 100 classes, and CIFAR10 & MNIST has 10 classes.\n",
    "nb_classes = 1000 if selected_dataset == \"ImageNet\" else 100 if selected_dataset == \"CIFAR100\" else 10\n",
    "\n",
    "# ImageNet Images Dimension : (299,299,3), CIFAR10 & CIFAR100 : (32,32,3), and MNIST : (28,28,1)\n",
    "input_shape = (299, 299, 3) if selected_dataset == \"ImageNet\" else (32, 32, 3) if \"CIFAR\" in selected_dataset else (28, 28, 1)\n",
    "\n",
    "# ImageNet use often a specific preprocessing. For the others dataset, it still an adapted normalisation (0,1)\n",
    "preprocessing = (0.5, 0.5) if selected_dataset == \"ImageNet\" else (0.0, 1.0)  # Normalisation adaptée\n",
    "\n",
    "# Clip values \n",
    "clip_values = (0.0, 1.0)  # Same for all datasets\n",
    "\n",
    "# Target Class Definition (You can change, here are just some examples)\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    y_target = np.array([641])  # \"maraca\"\n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    y_target = np.array([3])  # \"bear\"\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    y_target = np.array([1])  # \"automobile\"\n",
    "else:  # MNIST\n",
    "    y_target = np.array([np.random.randint(0, 10)])  # random digit between 0 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787fe1-db8a-4d32-aa92-2f11ecd764af",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset Data & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73add696-3799-40c0-ac09-31ed827579e6",
   "metadata": {},
   "source": [
    "In this step, we **load all dataset images and their labels into memory**.\n",
    "\n",
    "#### **How does it work?**\n",
    "1. We retrive the dataset path (`datasets/selected_dataset/`).\n",
    "2. We read all images from the dataset folders.\n",
    "3. We **normalize** the images (scale pixel values between `[0, 1]`).\n",
    "4. We store **both images and labels** for further processing.\n",
    "\n",
    " -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5041c04-64ef-435c-9cb9-2374e84ca1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Initializations\n",
    "x_all, y_all, original_images = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e9aff-599b-46b4-80fa-b06e92f9673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get our dataset path in our computer to keep all pictures and put them into our lists.\n",
    "dataset_path = os.path.join(\"datasets\", selected_dataset)\n",
    "# Check\n",
    "assert(dataset_path==\"datasets/\"+selected_dataset) # If nothing : It's ok. Otherwise, you will get an error if the dataset path doesn't exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b009a5-c170-4d80-8d6b-4de6e1695908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from the selected dataset\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        continue\n",
    "    \n",
    "    for img_file in sorted(os.listdir(class_path)):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        if selected_dataset == \"MNIST\":\n",
    "            im = load_img(img_path, color_mode=\"grayscale\", target_size=(28, 28))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        elif selected_dataset == \"ImageNet\":\n",
    "            im = load_img(img_path, target_size=(299, 299))\n",
    "            im_array = img_to_array(im)\n",
    "\n",
    "        elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "            im = load_img(img_path, target_size=(32, 32))\n",
    "            im_array = img_to_array(im)\n",
    "        \n",
    "        x = (im_array / 255.0).astype(np.float32)\n",
    "        \n",
    "        x_all.append(x)\n",
    "        y_all.append(int(class_label))\n",
    "        original_images.append(im_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dd7e1-03f5-42db-a0c6-48fb139331f6",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "#### Display Dataset (Optional)\n",
    "**You can choose to display all images or only one image per class)**\n",
    "\n",
    "#### How to enable visualization ?\n",
    "- To display **ALL images** --> **Uncomment the loop bellow**.\n",
    "- To display **ONLY 1 image per class** --> **Set `display_all_images = False`**.\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d59b-406f-4883-93bb-f0961d426afc",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying of the 100 pictures (can be long, you can modify the code to display only 1 picture per class if you want)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        print(class_path)\n",
    "        print(\"No os Path\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Class : {class_name} (Label: {class_label})\")\n",
    "    \n",
    "     # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = sorted(os.listdir(class_path))[:1] if not display_all_images else sorted(os.listdir(class_path))\n",
    "    # Go through the 10 pictures of each classes\n",
    "    for img_file in images_to_show:\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "\n",
    "        # Load & Normalize the picture\n",
    "        im = load_img(img_path, target_size=(299, 299))\n",
    "        im_array = img_to_array(im)\n",
    "\n",
    "        # Displaying all pictures\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(im_array.astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Class: {class_name} | {img_file}\", fontsize=10, fontweight=\"bold\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{img_file} well displayed in : {class_name}\")\n",
    "\n",
    "print(f\"All of the {len(ancestors_name)} classes & their images has been displayed !\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7c65f-03a9-44f8-913a-09d2957ef192",
   "metadata": {},
   "source": [
    "### Convert to Numpy Arrays for TensorFlow\n",
    "Since TensorFlow requires NumPy arrays, we convert our lists into arrays.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bde4b5-0d8d-4366-8a99-3c770534f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a numpy array\n",
    "x_all = np.array(x_all)\n",
    "y_all = np.array(y_all).reshape(-1, 1)\n",
    "\n",
    "# Check\n",
    "#for img_x, img_y in zip(x_all, y_all):\n",
    "#    print(f\"x_all shape: {x_all.shape}\")  # (N, H, W, C)\n",
    "#    print(f\"y_all shape: {y_all.shape}\")  # (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dded0f-4832-4d83-9055-c24bb6c7fd57",
   "metadata": {},
   "source": [
    "## Step 3 : Load Model & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411ad0a-c669-417d-9a28-b86681dd231c",
   "metadata": {},
   "source": [
    "### 1. Loading Dataset for Model Training\n",
    "Before creating the model, we **load and preprocess** the dataset to ensure it is correctly formatted for TensorFlow.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc8c37-3fb4-48d1-85b9-2a7175b4e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_dataset == \"MNIST\":\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "elif selected_dataset == \"CIFAR10\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)\n",
    "    \n",
    "elif selected_dataset == \"CIFAR100\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")\n",
    "\n",
    "    # We reproduce the list of all classes of CIFAR100\n",
    "    cifar100_labels = [\n",
    "    \"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\",\n",
    "    \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\",\n",
    "    \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"crab\", \"crocodile\", \"cup\", \"dinosaur\",\n",
    "    \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"computer_keyboard\",\n",
    "    \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\",\n",
    "    \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\",\n",
    "    \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\", \"rose\", \"sea\",\n",
    "    \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\", \"squirrel\", \"streetcar\", \"sunflower\",\n",
    "    \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\", \"train\", \"trout\", \"tulip\",\n",
    "    \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"\n",
    "]\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, nb_classes), to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6b412-2b3f-4389-814a-c07d7a2a9efc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ONE-HOT ENCODING**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3a5e9-ea8a-496f-831d-67d7d0f5efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "    y_train = to_categorical(y_train, nb_classes)\n",
    "    y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51714d-e20b-4bab-8d09-5fc35f30e8cc",
   "metadata": {},
   "source": [
    "### 2. Model Selection & Architecture\n",
    "\n",
    "On the **Carlini & Wagner** Paper, they test 1 model on each dataset : \n",
    "\n",
    "* MNIST --> Table 1\n",
    "* CIFAR10 --> Table 2\n",
    "* ImageNet --> InceptionV3\n",
    "\n",
    "#### WE HAVE TO IMPLEMENT THE CODE TO LOAD MODEL FROM .H5 FILES !\n",
    "\n",
    "-------------\n",
    "\n",
    "You can find in the below case the code ued to create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f22c76-5d1e-4347-9f6e-fad224b6e679",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**MNIST C&W PAPER MODEL**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa61522-c54a-44b7-ab15-39f926c5ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture CIFAR10 with respect to the Carlini & Wagner Paper\n",
    "def build_cw_mnist_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(10)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b4c27-e818-49c0-9190-cc9bababe6c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**CIFAR10 C&W PAPER MODEL**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a665190-dc82-449c-8de8-23b8f8fc6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture CIFAR10 with respect to the Carlini & Wagner Paper\n",
    "def build_cw_cifar10_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(10) \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778a0f-f1d3-457b-a7a2-4fd94d5eb502",
   "metadata": {},
   "source": [
    "### 3. Model Compilation & Training\n",
    "Once the model is selected, we **compile and train** it.\n",
    "\n",
    "- **For ImageNet**, the model is already pretrained\n",
    "- **For other datasets**, a quick training step (5-10 epochs) is performed.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aed09b-80e0-48cc-b4c2-3659ebe7612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= IMAGENET =============\n",
    "if selected_dataset == \"ImageNet\":\n",
    "    print(f\"SELECTED MODEL : InceptionV3.\") \n",
    "    model = InceptionV3(include_top=True, weights='imagenet', classifier_activation=None)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# ============= MNIST =============\n",
    "elif selected_dataset == \"MNIST\":\n",
    "    model = build_cw_mnist_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "    \n",
    "    model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    if y_train.shape[-1] != 10:\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        y_train = to_categorical(y_train, 10)\n",
    "        y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    # Training with respect to the paper\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, y_test)\n",
    "    )\n",
    "    \n",
    "# ============= CIFAR10/CIFAR100 =============\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    # Création du modèle\n",
    "    model = build_cw_cifar10_model()\n",
    "\n",
    "    \n",
    "    optimizer = optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Training with respect to the paper\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, y_test)\n",
    "    )\n",
    "\n",
    "# ============= ERROR =============\n",
    "else:\n",
    "    raise ValueError(f\"Error: Dataset '{selected_dataset} not recognized. Please ensure to use one of this dataset : ImageNet, CIFAR10, CIFAR100 or MNIST.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1fdd7-9038-4b6d-9326-227751a2a427",
   "metadata": {},
   "source": [
    "## Step 4 : Create the ART Classifier & Configure the Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53db541-c3ca-4a5f-8d75-c7dd4557d65d",
   "metadata": {},
   "source": [
    "Now that the model is **trained and ready**, we integrate it into **ART (Adversarial Robustness Toolbox)**.\n",
    "\n",
    "#### What is happening here ?\n",
    "1. We **create a classifier** for ART based on the trained model\n",
    "2. We **define an adversarial attack** (FGSM in this case)\n",
    "3. The attack can be **targeted or untargeted**, and parameters are fully configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597176e7-cdfb-4cc7-8f51-1f74f1a83f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model,\n",
    "                                    nb_classes=nb_classes,\n",
    "                                    loss_object=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                                    preprocessing=preprocessing,\n",
    "                                    preprocessing_defences=None,\n",
    "                                    clip_values=clip_values,\n",
    "                                    input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df1606-f56d-4151-b11a-c23d17350b33",
   "metadata": {},
   "source": [
    "### VERY IMPORTANT !\n",
    "\n",
    "**CarliniMethod** need **logits** and **NOT probabilities** in output !\n",
    "\n",
    "So between the 2 possibilites of loss_type in AutoProjectedGradientDescent instance : `cross_entropy` & `difference_logits_ratio`.\n",
    "\n",
    "We have to choose **cross_entropy**. However, you will get a Value error telling you that CarliniMethod is waiting for logits and not probabilities ! \n",
    "\n",
    "---\n",
    "\n",
    "### WHY ?\n",
    "\n",
    "Because Carlini & Wagner constructs its **personalised loss function** from *logits* in order to **be more sensitive to the ‘decisional distance’ between classes**. The probabilities (softmax output) flatten these differences, which **weakens** the attack.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfaf70-dae6-486c-8c62-41ce8639136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = CarliniLInfMethod(classifier=classifier,\n",
    "                           confidence=0,\n",
    "                           targeted=True,\n",
    "                           learning_rate=0.01,\n",
    "                           max_iter=10,\n",
    "                           decrease_factor=0.9,\n",
    "                           initial_const=1e-2,\n",
    "                           largest_const=2e-2,\n",
    "                           const_factor=2.0,\n",
    "                           batch_size=1,\n",
    "                           verbose=True\n",
    "                          )\n",
    "\n",
    "print(attack.targeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34060c84-e2de-438b-995b-119d73d6b112",
   "metadata": {},
   "source": [
    "## Step 5 : Predict Clean (Original) Images BEFORE the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c927354-cc5b-45d0-bcb3-65f152c94efd",
   "metadata": {},
   "source": [
    "Before applying any attack, we **predict the clean images** with our trained model.\n",
    "\n",
    "#### What happens here ?\n",
    "1. We run the classifier on all images **before the attack**.\n",
    "2. We display the **top-10 predictions** for each image.\n",
    "3. You can choose to **display all images or only one per class**.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54e745-a5c0-485a-8a21-b30f3f08cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Clean Images (returns logits)\n",
    "y_pred_clean_all = classifier.predict(np.array(x_all))\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "y_pred_clean_all = tf.nn.softmax(y_pred_clean_all, axis=1).numpy()\n",
    "\n",
    "\n",
    "# Check prediction shape\n",
    "print(\"Shape of Clean Predictions:\", y_pred_clean_all.shape)  # Expected (N, nb_classes)\n",
    "\n",
    "# Summarize Prediction\n",
    "top1_correct = np.mean(np.argmax(y_pred_clean_all, axis=1) == y_all.flatten()) * 100\n",
    "print(f\"Top-1 Accuracy on Clean Images: {top1_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570014ad-2367-4279-8515-96ff1eb668e3",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "#### Display Clean Images & Predictions\n",
    "You can **choose whether to display all images or just one per class**.\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a7ed8-e5c7-4a3b-ba27-bc7e4d6e4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = False  \n",
    "\n",
    "# Displaying Clean Images with Predictions\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    # Get all images from this class\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No Images found for {class_name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_all[index]),  # Original clean image\n",
    "            y_pred_clean_all[index].reshape(1, -1),  # Reshaped prediction\n",
    "            correct_class=y_all[index],  # True class\n",
    "            target_class=None  # No target class for clean images\n",
    "        )\n",
    "        print(f\"Image {index} displayed for class: {class_name}\")\n",
    "\n",
    "print(f\"\\n All {len(x_all)} clean images have been processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526ff66-f8ff-499a-b44d-5ed770fdd749",
   "metadata": {},
   "source": [
    "## Step 6: Generate and Evaluate Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7f94a-fb49-48f2-a6b1-a4ca9cb6dab6",
   "metadata": {},
   "source": [
    "Now, we **generate adversarial examples** and evaluate the effectiveness of the attack.\n",
    "\n",
    " **What happens here?**\n",
    "1. We **generate adversarial examples** using the selected attack.\n",
    "2. We **save the adversarial images** for later analysis. (optional)\n",
    "3. We **evaluate the attack's success** (accuracy, confidence scores, and performance metrics).\n",
    "4. We **generate a detailed report** summarizing the attack results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa12a5c-516c-4bc7-b099-0f62444d24b6",
   "metadata": {},
   "source": [
    "BE CAREFUL TO DON'T FORGET TO ONE-HOT ENCODED THE TARGET VALUES !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea74630-1505-4d22-b24b-a7a7255a8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot vector repeated for each image\n",
    "y_target_all = np.full(shape=(len(x_all),), fill_value=y_target)\n",
    "y_target_all_onehot = to_categorical(y_target_all, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e17c6-ac35-4194-a7d9-c09ae1aa8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTACK\n",
    "print(f\"Launching Carlini & Wagner L_inf targeted attack on {len(x_all)} images towards class {y_target}...\")\n",
    "x_adv_all = attack.generate(x=x_all, y=y_target_all_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2576dee-4277-4b85-90b7-1a43f803ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "print(\"x_adv_all shape:\", x_adv_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff0f8d-9591-441b-af62-c1a038141487",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "**Do you want to save all adversarial images?**  \n",
    "- **YES** → Uncomment the saving function below.\n",
    "- **NO** → Comment the function to skip saving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b885-6864-4199-9a58-632a74a93a13",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Define the save path for adversarial images\n",
    "adv_save_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset)\n",
    "os.makedirs(adv_save_path, exist_ok=True)  \n",
    "\n",
    "# Iterate through all classes to save adversarial images\n",
    "class_counters = {class_name: 1 for class_name in ancestors_name}  # Dictionary to track image indices per class\n",
    "\n",
    "for adv_img, class_label in zip(x_adv_all, y_all.flatten()):  # Ensure y_all is 1D\n",
    "    # Find the class name corresponding to the label\n",
    "    if str(class_label) not in ancestors_label:\n",
    "        print(f\"Label {class_label} not found in ancestors_label, skipping image.\")\n",
    "        continue  \n",
    "\n",
    "    class_index = ancestors_label.index(str(class_label))\n",
    "    class_name = ancestors_name[class_index]\n",
    "\n",
    "    # Determine the subfolder for the class\n",
    "    class_folder = os.path.join(adv_save_path, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True) \n",
    "\n",
    "    # Generate a unique filename with a counter (e.g., abacus1_adv.jpeg, abacus2_adv.jpeg, ..., acorn1_adv.jpeg, ...)\n",
    "    img_filename = f\"{class_name}{class_counters[class_name]:02d}_adv.jpeg\"\n",
    "    img_path = os.path.join(class_folder, img_filename)\n",
    "\n",
    "    # Convert and save the image\n",
    "    img = array_to_img(adv_img)\n",
    "    img.save(img_path, \"JPEG\")\n",
    "\n",
    "    print(f\"Image saved : {img_path}\")\n",
    "\n",
    "    # Increment the counter for this class\n",
    "    class_counters[class_name] += 1\n",
    "\n",
    "print(f\"\\nAll  {len(x_adv_all)} adversarial images have been successfully saved!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254fe32-85b2-4c37-85ce-1009bbe05e83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate Adversarial Example\n",
    "\n",
    "We now evaluate the adversarial examples by:\n",
    "- Measuring the **model's accuracy** on these images.\n",
    "- Computing the **confidence score** of predictions.\n",
    "- Generating a **visual comparison** between clean and adversarial images.\n",
    "\n",
    "---\n",
    "\n",
    "**How enable visualization?**\n",
    "- To display **ALL images** --> Set `display_all_images = True`\n",
    "- To display **ONLY 1 image per class** --> Set `display_all_images = False`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17136d23-7325-40dd-a592-aeffd89df52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Display\n",
    "for i in range(0, len(x_all), 10):  # Affiche une image sur 10\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    axs[0].imshow(x_all[i].squeeze(), cmap=\"gray\")\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(x_adv_all[i].squeeze(), cmap=\"gray\")\n",
    "    axs[1].set_title(\"Adversarial Image\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(np.abs(x_adv_all[i] - x_all[i]).squeeze(), cmap=\"hot\")\n",
    "    axs[2].set_title(\"Perturbation (abs diff)\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"True: {y_all[i][0]} | Target: {y_target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8c758-31b6-4c1e-b165-bd87a1c469e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Prediction & Visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d880a-f847-4872-94b5-cfa4d6f06df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on adversarial images\n",
    "y_pred_adv_all = classifier.predict(x_adv_all)\n",
    "y_pred_adv_all = tf.nn.softmax(y_pred_adv_all).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4390776-4d46-4ae0-8dba-3e033b99c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to display all images, False to show only 1 image per class\n",
    "display_all_images = True  \n",
    "\n",
    "# Display Adversarial Examples (Optional)\n",
    "for class_name, class_label in zip(ancestors_name, ancestors_label):\n",
    "    print(f\"\\nClass : {class_name} (Label: {class_label})\")\n",
    "\n",
    "    class_indices = np.where(y_all == int(class_label))[0]\n",
    "    \n",
    "    # Show only 1 image per class if display_all_images = False\n",
    "    images_to_show = class_indices[:1] if not display_all_images else class_indices\n",
    "    \n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No images found for {class_name}skipping...\")\n",
    "        continue\n",
    "\n",
    "    for index in images_to_show:\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_all[index]),\n",
    "            y_pred_adv_all[index].reshape(1, -1),\n",
    "            correct_class=y_all[index],\n",
    "            target_class=np.argmax(y_target_all_onehot[index])\n",
    "        )\n",
    "        print(f\"Adversarial Image {index} displayed for class: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d4cd8-dd10-4ac8-9b8d-f9e82797bb5b",
   "metadata": {},
   "source": [
    "### Compute Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f72e-928c-4d21-8da9-e69c53add45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence score\n",
    "confidence_scores = np.max(y_pred_clean_all, axis=1)\n",
    "average_confidence = np.mean(confidence_scores) * 100\n",
    "\n",
    "# Compute Tok-K Accuracy\n",
    "def compute_accuracy(predictions, true_labels, top_k=1):\n",
    "    top_k_preds = np.argsort(predictions, axis=1)[:, -top_k:]\n",
    "    match = np.any(top_k_preds == np.array(true_labels).reshape(-1, 1), axis=1)\n",
    "    return np.mean(match) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a0b18-9939-4b00-80a7-5dd7c57f141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_top1 = compute_accuracy(y_pred_clean_all, y_all, top_k=1)\n",
    "clean_top5 = compute_accuracy(y_pred_clean_all, y_all, top_k=5)\n",
    "adv_top1 = compute_accuracy(y_pred_adv_all, y_all, top_k=1)\n",
    "adv_top5 = compute_accuracy(y_pred_adv_all, y_all, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f2c9-093e-45ec-b3b7-e4079e242270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Performance Results\n",
    "attack_name = \"CarliniLinfMethod\" if isinstance(attack, CarliniLInfMethod) else \"fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b98b0b-0281-45c0-8d8b-a56b1ab187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Selected Attack: CarliniWagnerLinf | Dataset: {selected_dataset} | Confidence: {confidence} | Max Iter: {max_iter} | Batch Size: {batch}\")\n",
    "print(f\"Clean Images : Top-1 : {clean_top1:.2f}% | Top-5 : {clean_top5:.1f}%\")\n",
    "print(f\"Adv. Images  : Top-1 : {adv_top1:.2f}% | Top-5 : {adv_top5:.1f}%\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Confidence Score: {average_confidence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22d39a-2ee4-4ae4-ab55-4e63057fd0a0",
   "metadata": {},
   "source": [
    "### Generate a Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584bf76-61d4-46cd-89b4-776d59ae88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round epsilon for eadability\n",
    "# eps_rounded = round(epsilon, 3)\n",
    "\n",
    "# Define report save path\n",
    "if selected_dataset == \"MNIST\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_report.txt\"\n",
    "elif selected_dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_report.txt\"\n",
    "elif selected_dataset == \"ImageNet\":\n",
    "    report_filename = f\"{selected_attack}_with_{selected_dataset}_with_InceptionV3_report.txt\"\n",
    "else:\n",
    "    print(f\"This {selected_dataset} is not recognized. Be careful to provide an existing dataset between MNIST, CIFAR\")\n",
    "    \n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Report Title\n",
    "    f.write(f\"====== {selected_attack} Adversarial Attack Report ======\\n\\n\")\n",
    "\n",
    "    # Information generation for each image\n",
    "    for i in range(len(y_pred_adv_all)):  \n",
    "        # Find the class index in ancestors_label\n",
    "        class_label = str(y_all[i][0])  # Convert to string to match ancestors_label\n",
    "        if class_label in ancestors_label:\n",
    "            class_index = ancestors_label.index(class_label)  # Get index in ancestors_name\n",
    "            class_name = ancestors_name[class_index]  # Retrieve class name\n",
    "        else:\n",
    "            class_name = \"Unknown\"  # If not found, prevent error\n",
    "\n",
    "        # Original Image file name (ensuring correct numbering)\n",
    "        original_image_name = f\"{class_name}{(i % 10) + 1:02d}.jpeg\"\n",
    "\n",
    "        # Predict Class for the original image (top-1)\n",
    "        clean_pred_index = np.argmax(y_pred_clean_all[i])\n",
    "\n",
    "        # Predict Class for the Adversarial image (top-1)\n",
    "        adv_pred_index = np.argmax(y_pred_adv_all[i])\n",
    "\n",
    "        # Prediction\n",
    "        clean_pred_label = label_to_name_dynamic(clean_pred_index, selected_dataset)\n",
    "        adv_pred_label = label_to_name_dynamic(adv_pred_index, selected_dataset)\n",
    "\n",
    "\n",
    "        # Targeted or Untargeted Scenario Attack\n",
    "        attack_type = \"Targeted\" if attack.targeted else \"Untargeted\"\n",
    "\n",
    "        # If Targeted : Target Class\n",
    "        target_label = label_to_name(y_target[0]) if attack.targeted else \"N/A\"\n",
    "\n",
    "        # Write results in the report:\n",
    "        f.write(f\"------ CLASS : {class_name.upper()} ------\\n\")\n",
    "        f.write(f\"Original image name : {original_image_name}\\n\")\n",
    "        f.write(f\"Original Prediction : {clean_pred_label}\\n\")\n",
    "        f.write(f\"Targeted / Untargeted : {attack_type}\\n\")\n",
    "        if attack.targeted:\n",
    "            f.write(f\"Target Class : {target_label}\\n\")\n",
    "        f.write(f\"Adversarial Prediction : {adv_pred_label}\\n\")\n",
    "        f.write(\"------------------------------------------------\\n\\n\")\n",
    "\n",
    "    # Performance Summary at the end of the file\n",
    "    f.write(\"============ PERFORMANCE RESUME ============\\n\")\n",
    "    f.write(f\"Selected Attack: CarliniWagnerLinf | Dataset: {selected_dataset} | Confidence: {confidence} | Max Iter: {max_iter} | Batch Size: {batch}\")\n",
    "    f.write(f\"Clean Images : Top-1 : {clean_top1:.1f}% | Top-5 : {clean_top5:.1f}%\\n\")\n",
    "    f.write(f\"Adv. Images  : Top-1 : {adv_top1:.1f}% | Top-5 : {adv_top5:.1f}%\\n\")\n",
    "    f.write(\"----------------------------------------------------------\")\n",
    "    f.write(f\"Confidence Score: {average_confidence:.2f}%\")\n",
    "\n",
    "    attack_eff_top1 = 100 - adv_top1\n",
    "    attack_eff_top5 = 100 - adv_top5\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"{selected_attack} Efficiency : Top-1 : {attack_eff_top1:.1f}% | Top-5 : {attack_eff_top5:.1f}%\\n\")\n",
    "\n",
    "# Saving Confirmation\n",
    "print(f\"Report saved : {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a025e-ddb1-4939-b24b-bcf8373c50a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Going further (optional) : Expectation Over Transformation (EoT) \n",
    "Adversarial attacks like **FGSM** are often **sensitive to image transformations** such as **rotation, scaling, or noise**.\n",
    "\n",
    "**Why does this happen?**  \n",
    "- A small rotation (e.g., **5°**) can **invalidate** an adversarial example.\n",
    "- This **breaks the perturbation pattern** that misleads the classifier.\n",
    "  \n",
    "**How does EoT (Expectation Over Transformation) help?**  \n",
    "- Instead of using **a single perturbed image**, EoT **randomly transforms** the image (rotation, blur, etc.).\n",
    "- The attack is then **optimized over multiple transformations**, making it **more robust**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66723512-78b5-4b39-a210-545272a7c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "# Define rotation angles to test\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Apply rotation to all adversarial examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(0, 1), order=1, mode='constant')\n",
    "        for img in x_adv_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "# Get predictions after rotation\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995db06-f091-426e-89f2-30041d9861cc",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d583b-7e22-4b62-ac64-c0f727f6d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  # Set to True to display all, False to show a few per angle\n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}°\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:\n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa39f3d-a018-4ac6-85b4-604077cfa0ff",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a7f96-3779-403c-9a78-4f48e713819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}° → Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39c55a-fd95-4681-b97e-1b09b79835f1",
   "metadata": {},
   "source": [
    "## Step 7: Apply Expectation Over Transformation (EoT)\n",
    "\n",
    "### **What is EoT and Why is it Useful?**\n",
    "FGSM and adversarial attacks often **fail** when images undergo transformations like **rotations**.\n",
    "\n",
    "**EoT (Expectation Over Transformation) mitigates this issue by:**\n",
    "- Generating multiple **randomly transformed** versions of the adversarial image.\n",
    "- Applying these transformations **during model evaluation** (predictions & gradients).\n",
    "- Making the adversarial attack **robust to transformations** like **rotations, noise, and blur**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enable EoT in ART**\n",
    "We use ART’s **`EoTImageRotationTensorFlow`** to introduce **random rotations** during classification.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00187be4-535b-4780-8831-418fce1f3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ART Classifier with EoT\n",
    "eot_rotation = EoTImageRotationTensorFlow(nb_samples=eot_samples,  \n",
    "                                          clip_values=clip_values,  \n",
    "                                          angles=eot_angle)  # Random rotation range\n",
    "\n",
    "classifier_eot = TensorFlowV2Classifier(model=model,\n",
    "                                        nb_classes=nb_classes,\n",
    "                                        loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                        preprocessing=preprocessing,\n",
    "                                        preprocessing_defences=[eot_rotation],  # EoT applied\n",
    "                                        clip_values=clip_values,\n",
    "                                        input_shape=input_shape)\n",
    "\n",
    "print(f\"EoT Classifier created with {eot_samples} transformation samples per evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116e55a-284c-4fe5-b4b6-3dd27961243d",
   "metadata": {},
   "source": [
    "### Generate Adversarial Examples with EoT\n",
    "We generate **adversarial examples** that remain effective even **after transformations**.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235e4b7-6331-44d7-99bc-276b49b9bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare target labels for targeted attacks\n",
    "y_target_one_hot = np.zeros((1, nb_classes), dtype=np.float32)\n",
    "y_target_one_hot[0, name_to_label(\"guacamole\")] = 1.0  \n",
    "y_target_all = np.tile(y_target_one_hot, (len(x_all), 1))  \n",
    "\n",
    "x_adv_eot_all = []\n",
    "\n",
    "for i in tqdm(range(len(x_all)), desc=\"Generating EoT Examples\"):\n",
    "    x_i = np.expand_dims(x_all[i], axis=0)  \n",
    "    y_i = np.expand_dims(y_target_all[i], axis=0)  \n",
    "\n",
    "    if attack.targeted:\n",
    "        x_adv_i = attack.generate(x=x_i, y=y_i)\n",
    "    else:\n",
    "        x_adv_i = attack.generate(x=x_i)\n",
    "\n",
    "    x_adv_eot_all.append(np.squeeze(x_adv_i))  \n",
    "\n",
    "x_adv_eot_all = np.array(x_adv_eot_all)\n",
    "\n",
    "print(f\"Shape of EoT Adversarial Examples: {x_adv_eot_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0278d2-9459-4d43-b261-6db1e9b07a1f",
   "metadata": {},
   "source": [
    "### Apply Rotation to Adversarial Examples\n",
    "We now test the **robustness** of these adversarial examples by **rotating them** at different angles.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69ea7a-b552-4f9e-bd92-baaa5719ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotation angles\n",
    "rotation_angles = [-22.5, -10.0, -5.0, 0.0, 5.0, 10.0, 22.5]  \n",
    "\n",
    "# Rotate and Evaluate Adversarial Examples\n",
    "x_adv_rotated_all = {\n",
    "    angle: np.array([\n",
    "        scipy.ndimage.rotate(img, angle=angle, reshape=False, axes=(1, 2), order=1, mode='constant')\n",
    "        for img in x_adv_eot_all\n",
    "    ]) for angle in rotation_angles\n",
    "}\n",
    "\n",
    "y_pred_adv_rotated_all = {\n",
    "    angle: classifier.predict(x_adv_rotated_all[angle])\n",
    "    for angle in rotation_angles\n",
    "}\n",
    "\n",
    "print(f\"Adversarial images rotated and evaluated for {len(rotation_angles)} angles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342a24c-5eb9-4e8c-8bcb-9fe1feacf2f4",
   "metadata": {},
   "source": [
    "### Display Rotated Adversarial Examples\n",
    "You can **choose whether to display all images or just a few per rotation angle**.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece90eb-c169-4be1-becf-42a7df619fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_images = False  \n",
    "\n",
    "for angle in rotation_angles:\n",
    "    print(f\"\\nRotation Angle: {angle}°\")\n",
    "\n",
    "    for i in range(len(x_adv_rotated_all[angle])):\n",
    "        if not display_all_images and i > 1:  \n",
    "            break\n",
    "\n",
    "        plot_prediction(\n",
    "            np.squeeze(x_adv_rotated_all[angle][i]),  \n",
    "            y_pred_adv_rotated_all[angle][i].reshape(1, -1),  \n",
    "            correct_class=y_all[i],  \n",
    "            target_class=y_target  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb82c3-856a-4174-a218-3e3b80061d17",
   "metadata": {},
   "source": [
    "### Evaluate Performance After Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc01fc-c6b9-43c7-ada2-c2affe63d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy After Rotation\n",
    "for angle in rotation_angles:\n",
    "    adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "    adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "    \n",
    "    print(f\"Rotation {angle}° → Top-1: {adv_top1_rotated:.1f}% | Top-5: {adv_top5_rotated:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2709c-5a09-4a6a-a670-8aeafacdc02f",
   "metadata": {},
   "source": [
    "### Generate a Report on EoT Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c28d0f-e529-43fb-a248-67e6914fe814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define report save path\n",
    "report_filename = f\"EoT_{selected_attack}_{selected_dataset}_eps={round(epsilon, 2)}.txt\"\n",
    "report_path = os.path.join(\"adversarials_img\", selected_attack, selected_dataset, report_filename)\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "\n",
    "# Generate Report\n",
    "print(\"\\nGenerating EoT attack report...\")\n",
    "\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"====== EoT Adversarial Attack Report (ε = {round(epsilon, 2)}) ======\\n\\n\")\n",
    "    \n",
    "    for angle in rotation_angles:\n",
    "        adv_top1_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=1)\n",
    "        adv_top5_rotated = compute_accuracy(y_pred_adv_rotated_all[angle], y_all, top_k=5)\n",
    "        \n",
    "        f.write(f\"\\n=== Rotation {angle}° ===\\n\")\n",
    "        f.write(f\"Top-1 Accuracy: {adv_top1_rotated:.1f}%\\n\")\n",
    "        f.write(f\"Top-5 Accuracy: {adv_top5_rotated:.1f}%\\n\")\n",
    "\n",
    "print(f\"EoT Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c1bf-f0f3-460e-9e39-9f2056710a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (whiteboxattack)",
   "language": "python",
   "name": "whiteboxattack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
